Chapter 1. Introduction to Linux
After completing this chapter, you will be able to: Explain the purpose of an operating system
Outline the key features of the Linux operating system
Describe the origins of the Linux operating systems
Identify the characteristics of various Linux distributions and where to find them
Explain the common uses of Linux in industry today

Linux technical expertise is essential in today’s computer workplace as more and more companies switch to Linux to meet their computing needs. Thus, it is important to understand how Linux can be used, what benefits Linux offers to a company, and how Linux has developed and continues to develop. In the first half of this chapter, you will learn about operating system terminology and features of the Linux operating system, as well as the history and development of Linux. Later in this chapter, you will learn about the various types of Linux and about the situations in which Linux is used.

Operating Systems
Every computer has two fundamental types of components: hardware and software. You are probably familiar with these terms, but it’s helpful to review their meanings so you can more easily understand how Linux helps them work together.

Hardware consists of the physical components inside a computer that are electrical in nature; they contain a series of circuits that manipulate the flow of information. A computer can contain many pieces of hardware, including the following:

A processor (also known as the central processing unit or CPU), which computes information

Physical memory (also known as random access memory, or RAM), which stores information needed by the processor

Hard disk and solid state disk drives, which store most of the information that you use

CD/DVD drives, which read and write information to and from CD/DVD discs

Flash memory card readers, which read and write information to and from removable memory cards, such as Secure Digital (SD) cards

Sound cards, which provide audio to external speakers

Video cards, which display results to the display screen

Network adapter cards, which provide access to wired and wireless (Wi-Fi or Bluetooth) networks

Ports (such as USB, eSATA, GPIO, and Thunderbolt), which provide access to external devices including keyboards, mice, printers, and storage devices

Mainboards (also known as motherboards), which provide the circuitry (also known as a bus) for interconnecting all other components

Software, on the other hand, refers to the sets of instructions or programs that allow the hardware components to manipulate data (or files). When a bank teller types information into the computer behind the counter at a bank, for example, the bank teller is using a program that understands what to do with your bank records. Programs and data are usually stored on hardware media, such as hard disks or solid state disks, although they can also be stored on removable media or even embedded in computer chips. These programs are loaded into parts of your computer hardware (such as your computer’s memory and processor) when you first turn on your computer and when you start additional software, such as word processors or Internet browsers. After a program is executed on your computer’s hardware, that program is referred to as a process. In other words, a program is a file stored on your computer, whereas a process is that file in action, performing a certain task.

There are two types of programs. The first type, applications (or apps), includes those programs designed for a specific use and with which you commonly interact, such as word processors, computer games, graphical manipulation programs, and computer system utilities. The second type, operating system (OS) software, consists of a set of software components that control the hardware of your computer. Without an operating system, you would not be able to use your computer. Turning on a computer loads the operating system into computer hardware, which then loads and centrally controls all other application software in the background. At this point, the user (the person using the computer) is free to interact with the applications, perhaps by typing on the keyboard or clicking with a mouse. Applications take the information the user supplies and relay it to the operating system, which uses the computer hardware to carry out the requests. The relationship between users, application software, operating system software, and computer hardware is illustrated in Figure 1-1.
Details
The role of operating system software
The operating system carries out many tasks by interacting with different types of computer hardware. For the operating system to accomplish the tasks, it must contain the appropriate device driver software for every hardware device in your computer. Each device driver tells the operating system how to use that specific device. The operating system also provides a user interface, which is a program that accepts user input indicating what to do, forwards this input to the operating system for completion, and, after it is completed, gives the results back to the user. The user interface can be a command-line prompt, in which the user types commands, or it can be a graphical user interface (GUI), which consists of menus, dialog boxes, and symbols (known as icons) that the user can interact with via the keyboard or the mouse. A typical Linux GUI is shown in Figure 1-2.
Details
A Linux graphical user interface

Finally, operating systems offer system services, which are applications that handle system-related tasks, such as printing, scheduling programs, and network access. These system services determine most of the functionality in an operating system. Different operating systems offer different system services, and many operating systems allow users to customize the services they offer.

The Linux Operating System
Linux (pronounced “Lih-nucks”) is an operating system you use to run applications on a variety of hardware. Similar to other operating systems, Linux loads into computer memory when you first power on your computer and initializes (or activates) all of the hardware components. Next, it loads the programs that display the interface. From within the interface, you can execute commands that tell the operating system and other applications to perform specific tasks. The operating system then uses the computer hardware to perform the tasks required by the applications.

Linux can manage thousands of tasks at the same time, including allowing multiple users to access the system simultaneously. Hence, Linux is referred to as a multiuser and multitasking operating system.

Versions of the Linux Operating System
The core component of the Linux operating system is called the Linux kernel. The Linux kernel and supporting software (called function libraries) are written almost entirely in the C programming language, which is one of the most common languages that software developers use when creating programs.

Although you can use a variety of software to modify the appearance of Linux, the underlying kernel is common to all types of Linux. The Linux kernel is developed continuously; thus, you should understand the version numbers of the Linux kernel to decide which kernel version is appropriate for your needs. Because the Linux kernel is directly responsible for controlling the computer’s hardware (via device drivers), you might sometimes need to upgrade the kernel after installing Linux to take advantage of new technologies or to fix problems (also known as bugs) related to your computer’s hardware. Consequently, you need a good understanding of your system’s hardware to decide which kernel to use.

Note 
For a complete list of kernels, kernel versions, and their improvements, see www.kernel.org.

In some cases, you can use updates in the form of a kernel module or a kernel patch to provide or fix hardware supported by the kernel. Kernel modules and kernel patches are discussed later in this book.

Identifying Kernel Versions
Linux kernel versions are made up of the following three components:

Major number

Minor number

Revision number

Let’s look at a sample Linux kernel version, 4.17.6. In this example, the major number is the number 4, which indicates the major version of the Linux kernel. The minor number, represented by the number 17, indicates the minor revision of the Linux kernel. As new features are added to the Linux kernel over time, the minor number is incremented. The major number is usually incremented when a major kernel feature is implemented, when the minor number versioning reaches a high number, or to signify a major event; for example, the 3.0 kernel was introduced to commemorate the twentieth anniversary of Linux.

Linux kernel changes occur frequently. Very minor changes are represented by a revision number indicating the most current changes to the version of the particular kernel that is being released. For example, a 4.17.6 kernel has a revision number of 6. This kernel is the sixth release of the 4.17 kernel. Some kernels have over 100 revisions as a result of developers making constant improvements to the kernel code.

Note 
Sometimes, a fourth number is added to a kernel version to indicate a critical security or bug patch. For example, a 4.17.6.1 kernel is a 4.17.6 kernel with a critical patch number of 1.

Modern Linux kernels that have a major, minor, and revision number are referred to as production kernels; they have been thoroughly tested by several Linux developers and are declared stable. Developmental kernels are not fully tested and imply instability; they are tested for vulnerabilities by people who develop Linux software. Most developmental kernels append the minor number with the letters -rc (release candidate) followed by a number that represents the version of the developmental kernel. For example, the 4.18-rc3 developmental kernel is the third release candidate for the 4.18 kernel; if Linux developers declare it stable after being thoroughly tested, it will become the 4.18.0 production kernel.

Note 
Until Linux kernel 2.6.0, an odd-numbered minor number was used to denote a development kernel, and an even-numbered minor number was used to denote a production kernel.

Note 
When choosing a kernel for a mission-critical computer such as a server, ensure that you choose a production kernel. This reduces the chance that you will encounter a bug in the kernel, which saves you the time needed to change kernels.

Table 1-1 shows some sample kernel versions released since the initial release of Linux.

Table 1-1 Sample Linux Kernel Version History
Kernel version	Date released	Type
0.01	September 1991	First Linux kernel
0.12	January 1992	Production (stable)
0.95	March 1992	Developmental
0.98.6	December 1992	Production (stable)
0.99.15	March 1994	Developmental
1.0.8	April 1994	Production (stable)
1.3.100	May 1996	Developmental
2.0.36	November 1998	Production (stable)
2.3.99	May 2000	Developmental
2.4.17	December 2001	Production (stable)
2.5.75	July 2003	Developmental
2.6.35	August 2010	Production (stable)
3.0.0	July 2011	Production (stable)
3.2.21	June 2012	Production (stable)
3.10.4	July 2013	Production (stable)
3.15.10	August 2014	Production (stable)
4.0.0	April 2015	Production (stable)
4.6.3	March 2016	Production (stable)
4.12.5	August 2017	Production (stable)
4.18-rc3	July 2018	Developmental




Licensing Linux
Companies often choose Linux as their operating system because of the rules governing Linux licensing. Unlike most other operating systems, Linux is freely developed and continuously improved by a large community of software developers. For this reason, it is referred to as Open Source Software (OSS).

To understand OSS, you must first understand how source code is used to create programs. Source code refers to the list of instructions that a software developer writes to make up a program; an example of source code is shown in Figure 1-3.

Figure 1-3

Source code

After the software developer finishes writing the instructions, the source code is compiled into a format (called machine language) that only your computer’s processor can understand and execute. To edit an existing program, the software developer must edit the source code and then recompile it.

The format and structure of source code follows certain rules defined by the programming language in which it was written. Programmers write Linux source code in many programming languages. After being compiled into machine language, all programs look the same to the computer operating system, regardless of the programming language in which they were written. As a result, software developers choose a programming language to create source code based on ease of use, functionality, and comfort level.

The fact that Linux is an OSS operating system means that software developers can read other developers’ source code, modify that source code to make the software better, and redistribute that source code to other developers who might improve it further. Like all OSS, Linux source code must be distributed free of charge, regardless of the number of modifications made to it. People who develop OSS commonly use the Internet to share their source code, manage software projects, and submit comments and fixes for bugs (flaws). In this way, the Internet acts as the glue that binds together Linux developers in particular and OSS developers in general.

Note 
To read the complete open source definition, visit opensource.org.

Here are some implications of the OSS way of developing software:

Software is developed rapidly through widespread collaboration.

Software bugs (errors) are noted and promptly fixed.

Software features evolve quickly, based on users’ needs.

The perceived value of the software increases because it is based on usefulness and not on price.

As you can imagine, sharing ideas and source code is beneficial to software developers. However, a software company’s business model changes drastically when OSS enters the picture. The main issue is this: How can a product that is distributed freely generate revenue? After all, without revenue any company will go out of business.

The OSS process of software development was never intended to generate revenue directly. Its goal was to help people design better software by eliminating many of the problems associated with traditional software development, which is typically driven by predefined corporate plans and rigid schedules. By contrast, OSS development assumes that software creation is an art in which a particular problem can be solved in many ways. One software developer might create a program that measures widgets using four pages of source code, while another developer might create a program that does the same task in one page of source code. You might think that this openness to multiple ways of solving a problem would result in a haphazard software development process, but the sharing of ideas that is the heart of OSS development keeps developers focused on the best possible solutions. Also, while OSS developers contribute their strengths to a project, they learn new techniques from other developers at the same time.

Because the selling of software for profit discourages the free sharing of source code, OSS generates revenue indirectly. Companies usually make money by selling computer hardware that runs OSS, by selling customer support for OSS, or by creating closed source software programs that run on open source products such as Linux.

The OSS development process is, of course, not the only way to develop and license software. Table 1-2 summarizes the types of software you are likely to encounter. The following section explains these types in more detail.

Table 1-2 Software Types
Type	Description
Open source	Software in which the source code and software can be obtained free of charge and optionally modified to suit a particular need
Closed source	Software in which the source code is not available; although this type of software might be distributed free of charge, it is usually quite costly and commonly referred to as commercial software
Freeware	Closed source software that is given out free of charge; it is sometimes referred to as freemium software
Shareware	Closed source software that is initially given out free of charge but that requires payment after a certain period of use
Types of Open Source Licenses
Linux adheres to the GNU General Public License (GPL), which was developed by the Free Software Foundation (FSF). The GPL stipulates that the source code of any software published under its license must be freely available. If someone modifies that source code, that person must also redistribute that source code freely, thereby keeping the source code free forever.

Note 
“GNU” stands for “GNUs Not UNIX.”

Note 
The GPL is freely available at www.gnu.org and in this book’s Appendix B, “GNU General Public License.”

Another type of open source license is the artistic license, which ensures that the source code of the program is freely available yet allows the original author of the source code some control over the changes made to it. Thus, if one developer obtains and improves the source code of a program, the original author has the right to reject those improvements. As a result of this restriction, artistic licenses are rarely used because many developers do not want to work on potentially futile projects.

In addition to the two open source licenses mentioned are other types of open source licenses that differ only slightly from one another. Those licenses must adhere to the open source definition but might contain extra conditions that the open source definition does not.

Note 
For a list of approved open source licenses, visit opensource.org.

Types of Closed Source Licenses
Closed source software can be distributed for free or for a cost; either way, the source code for the software is unavailable from the original developers. The majority of closed source software is sold commercially and bears the label of its manufacturer. Each of these software packages can contain a separate license that restricts free distribution of the program and its source code in many ways.

Note 
Examples of closed source software are software created by companies such as Microsoft, Apple, and Electronic Arts (EA).

Another type of closed source software is freeware, in which the software program is distributed free of charge, yet the source code is unavailable. Freeware might also contain licenses that restrict the distribution of source code. Another approach to this style of closed source licensing is shareware, which is distributed free of charge, yet after a certain number of hours of usage or to gain certain features of the program, payment is required. Although freeware and shareware do not commonly distribute their source code under an open source license, some people incorrectly refer to freeware as OSS, assuming that the source code is free as well.

Linux Advantages
The main operating systems in use today include Linux, Microsoft Windows, UNIX, and macOS. Notably, Linux is the fastest growing operating system released to date. Although Linux was only created in 1991, the number of Linux users estimated by Red Hat in 1998 was 7.5 million, and the number of Linux users estimated by Google in 2010 was over 40 million (including the number of Linux-based Android smartphone and device users). In 2013,LinuxCounter.net (http://linuxcounter.net) estimated that the number of Linux users was over 70 million, and Google estimated that over 900 million Linux-based Android devices had shipped by then. Since 1998, many large companies, including IBM, Hewlett-Packard, Intel, and Dell, have announced support for Linux and OSS. In the year 2000, IBM announced plans to spend one billion dollars on Linux and Linux development alone.

People have begun using Linux for many reasons. The following advantages are examined in the sections that follow:

Risk reduction

Meeting business needs

Stability and security

Flexibility for different hardware platforms

Ease of customization

Ease of obtaining support

Cost reduction

Risk Reduction
Companies need software to perform mission-critical tasks, such as database tracking, Internet business (e-commerce), and data manipulation. However, changes in customer needs and market competition can cause the software a company uses to change frequently. Keeping the software up to date can be costly and time-consuming, but is a risk that companies must take. Imagine that a fictitious company, ABC Inc., buys a piece of software from a fictitious software vendor, ACME Inc., to integrate its sales and accounting information with customers via the Internet. What would happen if ACME went out of business or stopped supporting the software due to lack of sales? In either case, ABC would be using a product that had no software support, and any problems that ABC had with the software after that time would go unsolved and could result in lost revenue. In addition, all closed source software is eventually retired after it is purchased, forcing companies to buy new software every so often to obtain new features and maintain software support.

If ABC instead chose to use an OSS product and the original developers became unavailable to maintain it, then the ABC staff would be free to take the source code, add features to it, and maintain it themselves provided the source code was redistributed free of charge. Also, most OSS does not retire after a short period of time because collaborative open source development results in constant software improvement geared to the needs of the users.

Meeting Business Needs
Recall that Linux is merely one product of open source development. Many thousands of OSS programs are available, and new ones are created daily by software developers worldwide. Most open source Internet tools have been developed for quite some time now, and the focus in the Linux community in the past few years has been on developing application software, cloud technologies, and security-focused network services that run on Linux. Almost all of this software is open source and freely available, compared to other operating systems, in which most software is closed source and costly.

OSS is easy to locate on the Web, at sites such as SourceForge (sourceforge.net), GitHub (github.com), and GNU Savannah (savannah.gnu.org). New software is published to these sites daily. SourceForge alone hosts over 430,000 software development projects.

Common software available for Linux includes but is not limited to the following:

Scientific and engineering software

Software emulators

Web servers, Web browsers, and e-commerce suites

Desktop productivity software (e.g., word processors, presentation software, spreadsheets)

Graphics manipulation software

Database software

Security software

In addition, companies that run the UNIX operating system (including macOS, which is a flavor of UNIX) might find it easy to migrate to Linux. For those companies, Linux supports most UNIX commands and standards, which eases a transition to Linux because the company likely would not need to purchase additional software or retrain staff. For example, suppose a company that tests scientific products has spent time and energy developing custom software that runs on the UNIX operating system. If this company transitions to another operating system, its staff would need to be retrained or hired, and much of the custom software would need to be rewritten and retested, which could result in a loss of customer confidence. If, however, that company transitions to Linux, the staff would require little retraining, and little of the custom software would need to be rewritten and retested, hence saving money and minimizing impact on consumer confidence.

Companies that need to train staff on Linux usage and administration can take advantage of several educational resources and certification exams for various Linux skill levels. Certification benefits as well as the CompTIA Linux+ and LPIC-1 certifications are discussed in this book’s Appendix A, “Certification.”

In addition, for companies that require a certain development environment or need to support custom software developed in the past, Linux provides support for most programming languages.

Stability and Security
OSS is developed by people who have a use for it. This collaboration among several developers with a common need speeds up software creation, and when bugs in the software are found by these users, bug fixes are created quickly. Often, the users who identify the bugs can fix the problem because they have the source code, or they can provide detailed descriptions of their problems so that other developers can fix them.

By contrast, customers using closed source operating systems must rely on the operating system vendor to fix any bugs. Users of closed source operating systems must report the bug to the manufacturer and wait for the manufacturer to develop, test, and release a solution to the problem, known as a hot fix. This process might take weeks or even months, which is slow and costly for most companies and individuals. The thorough and collaborative open source approach to testing software and fixing software bugs increases the stability of Linux; it is not uncommon to find a Linux system that has been running continuously for months or even years without being turned off.

Security, a vital concern for most companies and individuals, is another Linux strength. Because Linux source code is freely available and publicly scrutinized, security loopholes are quickly identified and fixed by several developers. In contrast, the source code for closed source operating systems is not released to the public for scrutiny, which means customers must rely on the OS vendor to provide security. A security breach unnoticed by the vendor can be exploited by the wrong person. Every day, new malicious software (destructive programs that exploit security loopholes, such as viruses and malware) is unleashed on the Internet with the goal of infiltrating closed source operating systems, such as Windows. By contrast, the number of viruses that can affect Linux is exceedingly low. As of April 2008, Linux had fewer than 100 known viruses, whereas Windows had more than 1,000,000 known viruses. Compared to other systems, the amount of malicious software for Linux systems remains incredibly low, and nearly all of it is designed to breach older versions of unprotected Linux-based Android smartphones. As a result, most desktop and server Linux systems that run antivirus and antimalware software today do so because they host files that may be shared with Windows computers.

Note 
For a list of recent malicious software, visit securelist.com.

Flexibility for Different Hardware Platforms
Another important feature of Linux is that it can run on a variety of computer hardware platforms frequently found in different companies. Although Linux is most commonly installed on the Intel x86/x64 platforms, it can also be installed on other types of hardware, such as the POWER platform from IBM that runs on many of the largest supercomputers in the world. Companies can run Linux on very large and expensive hardware for big tasks, such as graphics rendering or chemical molecular modeling, as well as on smaller systems, such as point-of-sale terminals and inventory scanners. Few other operating systems run on more than two hardware platforms, making Linux the ideal choice for companies that use different types of or specialized hardware.

Here is a partial list of hardware platforms on which Linux can run:

Intel x86/x64

Itanium

Mainframe (S/390, z/Architecture)

ARM

MIPS

SPARC/Ultra-SPARC

PowerPC/POWER

In addition to the platforms in the preceding list, Linux can be customized to work on most hardware. Thousands of high-tech companies rely on embedded operating system technology to drive their systems. NASA spacecrafts, Internet routers and firewalls, Google Android smartphones and tablets, Amazon Kindle eBook readers, TomTom GPS navigation systems, smart thermostats, and Wi-Fi access points all run Linux. This focus on mobile and embedded devices will become more important as the need for new functionality increases. The rich set of OSS developers at work today makes Linux an attractive choice for manufacturers of mobile and embedded devices.

Ease of Customization
The ease of controlling the inner workings of Linux is another attractive feature, particularly for companies that need their operating system to perform specialized functions. If you want to use Linux as a Web server, you can simply recompile the Linux kernel to include only the support needed to be a Web server. This results in a much smaller and faster kernel.

Note 
A small kernel performs faster than a large kernel because it contains less code for the processor to analyze. On high-performance systems, you should remove any unnecessary features from the kernel to improve performance.

Today, customizing and recompiling the Linux kernel is a well-documented and easy process; however, it is not the only way to customize Linux. Only software packages necessary to perform certain tasks need to be installed; thus, each Linux system can have a unique configuration and set of applications available to the user. Linux also supports several system programming languages, such as Shell and PERL, which you can use to automate tasks or create custom tasks.

Consider a company that needs an application to copy a database file from one computer to another computer, yet also needs to manipulate the database file (perhaps by checking for duplicate records), summarize the file, and finally print it as a report. This might seem like a task that would require expensive software; however, in Linux, you can write a short PERL script that uses common Linux commands and programs to perform these tasks. This type of customization is invaluable to companies because it allows them to combine several existing applications to perform a certain task, which might be specific only to that company and, hence, not previously developed by another free software developer. Most Linux configurations present hundreds of small utilities, which, when combined with Shell or PERL programming, can make new programs that meet many business needs.

Ease of Obtaining Support
For those who are new to Linux, the Internet offers a world of Linux documentation. Frequently asked questions (FAQs) and instructions known as HOWTO documents are arranged by topic and are available to anyone. HOWTO documents are maintained by their authors yet are centrally collected by the Linux Documentation Project (LDP), which has several hundred websites worldwide that allow you to search or download HOWTO documents.

A search of the word “HOWTO” on a typical Internet search engine such as www.google.com displays thousands of results, or you can download the worldwide collection of HOWTO documents at www.tldp.org.

In addition, several Internet newsgroups allow Linux users to post messages and reply to previously posted messages. If you have a specific problem with Linux, you can post your problem on an Internet newsgroup and receive help from those who know the solution. Linux newsgroups are posted to frequently; thus, you can usually expect a solution to a problem within hours. A list of common Linux newsgroups can be found at http://groups.google.com.

An alternative to Internet newsgroups include Linux-focused Facebook groups and website forums. Appendix C, “Finding Linux Resources on the Internet,” describes how to navigate Internet resources and lists some resources that you might find useful.

Although online support is the typical method of getting help, other methods are available. Most Linux distributions provide professional telephone support services for a modest fee, and many organizations give free support to those who ask. The most common of these groups are referred to as Linux User Groups (LUGs), and most large cities across the globe have at least one. LUGs are groups of Linux users who meet regularly to discuss Linux-related issues and problems. An average LUG meeting consists of several new Linux users (also known as Linux newbies), administrators, developers, and experts (also known as Linux gurus). LUG meetings are a resource to solve problems and learn about the local Linux community. Most LUGs host websites that contain a multitude of Linux resources, including summaries of past meetings and discussions. One common activity seen at a LUG meeting is referred to as an Installfest; several members bring in their computer equipment to install Linux and other Linux-related software. This approach to transferring knowledge is very valuable to LUG members because concepts can be demonstrated and the solutions to problems can be modeled by more experienced Linux users.

Note 
To find a list of available LUGs in your region, search for the words “LUG cityname” on an Internet search engine such as www.google.com (substituting your city’s name for “cityname”). When searching for a LUG, keep in mind that LUGs might go by several different names; for example, the LUG in Hamilton, Ontario, Canada is known as HLUG (Hamilton Linux Users Group). Many LUGs today are managed using Facebook groups or meeting sites such as www.meetup.com.

Cost Reduction
Linux is less expensive than other operating systems such as Windows because there is no cost associated with acquiring the software. In addition, a wealth of OSS can run on different hardware platforms running Linux, and a large community of developers is available to diagnose and fix bugs in a short period of time for free. While Linux and the Linux source code are distributed freely, implementing Linux is not cost free. Costs include purchasing the computer hardware necessary for the computers hosting Linux, hiring people to install and maintain Linux, and training users of Linux software.

The largest costs associated with Linux are the costs associated with hiring people to maintain the Linux system. However, closed source operating systems have this cost in addition to the cost of the operating system itself. The overall cost of using a particular operating system is known as the total cost of ownership (TCO). Table 1-3 shows an example of the factors involved in calculating the TCO for operating systems.

Table 1-3 Calculating the Total Cost of Ownership
Costs	Linux	Closed source operating system
Operating system cost	$0	Greater than $0
Cost of administration	Low: Stability is high and bugs are fixed quickly by open source developers.	Moderate/high: Bug fixes are created by the vendor of the operating system, which could result in costly downtime.
Cost of additional software	Low/none: Most software available for Linux is also open source.	Moderate/high: Most software available for closed source operating systems is also closed source.
Cost of software upgrades	Low/none	Moderate/high: Closed source software is eventually retired, and companies must buy upgrades or new products to gain functionality and stay competitive.

The History of Linux
Linux is based on the UNIX operating system developed by Ken Thompson and Dennis Ritchie of AT&T Bell Laboratories in 1969 and was developed through the efforts of many people as a result of the hacker culture that formed in the 1980s. Therefore, to understand how and why Linux emerged on the operating system market, you must first understand UNIX and the hacker culture. Figure 1-4 illustrates a timeline representing the history of the UNIX and Linux operating systems.

Figure 1-4

Details
Timeline of UNIX and Linux development

UNIX
The UNIX operating system has roots running back to 1965, when the Massachusetts Institute of Technology (MIT), General Electric, and AT&T Bell Laboratories began developing an operating system called Multiplexed Information and Computing Service (MULTICS). MULTICS was a test project intended to reveal better ways of developing time-sharing operating systems, in which the operating system regulates the amount of time each process has to use the processor. The project was abandoned in 1969. However, Ken Thompson, who had worked on the MULTICS operating system, continued to experiment with operating systems. In 1969, he developed an operating system called UNIX that ran on the DEC (Digital Equipment Corporation) PDP-7 computer.

Shortly thereafter, Dennis Ritchie invented the C programming language that was used on Ken Thompson’s UNIX operating system. The C programming language was a revolutionary language. Most programs at the time needed to be written specifically for the hardware of the computer, which involved referencing volumes of information regarding the hardware in order to write a simple program. However, the C programming language was much easier to use to write programs, and it was possible to run a program on different machines without having to rewrite the code. The UNIX operating system was rewritten in the C programming language, and by the late-1970s, the UNIX operating system ran on different hardware platforms, something that the computing world had never seen until that time. Hence, people called UNIX a portable operating system.

Unfortunately, the company Ken Thompson and Dennis Ritchie worked for (AT&T) was restricted by a federal court order from marketing UNIX. In an attempt to keep UNIX viable, AT&T sold the UNIX source code to several companies, encouraging them to agree to standards among them. Each of these companies developed its own variety, or flavor, of UNIX yet adhered to standards agreed upon by all. AT&T also gave free copies of the UNIX source code to certain universities to promote widespread development of UNIX. One result was a UNIX version developed at the University of California, Berkeley in the early 1980s known as BSD (Berkeley Software Distribution). In 1982, one of the companies to whom AT&T sold UNIX source code (Sun Microsystems) marketed UNIX on relatively inexpensive hardware and sold thousands of computers that ran UNIX to companies and universities.

Throughout the 1980s, UNIX found its place primarily in large corporations that had enough money to purchase the expensive computing equipment needed to run UNIX (usually a DEC PDP-11, VAX, or Sun Microsystems computer). A typical UNIX system in the 1980s could cost over $100,000, yet it performed thousands of tasks for client computers (also known as dumb terminals). Today, UNIX still functions in that environment; many large companies employ different flavors of UNIX for their heavy-duty, mission-critical tasks, such as e-commerce and database hosting. Common flavors of UNIX today include BSD, Hewlett-Packard’s HP-UX, IBM’s AIX, as well as Apple’s macOS and iOS operating systems.

UNIX
The UNIX operating system has roots running back to 1965, when the Massachusetts Institute of Technology (MIT), General Electric, and AT&T Bell Laboratories began developing an operating system called Multiplexed Information and Computing Service (MULTICS). MULTICS was a test project intended to reveal better ways of developing time-sharing operating systems, in which the operating system regulates the amount of time each process has to use the processor. The project was abandoned in 1969. However, Ken Thompson, who had worked on the MULTICS operating system, continued to experiment with operating systems. In 1969, he developed an operating system called UNIX that ran on the DEC (Digital Equipment Corporation) PDP-7 computer.

Shortly thereafter, Dennis Ritchie invented the C programming language that was used on Ken Thompson’s UNIX operating system. The C programming language was a revolutionary language. Most programs at the time needed to be written specifically for the hardware of the computer, which involved referencing volumes of information regarding the hardware in order to write a simple program. However, the C programming language was much easier to use to write programs, and it was possible to run a program on different machines without having to rewrite the code. The UNIX operating system was rewritten in the C programming language, and by the late-1970s, the UNIX operating system ran on different hardware platforms, something that the computing world had never seen until that time. Hence, people called UNIX a portable operating system.

Unfortunately, the company Ken Thompson and Dennis Ritchie worked for (AT&T) was restricted by a federal court order from marketing UNIX. In an attempt to keep UNIX viable, AT&T sold the UNIX source code to several companies, encouraging them to agree to standards among them. Each of these companies developed its own variety, or flavor, of UNIX yet adhered to standards agreed upon by all. AT&T also gave free copies of the UNIX source code to certain universities to promote widespread development of UNIX. One result was a UNIX version developed at the University of California, Berkeley in the early 1980s known as BSD (Berkeley Software Distribution). In 1982, one of the companies to whom AT&T sold UNIX source code (Sun Microsystems) marketed UNIX on relatively inexpensive hardware and sold thousands of computers that ran UNIX to companies and universities.

Throughout the 1980s, UNIX found its place primarily in large corporations that had enough money to purchase the expensive computing equipment needed to run UNIX (usually a DEC PDP-11, VAX, or Sun Microsystems computer). A typical UNIX system in the 1980s could cost over $100,000, yet it performed thousands of tasks for client computers (also known as dumb terminals). Today, UNIX still functions in that environment; many large companies employ different flavors of UNIX for their heavy-duty, mission-critical tasks, such as e-commerce and database hosting. Common flavors of UNIX today include BSD, Hewlett-Packard’s HP-UX, IBM’s AIX, as well as Apple’s macOS and iOS operating systems.

Linux
Although Richard Stallman started the GNU Project to make a free operating system, the GNU operating system never took off. Much of the experience gained by hackers developing the GNU Project was later pooled into Linux. A Finnish student named Linus Torvalds first developed Linux in 1991 when he was experimenting with improving MINIX (Mini-UNIX, a small educational version of UNIX developed by Andrew Tannenbaum) for the Intel x86 platform. The Intel x86 platform was fast becoming standard in homes and businesses around the world and was a good choice for any free development at the time. The key feature of the Linux operating system that attracted the development efforts of the hacker culture was that Torvalds had published Linux under the GNU Public License.

Since 1991, when the source code for Linux was released, the number of software developers dedicated to improving Linux has increased each year. The Linux kernel was developed collaboratively and was centrally managed; however, many Linux add-on packages were developed freely worldwide by those members of the hacker culture who were interested in their release. Linux was a convenient focal point for free software developers. During the early-to-mid-1990s, Linux development proceeded at full speed, with hackers contributing their time to what turned into a large-scale development project. All of this effort resulted in several distributions of Linux. A distribution of Linux is a collection or bundle of software containing the commonly developed Linux operating system kernel and libraries, combined with add-on software specific to a certain use. Well-known distributions of Linux include Red Hat, openSUSE, Debian, Ubuntu, Gentoo, Linux Mint, and Arch.

This branding of Linux did not imply the fragmentation that UNIX experienced in the late-1980s. All distributions of Linux had a common kernel and utilities. Their blend of add-on packages simply made them look different on the surface. Linux still derived its usefulness from collaborative development.

Linux development continued to expand throughout the late-1990s as more developers grew familiar with the form of collaborative software development advocated by the hacker culture. By 1998, when the term “OSS” first came into use, there were already many thousands of OSS developers worldwide. Small companies formed to offer Linux solutions for business. People invested in these companies by buying stock in them. Unfortunately, this trend was short-lived. By the year 2000, most of these companies had vanished. At the same time, the OSS movement caught the attention and support of large companies (such as IBM, Compaq, Dell, and Hewlett-Packard), and there was a shift in Linux development over the following decade to support the larger computing environments and mobile devices.

It is important to note that Linux is a by-product of OSS development. Recall that the OSS developers are still members of the hacker culture and, as such, are intrinsically motivated to develop software that has an important use. Thus, OSS development has changed over time; in the 1980s, the hacker culture concentrated on developing Internet and programming tools, whereas in the 1990s, it focused on developing the Linux operating system. Since 2000, interest has grown in embedded Linux (Linux OSes that run on smaller hardware devices such as mobile devices) and developing cloud- and security-focused application programs for use on the Linux operating system. Because Linux is currently very well developed, even more application development can be expected from the OSS community in the next decade.

Note 
For more information on the free software movement and the development of Linux, watch the 2001 television documentary Revolution OS (available on YouTube). It features interviews with Linus Torvalds, Richard Stallman, and Eric S. Raymond.

Linux Distributions
It is time-consuming and inefficient to obtain Linux by first downloading and installing the Linux kernel and then adding desired OSS packages. Instead, it’s more common to download a distribution of Linux containing the Linux kernel, common function libraries, and a series of OSS packages.

Note 
Remember that although Linux distributions appear different on the surface, they run the same kernel and contain many of the same packages.

Despite the fact that varied distributions of Linux are essentially the same under the surface, they do have important differences. Different distributions might support different hardware platforms. Also, Linux distributions include predefined sets of software; some Linux distributions include many server-related tools, such as Web servers and database servers, whereas others include numerous workstation and development software applications. Still others might include a complete set of open source tools that you can use to customize a Linux system to perform specific functions. In that case, you simply choose the open source tools you want to install. For example, you might choose to install a database server.

While Linux distributions use the same Linux kernel versions that are community developed, they can modify those kernels in order to provide fixes and optimizations that are specific to the distribution and used for long-term support. These are called distribution kernels, and list a patch version and distribution identifier following the major, minor, and revision number. For example, the 4.17.6-100.fc28.x86_64 kernel is distribution release 100 of the Linux 4.17.6 production kernel used on a 64-bit (x86_64) version of the Fedora Linux 28 distribution (fc28).

Linux distributions that include many specialized tools might not contain a GUI; an example of this is a Linux distribution that fits within a single small flash memory chip and can be used as a home Internet router. Most distributions, however, do include a GUI that can be further customized to suit the needs of the user.

The core component of the GUI in Linux is referred to as X Windows. The original implementation of X Windows on Linux was called XFree86 but has since been replaced by X.org and Wayland. X.org is the latest implementation of X Windows based on the original MIT X Windows project that was released as OSS in 2004, and Wayland is an alternative to X.org that was designed to be easier to develop and maintain. In addition to X Windows, several Linux window managers and desktop environments are available, which together affect the look and feel of the Linux GUI. X Windows in combination with a window manager and desktop environment is referred to as a GUI environment. The two main competing GUI environments available in Linux are the GNU Network Object Model Environment (GNOME) and the K Desktop Environment (KDE). These two GUI environments are more or less comparable in functionality, although users might have a personal preference for one desktop over the other. This is often the case when a company wants to do a great deal of software development in the GUI environment; the GNOME desktop, written in the C programming language, uses the widely available gtk toolkit, whereas the KDE desktop, written in the C++ programming language, uses the qt toolkit. The language and toolkit that best fits a company’s need will be the one preferred at that time. Most common Linux distributions ship with both GNOME and KDE GUI environments, whereas others offer support for both so that either GUI environment can be easily downloaded and installed. Figures 1-5 and 1-6 compare these two GUI environments.

Figure 1-5

Details
The GNOME desktop

Figure 1-6

The KDE desktop

Note 
In addition to GNOME and KDE, several other desktop environments are available to Linux systems. One example is Cinnamon, which is a desktop derived from GNOME that is particularly easy to use. Another example is XFCE, which is a lightweight desktop environment designed for Linux systems with few CPU and RAM resources.

Although the differences between Linux distributions can help narrow the choice of Linux distributions to install, one of the most profound reasons companies choose one distribution over another is support for package managers. A package manager is a software system that installs and maintains software. It keeps track of installed software, requires a standard format and documentation, and can manage and remove software from a system by recording all relevant software information in a central software database on your computer.

Note 
A package manager in Linux is similar to the Apps and Features or Programs and Features section in the Windows Settings or Control Panel apps, respectively.

One of the most widely supported package managers is the Red Hat Package Manager (RPM). Most Linux software is available in RPM format, and the RPM is standard on many Linux distributions that were originally derived from the Red Hat Linux distribution. The Debian Package Manager (DPM) is also very common today; it offers the same advantages as the RPM but for systems that were originally derived from the Debian Linux distribution. Other, less common package managers available for Linux include Pacman (Arch Linux), Zypper (openSUSE Linux), and Portage (Gentoo Linux). In addition to obtaining software in package manager format, you can download software in tarball format. A tarball is a compressed archive of files, like WinZip or RAR files, usually containing scripts that install the software contents to the correct location on the system, or source code that can be compiled into a working program and copied to the system. Unfortunately, tarballs do not update a central software database and, as a result, are very difficult to manage, upgrade, or remove from the system. Traditionally, most Linux software was available in tarball format, but package managers have since become the standard method for installing software.

Note 
For a list of common Linux distributions, visit www.linux.org.

Anyone can create a Linux distribution by packaging OSS with the Linux kernel. As a result, over 500 Linux distributions are publicly registered. Many are small, specialized distributions designed to fulfill certain functions, but some are mainstream Linux distributions used widely throughout the computing world. Typically, a distribution is associated with a website from which the distribution can be downloaded for free. In addition, most Linux distributions can be obtained from other websites, such as iso.linuxquestions.org. Some distributions of Linux are also available on DVDs for a small fee from computer stores and websites; however, downloading from the Internet is the most common method of obtaining Linux.

Table 1-4 briefly describes some mainstream Linux distributions, their features, and where to find them on the Internet.

Table 1-4 Common Linux Distributions
Distribution	Features	Platforms	Location
Red Hat

One of the earliest Linux distributions, and one that is commonly used within organizations today. Two distributions of Red Hat are available: the Enterprise distribution geared for enterprise environments and the Fedora distribution geared for all environments (servers, desktops, laptops, etc.). Both editions ship with the RPM package manager and GNOME as the default desktop environment.

x86/x64

PPC/POWER

Mainframe

ARM

www.redhat.com getfedora.org

openSUSE

Originally developed primarily in Europe, openSUSE is the oldest business-focused Linux distribution. Novell purchased the distribution to replace its NetWare OS and distributes enterprise versions of openSUSE.

x86/x64

PPC/POWER

ARM

www.opensuse.org www.suse.com

Debian

Offering the largest number of customization options of all Linux distributions, Debian Linux contains software packages for any use and supports nearly all languages and hardware platforms.

x86/x64

PPC/POWER

Mainframe

ARM

MIPS

www.debian.org

Ubuntu

A Debian-based distribution that is widely used in all environments, Ubuntu is designed to be easy to use and supports nearly all hardware, including mobile computing devices.

x86/x64

PPC/POWER

Mainframe

ARM

www.ubuntu.com

Gentoo

A Linux distribution that focuses on hardware and software optimization. Each program and part of the operating system on a Gentoo system is compiled specifically for the hardware on the particular system and optimized for that hardware during the process.

x86/x64

PPC/POWER

ARM

www.gentoo.org

Linux Mint

A relatively new and easy-to-use Linux distribution that is focused on providing desktop and mobile user capabilities.

x86/x64

www.linuxmint.com

Arch

A very lightweight and customizable Linux distribution. Due to its focus on simplicity and customization, it is often used within specialized environments and on small footprint systems.

x86/x64

www.archlinux.org

Key Terms
AIX
Android
application (app)
Arch
artistic license
asymmetric encryption
authentication
Beowulf clustering
BSD (Berkeley Software Distribution)
certificate
Certification Authority (CA)
closed source software
cloud
cloud platform
cloud provider
cluster
clustering
container
cracker
cybersecurity
database
Database Management System (DBMS)
Debian
developmental kernel
device driver
digital signature
distribution
distribution kernel
Domain Name Space (DNS)
Dynamic Host Configuration Protocol (DHCP)
flavor
Free Software Foundation (FSF)
freeware
frequently asked questions (FAQs)
fully qualified domain name (FQDN)
Gentoo
GNU
GNU General Public License (GPL)
GNU Network Object Model Environment (GNOME)
GNU Project
graphical user interface (GUI)
GUI environment
hacker
hardware
hardware platform
hot fix
HOWTO
HP-UX
Infrastructure as a Service (IaaS)
Internet Protocol (IP) address
iOS
K Desktop Environment (KDE)
Kerberos
kernel
key
Linus Torvalds
Linux
Linux Documentation Project (LDP)
Linux Mint
Linux User Group (LUG)
load balancing
macOS
Mail Delivery Agent (MDA)
Mail Transfer Agent (MTA)
Mail User Agent (MUA)
major number
Message Passing Interface (MPI)
MINIX
minor number
Multiplexed Information and Computing Service (MULTICS)
multitasking
multiuser
Network Time Protocol (NTP)
newsgroup
Next Generation Firewall (NGFW)
Open Source Software (OSS)
openSUSE
operating system (OS)
package manager
penetration test
Platform as a Service (PaaS)
private cloud
private key
process
production kernel
program
programming language
proxy server
public key
Public Key Infrastructure (PKI)
Red Hat
revision number
router
scalability
search engine
security appliance
server
server services
shareware
software
Software as a Service (SaaS)
source code
symmetric encryption
system service
tarball
total cost of ownership (TCO)
Ubuntu
Unified Threat Management (UTM)
UNIX
user
user interface
vulnerability assessment
workstation
workstation services
X Windows


Chapter 2. Linux Installation and Usage
After completing this chapter, you will be able to:

Prepare for and install Fedora Linux using good practices
Outline the structure of the Linux interface
Enter basic shell commands and find command documentation
Properly shut down the Linux operating system
This chapter explores the concepts and procedures needed to install a Fedora Linux system. The latter half of the chapter presents an overview of the various components that you will use when interacting with the operating system, as well as how to enter basic shell commands, obtain help, and properly shut down the Linux system.

Installing Linux
Installing Linux requires careful planning as well as configuring parts of the Linux operating system via an installation program.

Preparing for Installation
An operating system is merely a series of software programs that interact with and control the computer hardware. Thus, all operating systems have a certain minimum set of computer hardware requirements to function properly. Although most up-to-date hardware is sufficient to run the Linux operating system, it is, nonetheless, important to ensure that a computer meets the minimum hardware requirements before performing an installation.

These minimum installation requirements can be obtained from several sources. If you obtained the operating system on DVD, a printed manual or file on the DVD might specify these requirements. You can also find the minimum hardware requirements for most operating systems on the vendor’s website. For the Fedora 28 Linux operating system, you can find the minimum hardware requirements at docs.fedoraproject.org or in Table 2-1.

Table 2-1 Fedora 28 Recommended Minimum Hardware Requirements
Type of hardware	Requirement
Central processing unit (CPU)	1GHz or faster Intel x64 CPU
Random access memory (RAM)	1GB
Free disk space (permanent storage)	10GB free space
Additional drives	DVD drive (for DVD-based installation)
Peripheral devices	Fedora-compliant peripheral devices (e.g., video cards, sound cards, network cards)
Furthermore, each operating system supports only particular types of hardware components. While modern Linux distributions support nearly all hardware components available on the market, it’s good practice to check a hardware vendor’s website to see whether uncommon components are compatible with Linux. Additionally, many Linux distribution websites and computer manufacturer websites have a Linux Hardware Compatibility List (HCL) that identifies whether the computer and hardware have Linux driver support.

Note 
You can visit linux-drivers.org to locate hardware and distribution-specific HC

Understanding Installation Media
Before performing a Linux installation, you must choose the source of the Linux packages and the installation program itself. The most common source of these packages is DVD media.

To install from DVD, you place the Linux DVD in the DVD drive and turn on the computer. Most computers automatically search for a startup program on the DVD immediately after being turned on; the computer can then use the DVD to start the Linux installation. Alternatively, most modern computers allow you to manually select the boot device using a special manufacturer-specific key, such as F12, during the startup sequence.

Note 
Turning on a computer to load an operating system is commonly referred to as booting a computer. Because the Linux installation program on DVD can be loaded when you first turn on the computer, it is referred to as a bootable DVD.

Nearly all Linux distributions provide a website from which you can download DVD images (called ISO images) that have an .iso file extension. These ISO images can be written to a blank writable DVD using disc burning software on a Windows, Linux, or Macintosh computer and then used to boot your computer to start the Linux installation.

In addition to a standard Linux installation DVD image, many Linux distribution websites allow you to download a bootable live media DVD image. If you write a live media DVD image to a blank DVD and boot your computer with it, a fully functional graphical Linux operating system that you can use will be loaded into RAM. This allows you to test the operating system on your computer to ensure that all hardware drivers were detected properly before installing it to permanent storage, such as hard disk or solid-state drive (SSD). After you are satisfied with the functionality of your Linux system loaded from live media, you can select the appropriate icon on the desktop to start the installation program that will install the Linux system to permanent storage.

Note 
To obtain a standard DVD image or live media DVD image of Fedora 28, you can visit getfedora.org.

If your computer does not have a DVD drive, you can still install Linux by imaging the standard DVD or live media DVD ISO image to a USB flash drive, provided that your computer supports booting from USB flash drive media. To make this process easier, many distributions provide a program and instructions that can be used to perform the imaging process. For Fedora Linux, you can download and install the Fedora Media Writer tool on a Windows or Macintosh system, as shown in Figure 2-1. If you click Custom image, the Fedora Media Writer tool allows you to download the latest version of Fedora Workstation or Fedora Server as well as image an already downloaded DVD ISO image of Fedora to a USB flash drive. After clicking Custom image, you select the appropriate DVD ISO image and target USB flash drive, and then click Write to disk to start the imaging process.

Figure 2-1

Details
The Fedora Media Writer tool

Note 
You can download the Fedora Media Writer tool from getfedora.org/workstation/download/.

After the imaging process is complete, you can insert your USB flash drive into a free USB slot, turn on your computer, use the appropriate key for your computer to select the target boot device (e.g., F12), and then choose the option to boot from your USB flash drive.

Many server and workstation computers today can run multiple operating systems concurrently using virtualization software. Several virtualization software products are available on the market today, including:

Microsoft Hyper-V

VMWare

Oracle VM VirtualBox

Each operating system that is run within virtualization software is called a virtual machine, and the underlying operating system running the virtualization software is called the virtual machine host. Figure 2-2 shows a Fedora Linux virtual machine running on the Windows 10 operating system using the Microsoft Hyper-V virtualization software.

Figure 2-2

Details
Fedora Linux running as a virtual machine on the Windows 10 operating system

Note 
Most enterprise environments today take advantage of virtualization software to run multiple server operating systems concurrently on the same computer hardware. This allows organizations to better utilize their server hardware and reduce costs. Chapter 6 discusses this use of virtualization.

To install Linux as a virtual machine, you need to download the standard DVD or live media DVD ISO image to a directory on your virtual machine host (e.g., Windows). When you open the virtualization software and choose to create a new virtual machine, you can specify the file location of the appropriate ISO image, and the virtualization software will boot from the ISO image directly, without requiring you to write the ISO image to a DVD or a USB flash drive. Figure 2-3 shows the section of the Hyper-V New Virtual Machine Wizard that allows you to specify the location of an ISO image containing the Fedora 28 installation media.

Figure 2-3

Details
Selecting installation media within the Hyper-V New Virtual Machine Wizard

Performing the Installation
Installing the Linux operating system involves interacting with an installation program, which prompts you for information regarding the nature of the Linux system being installed. More specifically, the installation program for Fedora 28 Linux involves the following general stages:

Starting the installation

Choosing an installation language, localization, and system options

Configuring disk partitions and filesystems

Configuring user accounts

Starting the Installation
As mentioned earlier, to perform an installation of Fedora Linux, you can boot your computer using Fedora installation media. If you are booting your system from standard Fedora installation media, you will be prompted to start the installation or perform troubleshooting actions. However, if you boot your system from Fedora live media, you will instead be prompted to start a live Fedora system (which later allows you to install Fedora), test your installation media, and start a live Fedora system or perform troubleshooting actions, as shown in Figure 2-4.

Figure 2-4

Details
Beginning a Fedora installation

If you select the Troubleshooting option shown in Figure 2-4, you will be presented with four additional options, as shown in Figure 2-5.

Figure 2-5

Details
Fedora installation troubleshooting options

Selecting Start Fedora-Workstation-Live 28 in basic graphics mode shown in Figure 2-5 will start the Fedora system with generic video drivers and low resolution, which is useful if the live Fedora system doesn’t detect the correct driver for your video card and cannot display a graphical desktop as a result.

Defective RAM is a common cause of a failed Linux installation. If you select Run a memory test shown in Figure 2-5, the memtest86 utility will start and perform a thorough check of your RAM for hardware errors, as shown in Figure 2-6.

Figure 2-6

Details
The memtest86 utility

The Boot from local drive option shown in Figure 2-5 is useful if you forget to remove your Fedora installation media after the installation has completed and your system is configured to boot an operating system from your DVD or USB drive before searching for an operating system on permanent storage.

In most cases, the troubleshooting options shown in Figure 2-5 aren’t necessary when installing Fedora Linux. As a result, you can choose Start Fedora-Workstation-Live 28 shown in Figure 2-4 to start a live Fedora system. After the live Fedora system has loaded, you will be presented with a welcome screen that prompts you to install Fedora Linux on permanent storage or continue using the live Fedora system loaded from your installation media, as shown in Figure 2-7.

Figure 2-7

Details
The Welcome to Fedora screen

If you choose Install to Hard Drive in Figure 2-7, the Fedora installation program will start. Alternatively, if you choose Try Fedora, you will be able to explore the desktop of a live Fedora system and can later select Install to Hard Drive from the Activities menu in the upper-left corner of the desktop to start the Fedora installation program.

Choosing an Installation Language, Localization, and System Options
After you start a Fedora installation, you will be prompted to select a language that is used during the installation program, as shown in Figure 2-8. If you click Continue, you will be prompted to configure the date and time, keyboard layout, and installation destination (e.g., hard disk, SSD) as shown in Figure 2-9.

Figure 2-8

Details
Selecting an installation language

Figure 2-9

Details
Configuring localization and system options

You can click each of the localization and system option icons in Figure 2-9 to modify the configuration. By default, your keyboard layout is automatically detected, your network interface is set to obtain network configuration using the DHCP protocol, and the date and time are obtained from the Internet if your network has Internet connectivity. However, you must manually review the installation destination settings before the installation can continue (hence the warning shown in Figure 2-9). The installation destination is a permanent storage device that will contain the Linux operating system; it is often a hard disk or an SSD.

Older systems often use Parallel Advanced Technology Attachment (PATA) hard disks that physically connect to the computer in one of four configurations. As shown in Table 2-2, Linux refers to each of these disks according to its configuration on your computer.

Table 2-2 PATA Hard Disk Configurations
Description	Linux name
Primary master PATA hard disk	hda
Primary slave PATA hard disk	hdb
Secondary master PATA hard disk	hdc
Secondary slave PATA hard disk	hdd
Note 
In the past, PATA hard disks were referred to as Integrated Drive Electronics (IDE) or Advanced Technology Attachment (ATA) hard disks.

Note 
You can verify your PATA disk configuration by accessing your computer’s Basic Input/Output System (BIOS) configuration. You can access your BIOS configuration by pressing the appropriate manufacturer-specific key, such as F10, during system startup.

Newer systems typically use Serial Advanced Technology Attachment (SATA) hard disks or SSDs or Non-Volatile Memory Express (NVMe) SSDs. Server systems have traditionally used Small Computer Systems Interface (SCSI) hard disks, and many server systems today use Serial Attached SCSI (SAS) hard disks or SSDs. Unlike PATA, you can have more than four SATA, NVMe, SCSI, and SAS hard disks or SSDs within a system. The first SATA/SCSI/SAS hard disk or SSD is referred to as sda, the second SATA/SCSI/SAS hard disk or SSD is referred to as sdb, and so on. The first NVMe SSD is referred to as nvme0, the second NVMe SSD is referred to as nvme1, and so on.

If you click the installation destination icon shown in Figure 2-9, you will be presented with a list of the permanent storage devices in your system. The system shown in Figure 2-10 contains a single SATA hard disk (sda).

Figure 2-10

Details
Configuring an installation destination

If you have multiple disk devices, you can select the disk that will be used to contain the Fedora Linux operating system and then click Done. Normally, this is a local hard disk or SSD, but you can also install Linux on an external iSCSI or FCoE Storage Area Network (SAN), Direct Access Storage Device (DASD), Multipath IO (MPIO), or firmware Redundant Array of Inexpensive Disks (RAID) device if you select Add a disk shown in Figure 2-10 and supply the appropriate configuration information.

Configuring Disk Partitions and Filesystems
Regardless of type, each hard disk or SSD is divided into sections called partitions. Before you can store files in a partition, you must format it with a filesystem. A filesystem is a structure that specifies how data should reside on the hard disk or SSD itself.

Note 
In the Windows operating system, each drive letter (e.g., C:, D:, E:) can correspond to a separate filesystem that resides on a partition on your hard disk or SSD.

There are limits to the number and types of partitions into which a hard disk or an SSD can be divided. By default, you can create a maximum of four major partitions (called primary partitions). To overcome this limitation, you can optionally label one of these primary partitions as “extended”; this extended partition can then contain an unlimited number of smaller partitions called logical drives. Each logical drive within the extended partition and all other primary partitions can contain a filesystem and be used to store data. The table of all partition information for a certain hard disk or an SSD is stored in the first readable sector outside all partitions; it is called the Master Boot Record (MBR). The MBR is limited to devices that are less than 2TB in size. Newer devices and devices larger than 2TB use a GUID Partition Table (GPT) instead of an MBR.

Note 
The MBR and GPT are functionally equivalent.

Recall that, in Linux, the first SATA/SCSI/SAS device in your system is referred to as sda, the first primary partition on this device is labeled sda1, the second sda2, and so on. Because only four primary partitions are allowed on an MBR disk, logical drives inside the extended partition are labeled sda5, sda6, and so on. An example of this partition strategy is listed in Table 2-3.

Table 2-3 Example MBR Partitioning Scheme for the First SATA/SCSI/SAS Device
Description	Linux name	Windows name
First primary partition on the first SATA/SCSI/SAS device	sda1	C:
Second primary partition on the first SATA/SCSI/SAS device	sda2	D:
Third primary partition on the first SATA/SCSI/SAS device	sda3	E:
Fourth primary partition on the first SATA/SCSI/SAS device (EXTENDED)	sda4	F:
First logical drive in the extended partition on the first SATA/SCSI/SAS device	sda5	G:
Second logical drive in the extended partition on the first SATA/SCSI/SAS device	sda6	H:
Third logical drive in the extended partition on the first SATA/SCSI/SAS device	sda7	I:
Note 
For the primary master PATA device, replace sda with hda in Table 2-3. NVMe devices can use namespace divisions in addition to partitions. As a result, the first NVMe partition (p1) within the first namespace (n1) on the first NVMe SSD (nvme0) would be called nvme0n1p1.

Note 
For devices that use a GPT instead of an MBR, there is no primary partition limitation, and hence no need for extended partitions or logical drives. The sda1 through sda7 partitions shown in Table 2-3 would refer to the first through seventh partitions on a GPT device.

Partitioning divides a hard disk into sections, each of which can contain a separate filesystem used to store data. Each of these filesystems can then be accessed by Linux if it is attached (or mounted) to a certain directory. When data is stored in that particular directory, it is physically stored on the respective filesystem on the hard disk. The Fedora installation program can automatically create partitions based on common configurations; however, it is generally good practice to manually partition to suit the needs of the specific Linux system.

At minimum, Linux typically requires only two partitions to be created: a partition that is mounted to the root directory in Linux (/) and that can contain all of the files used by the operating system, applications, and users, and a partition used for virtual memory (also known as swap memory). Virtual memory consists of an area on the hard disk or SSD that can be used to store information that would normally reside in physical memory (RAM) if the physical memory was being used excessively. When programs are executed that require a great deal of resources on the computer, information is continuously swapped from physical memory to virtual memory, and vice versa. Traditionally, Linux swap partitions were made to be at least the size of the physical RAM in the computer; however, they can be much larger if the Linux system is intended to run large applications. A swap partition does not contain a filesystem and is never mounted to a directory because the Linux operating system is ultimately responsible for swapping information.

Although you might choose to create only root and swap partitions, extra partitions make Linux more robust against filesystem errors. For example, if the filesystem on one partition encounters an error, only data on one part of the system is affected and not the entire system (i.e., other filesystems). Because some common directories in Linux are used vigorously and as a result are more prone to failure, it is good practice to mount these directories to their own filesystems. Additionally, having a /boot partition allows your system to be recovered easily in the event of a boot-related issue, and is often a requirement if your / partition uses a particular technology that is not directly bootable, such as Logical Volume Manager (LVM) or B-tree Filesystem (BTRFS). Consequently, most Linux installation programs will create a /boot partition if you select the option to automatically have partitions created for you. Table 2-4 lists directories that are commonly mounted to separate partitions as well as their recommended sizes.

Table 2-4 Common Linux Filesystems and Sizes
Directory	Description	Recommended size
/	Contains all directories not present on other filesystems	Depends on the size and number of other filesystems present, but is typically 20GB or more
/boot	Contains the Linux kernel and boot files	1GB
/home	Default location for user home directories	500MB per user
/usr	System commands and utilities	Depends on the packages installed—typically 30GB or more
/usr/local	Location for most additional programs	Depends on the packages installed—typically 30GB or more
/opt	An alternate location for additional programs	Depends on the packages installed—typically 30GB or more
/var	Contains log files and spools	Depends on whether the Linux system is used as a print server (which contains a large spool). For print servers, 10GB or more is typical. For other systems, 2GB or more is usually sufficient.
/tmp	Holds temporary files created by programs	1GB
Each of these filesystems can be of different types. The most common types used today are the ext2, ext3, ext4, VFAT, and XFS filesystems, although Linux can support upward of 50 filesystems. Each filesystem essentially performs the same function, which is to store files on a partition; however, each filesystem offers different features and is specialized for different uses. The ext2 filesystem is the traditional filesystem, and the Virtual File Allocation Table (VFAT) filesystem is compatible with the FAT and FAT32 filesystems in Windows. The ext3, ext4, and XFS filesystems, however, are much more robust than the ext2 and VFAT filesystems, as they perform a function called journaling. A journaling filesystem uses a journal to keep track of the information written to the hard disk. If you copy a file on the hard disk from one directory to another, that file must pass into RAM and then be written to the new location on the hard disk. If the power to the computer is turned off during this process, information might not be transmitted as expected and data might be lost or corrupted. With a journaling filesystem, each step required to copy the file to the new location is first written to a journal; this means the system can retrace the steps the system took prior to a power outage and complete the file copy. These filesystems also host a variety of additional improvements compared to ext2 and VFAT, including faster data transfer and indexing, which makes them common choices for Linux servers today.

After you have selected the appropriate hard disk or SSD as an installation destination during the Fedora Linux installation shown in Figure 2-10, you must also choose whether the installation program should create partitions for you (Automatic), or whether you want to configure partitions manually (Custom) or manually using an advanced interface (Advanced Custom). If you choose Custom shown in Figure 2-10 and click Done, you will be prompted to choose the partitioning scheme, and optionally create a default partition layout that you can modify to suit your needs. If you choose a Standard partitioning scheme, this default partition layout will consist of a /boot and a / partition with an ext4 filesystem (not encrypted), as well as a swap partition that is the same size as the RAM in your computer, as shown in Figure 2-11.

Figure 2-11

Details
Configuring disk partitions and filesystems

In addition to the standard partitions that we have discussed already, you can instead choose a partition scheme that creates logical volumes for your Linux filesystems using the Logical Volume Manager (LVM) or partitions that support the new B-tree Filesystem (BTRFS). LVM and BTRFS are discussed in Chapters 5 and 6.

To allow for easier system recovery, it is good form to choose a standard partition scheme and ensure that contents of disk partitions are not encrypted.

Note 
If your system has a Unified Extensible Firmware Interface (UEFI) BIOS on the motherboard, the installation program will also create a small UEFI System Partition to store boot-related information.

Note 
If your system has a hard disk or an SSD that uses a GPT instead of an MBR and does not contain a UEFI BIOS on the motherboard, the installation program will also create a small BIOS Boot partition to store boot-related information.

After you are satisfied with your partition and filesystem configuration, you can click Done shown in Figure 2-11, confirm your changes in the dialog box that appears, and return to the localization and system options configuration screen shown in Figure 2-9. If you click the Begin Installation button in Figure 2-9, partition changes will be written to your storage device and the Fedora system packages will be installed from the installation media to the appropriate filesystems. This process will take some time to complete. After it has completed, you can click the Quit button that appears to exit the installation program. Next, you can shut down your live Fedora system, remove the installation media from your computer, and boot your computer into your new Fedora system.

Configuring User Accounts
All Linux systems require secure authenticated access, which means that each user must log in with a valid user name and password before gaining access to a user interface. Two user accounts must be created at minimum: the administrator account (root), which has full rights to the system, as well as a regular user account; the root user account should only be used when performing system administration tasks.

On the first boot following a Fedora installation, you will be required to complete a Welcome wizard. This wizard allows you to disable location services and automatic error reporting, optionally integrate your online social media accounts, as well as configure a single regular user account for yourself, as shown in Figure 2-12. You can optionally click Set Up Enterprise Login in Figure 2-12 to join your Linux system to an Active Directory or Kerberos domain. After you click Next in Figure 2-12, you will be prompted to supply a password for your newly created user account and the Welcome wizard will then continue to boot into your system.

Figure 2-12

Details
Choosing a regular user account username during the Welcome wizard

By default, a root user is created during the installation process but not assigned a valid password. The regular user account that you create during the Welcome wizard is given special rights that allow you set a valid password for the root user using the sudo password root command following installation.

Basic Linux Usage
After the Linux operating system has been installed, you must log in to the system with a valid user name and password and interact with the user interface to perform tasks. To do this, it is essential to understand the Linux user interfaces, as well as basic tasks, such as command execution, obtaining online help, and shutting down the Linux system.

Shells, Terminals, and the Kernel
Recall that an operating system is merely a collection of software that allows you to use your computer hardware in a meaningful fashion. Every operating system has a core component, which loads all other components and serves to centrally control the activities of the computer. This component is known as the kernel, and in Linux it is simply a file, usually called vmlinuz, that is located on the hard disk and loaded when you first turn on your computer.

When you interact with a computer, you are ultimately interacting with the kernel of the computer’s operating system. However, this interaction cannot happen directly; it must have a channel through which it can access the kernel as well as a user interface that passes user input to the kernel for processing. The channel that allows you to log in is called a terminal. Linux can have many terminals that allow you to log in to the computer locally or across a network. After you log in to a terminal, you receive a user interface called a shell, which then accepts your input and passes it to the kernel for processing. The shell that is used by default in Linux is the BASH shell (short for Bourne Again Shell), which is an improved version of the Bourne shell from AT&T and is the shell that is used throughout this book. The whole process looks similar to what is shown in Figure 2-13.

Figure 2-13

Details
Shells, terminals, and the kernel

As mentioned earlier, Linux is a multiuser and multitasking operating system and, as such, can allow for thousands of terminals. Each terminal could represent a separate logged-in user that has its own shell. The four different “channels” shown in Figure 2-13 could be different users logged in to the same Linux computer. Two users could be logged in locally to the server (seated at the server itself), and the other two could be logged in across a network, such as the Internet.

By default, when you log in to a terminal, you receive a command-line shell (BASH shell), which prompts you to type commands to tell the Linux kernel what to do. However, in this computing age, most people prefer to use a graphical interface in which they can use a pointing device such as a mouse to navigate and start tasks. In this case, you can choose to start a graphical user interface (GUI) environment on top of your BASH shell after you are logged in to a command-line terminal, or you can switch to a graphical terminal, which allows users to log in and immediately receive a GUI environment. A typical command-line terminal login prompt looks like the following:


A typical graphical terminal login for Fedora Linux (called the GNOME Display Manager or gdm) is shown in Figure 2-14.

Figure 2-14

Details
The GNOME display manager (gdm)

To access a terminal device at the local server, you can press a combination of keys, such as Ctrl+Alt+F2, to change to a separate terminal. If you are logging in across the network, you can use a variety of programs that connect to a terminal on the Linux computer. A list of local Linux terminals, along with their names and types, is shown in Table 2-5.

Table 2-5 Common Linux Terminals
Terminal name	Key combination	Login type
tty1	Ctrl+Alt+F1	graphical (gdm)
tty2	Ctrl+Alt+F2	command-line
tty3	Ctrl+Alt+F3	command-line
tty4	Ctrl+Alt+F4	command-line
tty5	Ctrl+Alt+F5	command-line
tty6	Ctrl+Alt+F6	command-line
After you are logged in to a command-line terminal, you receive a prompt at which you can enter commands. The following example shows the user logging in as the root (administrator) user. As you can see in this example, after you log in as the root user, you see a # prompt:


However, if you log in as a regular user to a command-line terminal (e.g., user1), you see a $ prompt, as follows:


When you log in to a graphical terminal, the GUI environment of your choice is started; the default GUI environment in Fedora Linux is GNOME on Wayland, but you can instead select GNOME on X.org from the settings icon next to the Sign In button within the gdm. On most legacy Linux systems, the GUI environment replaces the gdm on tty1. However, on modern Linux systems such as Fedora 28, the GUI environment is typically loaded on tty2, and the gdm remains available on tty1 to allow for additional graphical logins for different users. Each additional graphical login results in an additional GUI environment loaded on the next available terminal (tty3, tty4, and so on).

After the GUI environment starts, you can access a command-line terminal window by accessing the Activities menu in the upper left of the desktop and navigating to Show Applications, Utilities, Terminal. This will start a command-line terminal window within your GNOME desktop, as shown in Figure 2-15. You can open multiple, separate command-line terminal windows within a single GUI environment. Similarly, you can access several different BASH windows within a single command-line terminal (e.g., tty3) using an interactive terminal program such as screen or tmux. A sample tmux session with three BASH windows on tty3 is shown in Figure 2-16.

Figure 2-15

Details
Using a command-line terminal within the GNOME desktop

Figure 2-16

Accessing multiple BASH windows within a single command-line terminal using tmux

Note 
By default, Fedora 28 does not allow the root user to log in to a GUI environment from the gdm for security reasons, as several graphical programs are not designed to be run as the root user. Instead, you should log in to a GUI environment as a regular user. When you run a graphical administrative utility as a regular user, the GUI environment prompts you for the root user password in order to continue.

Basic Shell Commands
When using a command-line terminal, the shell ultimately interprets all information you enter into the command line. This information includes the command itself, as well as options and arguments. Commands indicate the name of the program to execute and are case sensitive. Options are specific letters that start with a dash (-) and appear after the command name to alter the way the command works. Options are specific to the command in question; the persons who developed the command determined which options to allow for that command.

Note 
Some options start with two dashes (--); these options are referred to as POSIX options and are usually composed of a whole word, not just a letter.

Arguments also appear after the command name, yet they do not start with a dash. They specify the parameters that tailor the command to your particular needs. Suppose, for example, that you want to list all of the files in the /etc/rpm directory on the hard disk. You could use the ls command with the –a option (which tells the ls command to list all files, including hidden files) and the /etc/rpm argument (which tells ls to look in the /etc/rpm directory), as shown in the following example:


After you type the command and press Enter in the preceding output, the ls command shows that there are six files in the /etc/rpm directory (macros.color, macros.fjava, and so on). The command prompt then reappears, so that you can enter another command.

Note 
Commands, options, and arguments are case sensitive; an uppercase letter (A), for instance, is treated differently than a lowercase letter (a).

Note 
Always put a space between the command name, options, and arguments; otherwise, the shell does not understand that they are separate, and your command might not work as expected.

Although you can pass options and arguments to commands, not all commands need to have arguments or options to work properly. The date command, which simply prints the current date and time, is an example:


Table 2-6 lists some common commands that you can use without specifying any options or arguments.

Table 2-6 Some Common Linux Commands
Command	Description
clear	Clears the terminal screen
reset	Resets your terminal to use default terminal settings
who	Displays currently logged-in users
w	Displays currently logged-in users and their tasks
whoami	Displays your login name
id	Displays the numbers associated with your user account name and group names; these are commonly referred to as User IDs (UIDs) and Group IDs (GIDs)
date	Displays the current date and time
cal	Displays the calendar for the current month
uname -a	Displays system information
ls	Lists files
exit	Exits out of your current shell
If the output of a certain command is too large to fit on the terminal screen, press the Shift+Page Up keys simultaneously to view previous screens of information. Press Shift+Page Down simultaneously to navigate in the opposite direction.

You can recall commands previously entered in the BASH shell using the keyboard arrow keys (the up, down, right, and left arrow keys). Thus, if you want to enter the same command again, cycle through the list of available commands with the keyboard up and down arrow keys and press Enter to re-execute that command.

As a Linux administrator, you will regularly run commands that only the root user can run to perform system configuration. Even if you are logged in to the system as a regular user account, you can easily switch to the root user to perform any administrative tasks using the su (switch user) command. To switch to the root user and load the root user’s environment variables, you can run the su command with the — option and supply the root user’s password when prompted:


Alternatively, to run a single command as the root only, you can run su –c “command” root and specify the root user’s password when prompted.

If you do not specify the user name when using the su command, the root user is assumed. Additionally, the root user can use the su command to switch to any other user account without specifying a password:

Shell Metacharacters
Another important part of the shell are shell metacharacters, which are keyboard characters that have special meaning. One of the most commonly used metacharacters is the $ character, which tells the shell that the following text refers to a variable. A variable is a piece of information that is stored in memory; variable names are typically uppercase words, and most variables are set by the Linux system when you log in. An example of how you might use the $ metacharacter to refer to a variable is by using the echo command (which prints text to the terminal screen):


Notice from the preceding output that $SHELL was translated into its appropriate value from memory (/bin/bash, the BASH shell). The shell recognized SHELL as a variable because it was prefixed by the $ metacharacter. Table 2-7 presents a list of common BASH shell metacharacters that are discussed throughout this book.

Table 2-7 Common BASH Shell Metacharacters
Metacharacter(s)	Description
$	Shell variable
~	Special home directory variable
#	Shell script comment
&	Background command execution
;	Command termination
< << > >>	Input/Output redirection
|	Command piping
* ? [ ]	Shell wildcards
’ " \	Metacharacter quotes
`	Command substitution
( ) { }	Command grouping
It is good practice to avoid metacharacters when typing commands unless you need to take advantage of their special functionality, as the shell readily interprets them, which might lead to unexpected results.

Note 
If you accidentally use one of these characters and your shell does not return you to the normal command prompt, press the Ctrl+c keys to cancel your current command and return to the normal command prompt.

In some circumstances, you might need to use a metacharacter in a command and prevent the shell from interpreting its special meaning. To do this, enclose the metacharacters in single quotation marks ´ ´. Single quotation marks protect those metacharacters from being interpreted specially by the shell (i.e., a $ is interpreted as a $ character and not a variable identifier). You can also use double quotation marks (" ") to perform the same task; however, double quotation marks do not protect $, \, and ` characters. If only one character needs to be protected from shell interpretation, you can precede that character by a slash \ rather than enclosing it within quotation marks. An example of this type of quoting follows:


As shown in Table 2-7, not all quotation characters protect characters from the shell. The back quotation characters ` ` can be used to perform command substitution; anything between back quotes is treated as another command by the shell, and its output is substituted in place of the back quotes. Take the expression ‛date‛ as an example:

Shell Metacharacters
Another important part of the shell are shell metacharacters, which are keyboard characters that have special meaning. One of the most commonly used metacharacters is the $ character, which tells the shell that the following text refers to a variable. A variable is a piece of information that is stored in memory; variable names are typically uppercase words, and most variables are set by the Linux system when you log in. An example of how you might use the $ metacharacter to refer to a variable is by using the echo command (which prints text to the terminal screen):


Notice from the preceding output that $SHELL was translated into its appropriate value from memory (/bin/bash, the BASH shell). The shell recognized SHELL as a variable because it was prefixed by the $ metacharacter. Table 2-7 presents a list of common BASH shell metacharacters that are discussed throughout this book.

Table 2-7 Common BASH Shell Metacharacters
Metacharacter(s)	Description
$	Shell variable
~	Special home directory variable
#	Shell script comment
&	Background command execution
;	Command termination
< << > >>	Input/Output redirection
|	Command piping
* ? [ ]	Shell wildcards
’ " \	Metacharacter quotes
`	Command substitution
( ) { }	Command grouping
It is good practice to avoid metacharacters when typing commands unless you need to take advantage of their special functionality, as the shell readily interprets them, which might lead to unexpected results.

Note 
If you accidentally use one of these characters and your shell does not return you to the normal command prompt, press the Ctrl+c keys to cancel your current command and return to the normal command prompt.

In some circumstances, you might need to use a metacharacter in a command and prevent the shell from interpreting its special meaning. To do this, enclose the metacharacters in single quotation marks ´ ´. Single quotation marks protect those metacharacters from being interpreted specially by the shell (i.e., a $ is interpreted as a $ character and not a variable identifier). You can also use double quotation marks (" ") to perform the same task; however, double quotation marks do not protect $, \, and ` characters. If only one character needs to be protected from shell interpretation, you can precede that character by a slash \ rather than enclosing it within quotation marks. An example of this type of quoting follows:


As shown in Table 2-7, not all quotation characters protect characters from the shell. The back quotation characters ` ` can be used to perform command substitution; anything between back quotes is treated as another command by the shell, and its output is substituted in place of the back quotes. Take the expression ‛date‛ as an example:

Changing Directories
When you log into a Linux system, you are placed in your home directory, which is a place unique to your user account for storing personal files. Regular users usually have a home directory named after their user account under the /home directory, as in /home/sue. The root user, however, has a home directory called root under the root directory of the system (/root), as shown in Figure 3-2. Regardless of your user name, you can always refer to your own home directory using the ~ metacharacter.

To confirm the system directory that you are currently in, simply observe the name at the end of the shell prompt or run the pwd (print working directory) command at a command-line prompt. If you are logged in as the root user, the following output is displayed on the terminal screen:


However, if you are logged in as the user sue, you see the following output:


To change directories, you can issue the cd (change directory) command with an argument specifying the destination directory. If you do not specify a destination directory, the cd command returns you to your home directory:


You can also use the ~ metacharacter to refer to another user’s home directory by appending a user name at the end:


In many of the examples discussed earlier, the argument specified after the cd command is an absolute pathname to a directory, meaning that the system has all the information it needs to find the destination directory because the pathname starts from the root (/) of the system. However, in most Linux commands, you can also use a relative pathname in place of an absolute pathname to reduce typing. A relative pathname is the pathname of a target file or directory relative to your current directory in the tree. To specify a directory below your current directory, refer to that directory by name (do not start the pathname with a / character). To refer to a directory one step closer to the root of the tree (also known as a parent directory), use two dots (..). An example of using relative pathnames to move around the directory tree is shown next:


The preceding example used “..” to move up one parent directory and then used the word “mary” to specify the mary subdirectory relative to the current location in the tree; however, you can also move more than one level up or down the directory tree:


Note 
You can also use one dot ( . ) to refer to the current directory. Although this is not useful when using the cd command, you do use one dot later in this book.

Although absolute pathnames are straightforward to use as arguments to commands when specifying the location of a certain file or directory, relative pathnames can save you a great deal of typing and reduce the potential for error if your current directory is far away from the root directory. Suppose, for example, that the current directory is /home/sue/projects/acme/plans and you need to change to the /home/sue/projects/acme directory. Using an absolute pathname, you would type cd /home/sue/projects/acme; however, using a relative pathname, you only need to type cd .. to perform the same task because the /home/sue/projects/acme directory is one parent directory above the current location in the directory tree.

An alternate method for saving time when typing pathnames as arguments to commands is to use the Tab-completion feature of the BASH shell. To do this, type enough unique letters of a directory and press the Tab key to allow the BASH shell to find the intended file or directory being specified and fill in the appropriate information. If there is more than one possible match, the Tab-completion feature alerts you with a beep; pressing the Tab key again after this beep presents you with a list of possible files or directories.

Observe the directory structure in Figure 3-2. To use the Tab-completion feature to change the current directory to /home/sue, you type cd /h and then press the Tab key. This changes the previous characters on the terminal screen to display cd /home/ (the BASH shell was able to fill in the appropriate information because the /home directory is the only directory under the / directory that starts with the letter “h”). Then, you could add an s character to the command, so that the command line displays cd /home/s, and press the Tab key once again to allow the shell to fill in the remaining letters. This results in the command cd /home/sue/ being displayed on the terminal screen (the sue directory is the only directory that begins with the s character under the /home directory). At this point, you can press Enter to execute the command and change the current directory to /home/sue.

Note 
In addition to directories, the Tab-completion feature of the BASH shell can be used to specify the pathname to files and executable programs.

Displaying the Contents of Text Files
So far, this chapter has discussed commands that can be used to navigate the Linux directory structure and view filenames and file types; it is usual now to display the contents of these files. By far, the most common file type that users display is text files. These files are usually small and contain configuration information or instructions that the shell interprets (called a shell script) but can also contain other forms of text, as in email messages. To view an entire text file on the terminal screen (also referred to as concatenation), you can use the cat command. The following is an example of using the cat command to display the contents of an email message (in the fictitious file project4):


You can also use the cat command to display the line number of each line in the file in addition to the contents by passing the –n option to the cat command. In the following example, the number of each line in the project4 file is displayed:


In some cases, you might want to display the contents of a certain text file in reverse order, which is useful when displaying files that have text appended to them continuously by system services. These files, also known as log files, contain the most recent entries at the bottom of the file. To display a file in reverse order, use the tac command (tac is cat spelled backwards), as shown next with the file project4:


If the file displayed is very large and you only want to view the first few lines of it, you can use the head command. The head command displays the first 10 lines (including blank lines) of a text file to the terminal screen but can also take a numeric option specifying a different number of lines to display. The following shows an example of using the head command to view the top of the project4 file:


Just as the head command displays the beginning of text files, the tail command can be used to display the end of text files. By default, the tail command displays the final 10 lines of a file, but it can also take a numeric option specifying the number of lines to display on the terminal screen, as shown in the following example with the project4 file:


Although some text files are small enough to be displayed completely on the terminal screen, you might encounter text files that are too large to fit in a single screen. In this case, the cat command sends the entire file contents to the terminal screen; however, the screen only displays as much of the text as it has room for. To display a large text file in a page-by-page fashion, you need to use the more and less commands.

The more command gets its name from the pg command once used on UNIX systems. The pg command displayed a text file page-by-page on the terminal screen, starting at the beginning of the file; pressing the spacebar or Enter key displays the next page, and so on. The more command does more than pg did, because it displays the next complete page of a text file if you press the spacebar, but displays only the next line of a text file if you press Enter. In that way, you can browse the contents of a text file page-by-page or line-by-line. The fictitious file project5 is an excerpt from Shakespeare’s tragedy Macbeth and is too large to be displayed fully on the terminal screen using the cat command. Using the more command to view its contents results in the following output:


As you can see in the preceding output, the more command displays the first page without returning you to the shell prompt. Instead, the more command displays a prompt at the bottom of the terminal screen that indicates how much of the file is displayed on the screen as a percentage of the total file size. In the preceding example, 71 percent of the project5 file is displayed. At this prompt, you can press the spacebar to advance one whole page, or you can press the Enter key to advance to the next line. In addition, the more command allows other user interactions at this prompt. Pressing the h character at the prompt displays a help screen, which is shown in the following output, and pressing the q character quits the more command completely without viewing the remainder of the file.


Just as the more command was named as a result of allowing more user functionality, the less command is named for doing more than the more command (remember that “less is more,” more or less). Like the more command, the less command can browse the contents of a text file page-by-page by pressing the spacebar and line-by-line by pressing the Enter key; however, you can also use the arrow keys on the keyboard to scroll up and down the contents of the file. The output of the less command, when used to view the project5 file, is as follows:


Like the more command, the less command displays a prompt at the bottom of the file using the : character or the filename of the file being viewed (project5 in our example), yet the less command contains more keyboard shortcuts for searching out text within files. At the prompt, you can press the h key to obtain a help screen or the q key to quit. The first help screen for the less command is shown next:


The more and less commands can also be used in conjunction with the output of commands if that output is too large to fit on the terminal screen. To do this, use the | metacharacter after the command, followed by either the more or less command, as follows:


In the preceding example, the output of the ls –l command was redirected to the more command, which displays the first page of output on the terminal. You can then advance through the output page-by-page or line-by-line. This type of redirection is discussed in Chapter 7.

Note 
You can also use the diff command to identify the content differences between two text files, which is often useful when comparing revisions of configuration files on a Linux system. For example, the diff file1 file2 command would list the lines that are different between file1 and file2.
compression notes:

compress , uncompress
 zcat, zmore, zless
 40 - 50% compression
 .Z the extension
=========================
gnuzip, gzip, gunzip, zcat
 Can control level of compression. 1 to 9, fastest to best
 60 - 70% compression
 .gz is the extension
=========================
xz, unxz: better compression
 xzmore, xzcat, xzless
 Higher compression ratio Thant compress and gzip
 .xz, must specify extension to uncompress
=========================
zip, unzip: fairly standard
 Similar compression ration to gzip
 .zip is the default extension
=========================
bzip2, bunzip2: different algorithm, better suited for binary
 and already compressed files.
 bzcat, bzless, bzmore
 60 - 80% compression ratio
 .bz2 is the default extensio

Backup Notes
=========================
Tape devices:

 /dev/st0, /dev/nst0
=========================
=========================
TAR
###################
# To copy from one directory to another
###################
tar cf - $DIR | ( cd $TDIR ; tar xvf - ) #Where DIR is name of src dir
 #And TDIR is name of target dir
###################
# To copy from one directory to another directory on a remote system
###################
tar cf - $DIR | remsh $NODE "( cd $TDIR ; tar xvf - )" #where NODE is an
 #IP addr
 #or valid DNS name
#NOTE THAT NOWADAYS WE WOULD USE SSH INSTEAD OF REMSH OR RSH
Can’t do long file paths. Some systems have btar
=========================
=========================
CPIO
remsh hpgold "(cd /gold; find . -xdev | cpio -ocBx)" | \
(cd /gold; cpio -icxBduvm) >/tmp/log 2>&1
cd /gold; find . -xdev | cpio -ocBx | \
remsh hpgold "(cd /gold; cpio -icxBduvm)"
Can do larger file paths and device files
=========================
=========================
dump/restore
=========================
Has backup levels from 0 for full backups and 1 thru 9 for partial backups. Only works on ext type file
systems.
=========================
dd : A raw/block copy
=========================
See my example for copying over an iso file to a thumb drive
=========================
Disk burning: usually a gui interface
=========================
=========================
cp,
=========================
cp -r would be used for this
=========================
Rsync
 rsync -auzb --ignore-errors --delete Common/ ~/Common_Save
=========================
=========================
Mirror splitting
=========================
Raid would be used for this.
=========================
Backup Software
=========================
Lot’s on the market.
=========================
Clones, bootable clones
=========================
=========================
Shadowing
=========================
Uses hard links in hidden directories that mimic the actual file system. One set of directories for daily links.
One set for weekly links and one set for monthly links. Daily links removed after a day, weekly links remove
after a week, monthy links removed after a month. All only if the original link has been removed.

Installation Notes
=========================
Makefile
 Usually starts with a tarball, archive.tar.gz
 uncompress, untar, cd to main directory
 Check out README file and INSTALL file
 make, make config, make install
=========================
rpm, dnf, yum. - red hat, fedora, centos
=========================
dpm, dpkg, apt - debian, ubuntu
 aptitude: text menu interface
=========================
 yum, zipper, probably based on RPM - opensuse
=========================
ldd /bin/bash #to see what shared libraries are used to build bash binary
=========================
Various graphical interfaces depending on the Linux Distribution
=========================
Also, if you just type a command that you want, fedora will volunteer to load it for you if
it knows how. Ubuntu will tell you how to load it. Opensuse will scold you and tell you
to type ‘cnf command’ to find out how to do a load if it is possible.


Printer Administration
Many users commonly need to print files on a Linux system, and printing log files and system configuration information is good procedure in case of a system failure. Thus, a firm understanding of how to set up, manage, and print to printers is vital for those who set up and administer Linux servers.

The Common UNIX Printing System
Today, the most common printing system used on Linux computers is the Common Unix Printing System (CUPS), which was developed by Apple, Inc. Fundamental to using CUPS on a Linux system is an understanding of how information is sent to a printer. A set of information sent to a printer at the same time is called a print job. Print jobs can consist of a file, several files, or the output of a command. To send a print job to a printer, you must first use the lp command and specify what to print.

Next, the CUPS daemon (cupsd) assigns the print job a unique print job ID and places a copy of the print job into a temporary directory on the filesystem called the print queue, provided the printer is accepting requests. If the printer is rejecting requests, the CUPS daemon displays an error message stating that the printer is not accepting print jobs.

Note  
Accepting print jobs into a print queue is called spooling or queuing.

The print queue for a printer is typically /var/spool/cups. Regardless of how many printers you have on your Linux system, all print jobs are sent to the same directory.

When a print job is in the print queue, it is ready to be printed. If the printer is enabled and ready to accept the print job, the CUPS daemon sends the print job from the print queue to the printer and removes the copy of the print job in the print queue. Conversely, if the printer is disabled, the print job remains in the print queue.

Note  
Sending print jobs from a print queue to a printer is commonly called printing.

An example of this process for a printer called printer1 is illustrated in Figure 10-1.

The print process

Figure shows the print process. Overall the process is controlled by the daemon cups d. In the spooling part, the l p command is issued. This is accepted or rejected and to the print queue at slash var slash spool slash cups. In the printing part of the process the printer designated in the diagram as printer 1 is enabled or disabled.
To see a list of all printers on the system and their status, you can use the –t (total) option to the lpstat command, as shown in the following output:

This output indicates the system has only one printer, called printer1, that it prints to a printer connected to the first parallel port on the computer (parallel:/dev/lp0), and that no print jobs are in its print queue. In addition, the CUPS daemon (scheduler) is running and accepting jobs into the print queue for this printer. The CUPS daemon sends print jobs from the print queue to the printer because the printer is enabled.
You can manipulate the status of a printer using the cupsaccept, cupsreject, cupsenable, or cupsdisable command followed by the printer name. To enable spooling and disable printing for the printer printer1, you can use the following commands:
Any print jobs now sent to the printer printer1 are sent to the print queue but remain in the print queue until the printer is started again.

Managing Print Jobs
Recall that you create a print job by using the lp command. To print a copy of the /etc/inittab file to the printer printer1 shown in earlier examples, you can use the following command, which returns a print job ID that you can use to manipulate the print job afterward:

 
The lp command uses the –d option to specify the destination printer name. If you omit this option, the lp command assumes the default printer on the system. Because printer1 is the only printer on the system making it the default printer, the command lp /etc/inittab is equivalent to the one used in the preceding output.

Note  
You can set the default printer for all users on your system by using the lpoptions –d printername command, where printername is the name of the default printer. This information is stored in the /etc/cups/lpoptions file.

You can specify your own default printer by adding the line default printername to the .lpoptions file in your home directory, where printername is the name of the default printer. Alternatively, you can use the PRINTER or LPDEST variables to set the default printer. For example, to specify printer2 as the default printer, you can add either the line export PRINTER=printer2 or the line export LPDEST=printer2 to an environment file in your home directory, such as .bash_profile.

Table 10-1 lists some common options to the lp command.

Table 10-1
Common Options to the lp Command
Option	Description
-d printer name	Specifies the name of the destination printer
-i print job ID	Specifies a certain print job ID to modify
-n number	Prints a certain number of copies
-m	Mails you confirmation of print job completion
-o option	
Specifies certain printing options; common printing options include the following:

cpi=number—Sets the characters per inch to number

landscape—Prints in landscape orientation

number-up=number—Prints number pages on a single page, where number is 1, 2, or 4

sides=string—Sets double-sided printing, where string is either “two-sided-short-edge” or “two-sided-long-edge”

-q priority	Specifies a print job priority from 1 (low priority) to 100 (high priority); by default, all print jobs have a priority of 50
Enlarge Table
You can also specify several files to print using a single lp command by including the files as arguments. In this case, the system creates only one print job to print all of the files. To print the files /etc/hosts and /etc/issue to the printer printer1, you can execute the following command:

 
The lp command accepts information from Standard Input, so you can place the lp command at the end of a pipe to print information. To print a list of logged-in users, you can use the following pipe:

 
To see a list of print jobs in the queue for printer1, you can use the lpstat command. Without arguments, this command displays all jobs in the print queue that you have printed:

 Enlarge Image
Table 10-2 lists other options that you can use with the lpstat command.

Table 10-2
Common Options to the lpstat Command
Option	Description
-a	Displays a list of printers that are accepting print jobs
-d	Displays the default destination printer
-o printername	Displays the print jobs in the print queue for printername only
-p	Displays a list of printers that are enabled
-r	Shows whether the CUPS daemon (scheduler) is running
-s	Shows printer and printer status information
-t	Shows all information about printers and their print jobs
To remove a print job from the print queue, you can use the cancel command followed by the IDs of the print jobs to remove. To remove the print job IDs printer1-1 and printer1-2 created earlier, you can use the following command:

 Enlarge Image
You can also remove all jobs started by a certain user by specifying the –u option to the cancel command followed by the user name. To remove all jobs in a print queue, you can use the –a option to the cancel command, as shown in the following example, which removes all jobs destined for printer1:

 
Not all users might be allowed access to a certain printer. As a result, you can restrict access to certain printers by using the lpadmin command. For example, to deny all users other than root and user1 from printing to the printer1 printer created earlier, you can use the following command:

 Although CUPS is the preferred printing system for Linux computers today, many legacy Linux systems use the traditional Line Printer Daemon (LPD) printing system. In this printing system, you use the lpr command to print documents to the print queue much like the lp command. You can use the lpc command to view the status of printers, the lpq command to view print jobs in the print queue, much like the lpstat command, and the lprm command to remove print jobs, much like the cancel command.

For those who are accustomed to using the LPD printing system, CUPS contains versions of the lpr, lpc, lpq, and lprm commands. The following output displays the status of all printers on the system, prints two copies of /etc/inittab to printer1, views the print job in the queue, and removes the print job:

Configuring Printers
Recall that the core component of printing is the CUPS daemon, which accepts print jobs into a queue and sends them to the printer. The file that contains settings for the CUPS daemon is /etc/cups/cupsd.conf, and the file that contains the configuration information for each printer installed on the system is /etc/cups/printers.conf. By default, the CUPS daemon detects locally connected and network-shared printers and automatically adds an entry for them in the /etc/cups/printers.conf file using a name based on the printer model number (e.g., HP Laserjet 6M). For any printers that the CUPS daemon does not detect and configure, you must provide the necessary information.

Because the format of the /etc/cups/printers.conf file is rigid, it is safer to use a program to create or modify its entries. You can provide a series of options to the lpadmin command discussed earlier to configure a printer on the system, or you can use one of several graphical utilities within a desktop environment that can do the same. On Fedora Linux, you can use the Printers tool within the GNOME desktop (shown in Figure 10-2) to create new printers, print a test page, view print jobs, control spooling, and specify basic printing options. You can access the Printers tool within the GNOME desktop environment on Fedora 28 by navigating to the Activities menu, Show Applications, Settings, Devices, Printers.

Figure 10-2
The Printers tool

Figure shows the printers tool. This is a G U I tool that allows a user to add or remove a printer. The Add button can be used to add a printer. A printer with the name printer 1 is seen in this image.Enlarge Image
To manually add a printer using the Printers tool, you can click the Unlock button in the upper-right corner and supply your account password, and then click the Add button that appears in its place, as shown in Figure 10-2. The Printers tool prompts you to select your device from a list of detected printers, or specify the name or IP address of a printer on the network.

Note  
The Printers tool provides for a quick and easy method of configuring and managing printers on a system. It does not allow you to specify detailed printer configuration when adding a printer.

The most comprehensive way to create and manage CUPS printers is by using the CUPS Web administration tool, which allows Linux administrators to create and manage all printer settings. You can access the CUPS Web administration tool using a Web browser on TCP port 631 by navigating to http://servername:631, as shown in Figure 10-3.

Figure 10-3
The CUPS Web administration tool

The CUPS web administration tool. The home page of the the administration tool for common unix printing system is seen with comprehensive information. Documentation links are seen under the headings CUPS for users, CUPS for administrators, and CUPS for developers in this page.Enlarge Image
To create a new printer using this tool, select the Administration tab, click Add Printer, and log in using the root user name and password. You are then prompted to choose the type of printer, as shown in Figure 10-4. You can choose to print to a printer connected to a local serial, parallel, or USB port, to a printer connected to the network using Hewlett-Packard JetDirect technology, or to a printer that is shared on a remote computer across a network with the Internet Printing Protocol (IPP), the Line Printer Daemon (LPD), or the Windows (SAMBA) printing service.

Figure 10-4
Selecting the printer type for a new printer

Illustrations shows the add printer page under the administration tab in the CUPS web administration tool. Under local printers, two printers are listed. Under network printers no printers are listed and it is blank. Under other network printers, a list of printer protocols are available for selection in order to add a printer that exists on the network.
Note  
The local HP Printer selections shown in Figure 10-4 can be used to configure other brands of locally connected printers as well.

After selecting a printer type, you specify information related to the printer type (e.g., the network address for a network printer), as well as a printer name, description, manufacturer and model, default printer options, and whether to share the printer using IPP to other systems on the network.

After creating a printer, you can use the other options available on the Administration tab of the CUPS Web administration tool to configure and manage the CUPS printing service, as shown in Figure 10-5.

Figure 10-5
CUPS administration options

CUPS administration options are seen in the CUPS web administration tool. Under the heading Printers, there are buttons to add a printer, find new printers, and manage printers. Under the heading classes, there are buttons to add a class and manage classes. Under the heading Jobs, there is a button to manage jobs. Under the heading Server, the configuration file can be edited and the access log, error log, and page log can be viewed. Under server settings advanced options can be configured.Enlarge Image
The Find New Printers button shown in Figure 10-5 performs a detailed local and network search for new printer devices. The Manage Printers button (which switches to the Printers tab) allows you to configure individual printer settings.

CUPS also allows you to configure collections of printers to use as a single unit. These collections are called printer classes. When you print to a printer class, the print job is sent to the first available printer in the printer class. Printer classes are often used in larger organizations, where multiple printers are stored in a print room. To create a printer class, select the Add Class button shown in Figure 10-5, supply a name and description for the new class, and then choose the printers to include in the class. Next, you can click the Manage Classes button (which switches to the Classes tab) to configure settings for your new printer class.

Clicking Manage Jobs shown in Figure 10-5 (which switches to the Jobs tab) allows you to view, modify, and delete print jobs in the queue. Regular users can also access the CUPS Web administration tool and select the Jobs tab to manage their own jobs after logging in with their user name and password.

The Server section shown in Figure 10-5 allows you to edit the CUPS configuration file, access log files, and perform advanced functions. Select the Allow remote administration option to access the CUPS Web administration tool from other computers on the network. By selecting the Share printers connected to this system and Allow printing from the Internet options, IPP printer sharing will be enabled for all printers that allow IPP printer sharing in their settings. To configure a shared printer on a remote system using IPP, you can specify the URL http://servername:631/printers/printername when adding the printer.


Log File Administration
To identify and troubleshoot problems on a Linux system, you must view the events that occur over time. Because administrators cannot observe all events that take place on a Linux system, most daemons record information and error messages to files stored on the filesystem. These files are referred to as log files and are typically stored in the /var/log directory.

Many programs store their log files in subdirectories of the /var/log directory. For example, the /var/log/samba directory contains the log files created by the samba file-sharing daemons. Table 10-3 lists some common log files that you may find in the /var/log directory, depending on your Linux distribution.

Table 10-3
Common Linux Log Files Found in /var/log
Log file	Description
auth.log	Contains a history of all authentication requests on the system by users and daemons
btmp	Contains a history of failed login sessions; must be viewed using the lastb command
boot.log	Contains basic information regarding daemon startup obtained during system initialization
cron	Contains information and error messages generated by the cron and at daemons
dmesg	Contains detected hardware information obtained during system startup
kern.log	Contains information and error messages generated by the kernel
mail.log	Contains information and error messages generated by the sendmail or postfix daemon
secure	Contains information and error messages regarding network access generated by daemons such as sshd and xinetd
wtmp	Contains a history of all login sessions; must be viewed using the last or who commands
rpmpkgs

yum.log

dnf.rpm.log

Contains messages for actions taken and packages installed by the Red Hat Package Manager
dpkg.log	Contains messages for actions taken and packages installed by the Debian Package Manager
xferlog	Contains information and error messages generated by the FTP daemon
Xorg.0.log	Contains information and error messages generated by X Windows
lastlog	Contains a list of users and their last login time; must be viewed using the lastlog command
messages

syslog

Contains detailed information regarding daemon startup obtained at system initialization as well as important system messages produced after system initialization
Enlarge Table
Daemons that provide a service to other computers on the network typically manage their own log files within the /var/log directory. The logging for other daemons and operating system components is performed by a logging daemon. The two most common logging daemons used on Linux systems today are the System Log Daemon (rsyslogd) and the Systemd Journal Daemon (journald).

Note  
Many third-party software suites can monitor different systems on the network; these software suites provide a small program that is installed on each Linux server (called an agent) that collects the events logged by rsyslogd or journald, and sends them to a central monitoring server on the network.

The System Log Daemon (rsyslogd) is the traditional and most common logging daemon used on Linux systems. When this daemon is loaded upon system startup, it creates a socket (/dev/log) for other system processes to write to. It then reads any information written to this socket and saves the information in the appropriate log file according to entries in the /etc/rsyslog.conf file and any files within the /etc/rsyslog.d directory. A sample /etc/rsyslog.conf file is shown in the following output:

 Enlarge Image
Note  
On legacy Linux systems, the System Log Daemon is represented by syslogd and configured using the /etc/syslog.conf file.

Lines in the /etc/rsyslog.conf file or in files within the /etc/rsyslog.d directory that start with a # character are comments. All other entries have the following format:

 
The facility is the area of the system to listen to, whereas the priority refers to the importance of the information. For example, a facility of kern and priority of warning indicates that the System Log Daemon should listen for kernel messages of priority warning and more serious. When found, the System Log Daemon places these messages in the /var/log/logfile file. This entry would read as follows:

 
To only log warning messages from the kernel to /var/log/logfile, you can use the following entry instead:

 
Alternatively, you can log all error messages from the kernel to /var/log/logfile by using the * wildcard, as shown in the following entry:

 
In addition, you can specify multiple facilities and priorities. To log all error messages except warnings from the kernel to /var/log/logfile, you can use the following entry:

 
To log all error messages from the kernel and news daemons, you can use the following entry:

 
To log all warnings from all facilities except for the kernel, you can use the “none” keyword, as shown in following entry:

 
You can also prefix the pathname to the log file in any entry to ensure that the system synchronizes changes to the disk immediately after a new event occurs, as shown in the following entry:

 
Table 10-4 describes the different facilities available and their descriptions.

Table 10-4
Facilities Used by the System Log Daemon
Facility	Description
auth

or

security

Specifies messages from the login system, such as the login program, the getty program, and the su command
authpriv	Specifies messages from the login system when authenticating users across the network or to system databases
cron	Specifies messages from the cron and at daemons
daemon	Specifies messages from system daemons such as the FTP daemon
kern	Specifies messages from the Linux kernel
lpr	Specifies messages from the printing system (lpd)
mail	Specifies messages from the email system (sendmail)
mark	Specifies time stamps used by syslogd; used internally only
news	Specifies messages from the Inter Network News daemon and other USENET daemons
syslog	Specifies messages from the syslog daemon
user	Specifies messages from user processes
uucp	Specifies messages from the uucp (UNIX to UNIX copy) daemon
local0-7	Specifies local messages; these are not used by default but can be defined for custom use
Enlarge Table
Table 10-5 displays the different priorities available listed in ascending order.

Table 10-5
Priorities Used by the System Log Daemon
Priority	Description
debug	Indicates all information from a certain facility
info	Indicates normal information messages as a result of system operations
notice	Indicates information that should be noted for future reference, yet does not indicate a problem
warning

or

warn

Indicates messages that might be the result of an error but are not critical to system operations
error

or

err

Indicates all other error messages not described by other priorities
crit	Indicates system critical errors such as hard disk failure
alert	Indicates an error that should be rectified immediately, such as a corrupt system database
emerg

or

panic

Indicates very serious system conditions that would normally be broadcast to all users
Enlarge Table
The /etc/rsyslog.conf file may also send logging information to another computer using the format facility.priority @hostname:portnumber; however the remote computer must have the modules that listen on either the TCP or UDP protocol uncommented in the /etc/rsyslog.conf file. For example, by uncommenting the same lines shown next in the /etc/rsyslog.conf file, you allow your system to accept incoming requests from another System Log Daemon on TCP and UDP port 514 (the default System Log Daemon port)


Working with the Systemd Journal Daemon
The Systemd Journal Daemon (journald) replaces the System Log Daemon on Linux distributions that use Systemd. As with the System Log Daemon, journald creates a socket (/dev/log) for other system processes to write to and reads any information that is written to this socket. However, the events logged are not controlled by specific rules. Instead, journald logs all information to a database under the /var/log/journal directory structure, and events are tagged with labels that identify the same facility and priority information that you examined earlier with the rsyslogd daemon.

Note  
You can configure daemon settings for journald by editing its configuration file /etc/systemd/journald.conf. For example, to configure journald to forward all events to the System Log Daemon, you could uncomment and configure the ForwardToSyslog=yes line within /etc/systemd/journald.conf and restart journald.

To view events within the journald database, you can use the same journalctl command introduced in Chapter 6 to view boot-related messages. If you type journalctl at a command prompt and press the Tab key, you will see a multipage list of areas and criteria that can be queried, as shown in the following sample excerpt:

 
Say, for example, that you want to search for information in the logs from a particular command on the system. You can type journalctl _COMM= and press the Tab key to see a multipage list of the available commands that perform logging via journald on the system, as shown in the following sample excerpt:

 Enlarge Image
To see the logs from the cron daemon (crond), you can run the command journalctl _COMM=crond | less because the output will likely have many lines. However, when troubleshooting an issue regarding the cron daemon, it is more useful to narrow the time period. The following command displays crond log entries from 1:00 pm (13:00) until 2:00 pm (14:00) from the current day:

 Enlarge Image
The time format used within the journalctl command follows the standard YYYY-MM-DD HH-MM-SS. Thus, to obtain crond events since 5:30 pm (17:30) on August 22, 2019, you could instead run the journalctl _COMM=crond --since "2019-08-22 17:30:00" command.

You can also query events related to a specific process or daemon if you specify the path name to the executable file or PID. For example, because the Systemd init daemon has a path of /usr/lib/systemd/systemd and a PID of 1, you could run the journalctl /usr/lib/systemd/systemd command or the journalctl _PID=1 command to view events related to the Systemd init daemon.

Note  
You can use the systemd-cat command or logger command to add custom log file entries to the journald database; for example, you could use the echo "Backup of $DIR completed successfully at `date`" | systemd-cat command or the logger "Backup of $DIR completed successfully at `date`" command within a shell script to log a message to the journald database indicating that a backup of a directory within the $DIR variable was completed at a particular time. The logger command can also be used to add custom log file entries on a system that uses the System Log Daemon; simply specify the -p facility.priority option alongside the logger command to control how the events are logged.

10-2c
Managing Log Files and the Journald Database
Although log files and the journald database contain important system information, they might take up unnecessary space on the filesystem over time because journald is configured to use persistent storage by default. However, you can uncomment and configure the line Storage=volatile within /etc/systemd/journald.conf to ensure that events are only stored in memory and not on the filesystem. Alternatively, you could limit the amount of space on the filesystem that journald will use to store events by uncommenting and configuring the SystemMaxUse line in /etc/systemd/journald.conf. For example, SystemMaxUse=75M would limit the database to 75MB, and the oldest events would be deleted as new events are logged. To prevent key older events from being overwritten, you can create a shell script that executes the appropriate journalctl commands and either prints the results or saves them to a text file. You can then configure the cron daemon to execute this shell script periodically, as discussed in Chapter 9.

For log files within the /var/log directory, you can periodically clear their contents to reclaim space on the filesystem.

Note  
Do not remove log files, because the permissions and ownership will be removed as well.

Before clearing important log files, it is good form to save them to a different location or print their contents for future reference.

To clear a log file, recall that you can use a > redirection symbol. The following commands display the size of the /var/log/boot.log file before and after it has been printed and cleared:

 Enlarge Image
Alternatively, you can schedule the logrotate command to back up and clear log files from entries stored in the /etc/logrotate.conf file and files stored in the /etc/logrotate.d directory. The logrotate command typically renames (or rotates) log files on a cyclic basis; the log file will be renamed to contain a numeric or date extension, and a new log file will be created to accept system information. You can specify the type of extension as well as the number of log files that are kept. If configured to keep only two copies of old log files, then after two log rotations, the oldest log file will automatically be removed.

An example of the /etc/logrotate.conf file is shown in the following output:

 Enlarge Image
In the preceding output, any # characters indicate a comment and are ignored. The other lines indicate that log files contained in this file and all other files in the /etc/logrotate.d directory (include /etc/logrotate.d) are rotated on a weekly basis unless otherwise specified (weekly) using a date extension (dateext), and up to 4 weeks of log files will be kept (rotate 4).

The bottom of the /etc/logrotate.conf file has two entries that override these values. For the file /var/log/wtmp, this rotation occurs monthly instead of weekly, only if the size of the log file is greater than 1MB. Only one old log file is kept, and the new log file created has the permissions 0664(rw-rw-r--), the owner root, and the group utmp. For the file /var/log/btmp, this rotation occurs monthly instead of weekly, and no errors are reported if the log file is missing. Only one old log file is kept, and the new log file created has the permissions 0600 (rw-------), the owner root, and the group utmp.

Most rotation information within /etc/logrotate.conf is overridden from files stored in the /etc/logrotate.d directory. Take the file /etc/logrotate.d/psacct as an example:

 Enlarge Image
This file indicates that the /var/account/pacct file should be rotated daily if it is not empty, and that new log files will be owned by the root user/group and have the permissions 0600 (rw-------). Up to 31 old log files will be kept and compressed, but only on the next rotation (delaycompress). In addition, the /usr/sbin/accton /var/account/pacct command is run after each rotation if the /usr/bin/systemctl --quiet is-active psacct.service command returns true.

On most Linux systems, the logrotate command is automatically scheduled to run daily via the file /etc/cron.daily/logrotate; however, you might choose to run it manually by typing logrotate /etc/logrotate.conf at a command prompt.

Over time, the logrotate command generates several copies of each log file, as shown in the following listing of the /var/log directory:

 Enlarge Image
Given the preceding output, the most recent events for the cron daemon are recorded in the cron file, followed by the cron-20190819 file, followed by the cron-20190826 file, the cron-20190911 file, and the cron-20190918 file.

Administering Users and Groups
You must log in to a Linux system with a valid user name and password before a BASH shell is granted. This process is called authentication because the user name and password are authenticated against a system database that contains all user account information. Authenticated users are then granted access to files, directories, and other resources on the system based on their user account.

The system database that contains user account information typically consists of two files: /etc/passwd and /etc/shadow. Every user typically has a line that describes the user account in /etc/passwd and a line that contains the encrypted password and expiration information in /etc/shadow.

Legacy Linux systems stored the encrypted password in the /etc/passwd file and did not use an /etc/shadow file at all. This is considered poor security today because processes often require access to the user information in /etc/passwd. Storing the encrypted password in a separate file that cannot be accessed by processes prevents a process from obtaining all user account information. By default, Linux configures passwords using an /etc/shadow file. However, you can use the pwunconv command to revert to using an /etc/passwd file only, or the pwconv command to configure the system again using an /etc/shadow file for password storage.

Each line of the /etc/passwd file has the following colon-delimited format:

 
The name in the preceding output refers to the name of the user. If an /etc/shadow is not used, the password field contains the encrypted password for the user; otherwise, it just contains an x character as a placeholder for the password stored in /etc/shadow.

The User Identifier (UID) specifies the unique User ID that is assigned to each user. Typically, UIDs that are less than 1000 refer to user accounts that are used by daemons when logging in to the system. The root user always has a UID of zero.

The Group Identifier (GID) is the primary Group ID for the user. Each user can be a member of several groups, but only one of those groups can be the primary group. The primary group of a user is the group that is made the group owner of any file or directory that the user creates. Similarly, when a user creates a file or directory, that user becomes the owner of that file or directory.

GECOS represents an optional text description of the user; this information was originally used in the General Electric Comprehensive Operating System (GECOS). The last two fields represent the absolute pathname to the user’s home directory and the shell, respectively.

An example of an /etc/passwd file is shown next:

 Enlarge Image
The root user is usually listed at the top of the /etc/passwd file, as just shown, followed by user accounts used by daemons when logging in to the system, followed by regular user accounts. The final line of the preceding output indicates that the user user1 has a UID of 1000, a primary GID of 1000, a GECOS of “Jason Eckert” the home directory /home/user1, and uses the BASH shell.

Like /etc/passwd, the /etc/shadow file is colon-delimited yet has the following format:

 
Although the first two fields in the /etc/shadow file are the same as those in /etc/passwd, the contents of the password field are different. The password field in the /etc/shadow file contains the encrypted password, whereas the password field in /etc/passwd contains an x character, because it is not used.

The lastchange field represents the date of the most recent password change; it is measured in the number of days since January 1, 1970. For example, the number 10957 represents January 1, 2000, because January 1, 2000 is 10957 days after January 1, 1970.

Note  
Traditionally, a calendar date was represented by a number indicating the number of days since January 1, 1970. Many calendar dates found in configuration files follow the same convention.

To prevent unauthorized access to a Linux system, it is good form to change passwords for user accounts regularly. To ensure that passwords are changed, you can set them to expire at certain intervals. The next three fields of the /etc/shadow file indicate information about password expiration: min represents the number of days a user must wait before he changes his password after receiving a new one, max represents the number of days a user can use the same password without changing it, and warn represents the number of days a user is warned to change his password before it expires.

By default on most Linux systems, min is equal to zero days, max is equal to 99999 days, and warn is equal to seven days. This means you can change your password immediately after receiving a new one, your password expires in 99999 days, and you are warned seven days before your password needs to be changed.

When a password has expired, the user is still allowed to log in to the system for a certain period of time, after which point the user is disabled from logging in. The number of days a user account is disabled after a password expires is represented by the disable1 field in /etc/shadow. In addition, you can choose to disable a user from logging in at a certain date, such as the end of an employment contract. The disable2 field in /etc/shadow represents the number of days since January 1, 1970 that a user account will be disabled.

An example /etc/shadow file is shown next:

 Enlarge Image
Note from the preceding output that most user accounts used by daemons do not receive an encrypted password.

Although every user must have a primary group listed in the /etc/passwd file, each user can be a member of multiple groups. All groups and their members are listed in the /etc/group file. The /etc/group file has the following colon-delimited fields:

 
The first field is the name of the group, followed by a group password.

Note  
The password field usually contains an x, because group passwords are rarely used. If group passwords are used on your system, you need to specify a password to change your primary group membership using the newgrp command discussed later in this chapter. These passwords are set using the gpasswd command and can be stored in the /etc/gshadow file for added security. Refer to the gpasswd manual or info page for more information.

The GID represents the unique Group ID for the group, and the members field indicates the list of group members. An example /etc/group file is shown next:

 Enlarge Image
From the preceding output, the “bin” group has a GID of 1 and three users as members: root, bin, and daemon. Similarly, the wheel group has a GID of 10 and user1 as a member.

Note  
The wheel group is a special group that provides its members with the ability to run the su and sudo commands; as a result, the first user that you create during a Linux installation is added to the wheel group to allow them to set the root password using the sudo command. Some Linux distributions, including Ubuntu, use the sudo group in place of the wheel group.

You can also use the getent command to view the entries within system databases such as /etc/passwd, /etc/shadow, /etc/group, and /etc/gshadow; for example, getent shadow will display the entries within the shadow database (/etc/shadow).

Creating User Accounts
You can create user accounts on the Linux system by using the useradd command, specifying the user name as an argument, as shown next:

 
In this case, all other information, such as the UID, shell, and home directory location, is taken from two files that contain user account creation default values.

The first file, /etc/login.defs, contains parameters that set the default location for email, password expiration information, minimum password length, and the range of UIDs and GIDs available for use. In addition, it determines whether home directories will be automatically made during user creation, as well as the password hash algorithm used to store passwords within /etc/shadow.

A sample /etc/login.defs file is depicted in the following example:

 Enlarge Image
The second file, /etc/default/useradd, contains information regarding the default primary group, the location of home directories, the default number of days to disable accounts with expired passwords, the date to disable user accounts, the shell used, and the skeleton directory used. The skeleton directory, which is /etc/skel on most Linux systems, contains files that are copied to all new users’ home directories when the home directory is created. Most of these files are environment files, such as .bash_profile and .bashrc.

A sample /etc/default/useradd file is shown in the following output:

 
To override any of the default parameters for a user in the /etc/login.defs and /etc/default/useradd files, you can specify options to the useradd command when creating user accounts. For example, to create a user named maryj with a UID of 762, you can use the –u option to the useradd command, as shown in the following example:

 
Table 10-6 lists some common options available to the useradd command and their descriptions.

Table 10-6
Common Options to the useradd Command
Option	Description
-c "description"	Adds a description for the user to the GECOS field of /etc/passwd
-d homedirectory	Specifies the absolute pathname to the user’s home directory
-e expirydate	Specifies a date to disable the account from logging in
-f days	Specifies the number of days until a user account with an expired password is disabled
-g group	Specifies the primary group for the user account; on most Linux distributions, a group is created with the same name as the user and made the primary group for that user via the USERGROUPS_ENAB entry in the /etc/login.defs file
-G group1,group2,etc.	Specifies all other group memberships for the user account
-m	Specifies that a home directory should be created for the user account; on most Linux distributions, home directories are created for all users by default via the CREATE_HOME entry in the /etc/login.defs file
-k directory	Specifies the skeleton directory used when copying files to a new home directory
-s shell	Specifies the absolute pathname to the shell used for the user account
-u UID	Specifies the UID of the user account
Enlarge Table
After a user account has been added, the password field in the /etc/shadow file contains either two ! characters or a single * character, indicating that no password has been set for the user account. To set the password, type the passwd command, type the name of the new user account at a command prompt, and then supply the appropriate password when prompted. An example of setting the password for the bobg user is shown in the following:

 
Note  
Without arguments, the passwd command changes the password for the current user.

All user accounts must have a password set before they are used to log in to the system.

The root user can set the password on any user account using the passwd command; however, regular users can change their password only using this command.

Passwords should be difficult to guess and contain a combination of uppercase, lowercase, and special characters to increase system security. An example of a good password is C2Jr1;Pwr.

Modifying User Accounts
To modify the information regarding a user account after creation, you can edit the /etc/passwd or /etc/shadow file. This is not recommended, however, because typographical errors in these files might prevent the system from functioning. Instead, it’s better to use the usermod command, which you can use to modify most information regarding user accounts. For example, to change the login name of the user bobg to barbg, you can use the –l option to the usermod command:

 
Table 10-7 displays a complete list of options used with the usermod command to modify user accounts.

Table 10-7
Common Options to the usermod Command
Option	Description
-c "description"	Specifies a new description for the user in the GECOS field of /etc/passwd
-d homedirectory	Specifies the absolute pathname to a new home directory
-e expirydate	Specifies a date to disable the account from logging in
-f days	Specifies the number of days until a user account with an expired password is disabled
-g group	Specifies a new primary group for the user account
-G group1,group2,etc.	Specifies all other group memberships for the user account
-l name	Specifies a new login name
-s shell	Specifies the absolute pathname to a new shell used for the user account
-u UID	Specifies a new UID for the user account
Enlarge Table
Note  
If installed, the finger command can be used to view information about users. This information is stored in the GECOS field of /etc/passwd. As a result, instead of using the –c option to the usermod command, users can change their own GECOS, or finger information, using the chfn command.

The only user account information that the usermod command cannot modify is the password expiration information stored in /etc/shadow (min, max, warn), discussed earlier. To change this information, you can use the chage command with the appropriate option. For example, to specify that the user bobg must wait two days before changing his password after receiving a new password, as well as to specify that his password expires every 50 days with seven days of warning prior to expiration, you can use the following options to the chage command:

 
Note  
You can also use the chage command to view password expiration information. For example, the chage -l bobg command displays the password expiration information for the user bobg.

Sometimes it’s necessary to lock an account—that is, to temporarily prevent a user from logging in. To lock an account, you can use the command usermod –L username at the command prompt. This places a ! character at the beginning of the encrypted password field in the /etc/shadow file. To unlock the account, type usermod –U username at the command prompt, which removes the ! character from the password field in the /etc/shadow file.

Alternatively, you can use the passwd –l username command to lock a user account, and the passwd –u username command to unlock a user account. These commands place and remove two ! characters at the beginning of the encrypted password field in the /etc/shadow file, respectively.

Yet another method commonly used to lock a user account is to change the shell specified in /etc/passwd for a user account from /bin/bash to an invalid shell such as /bin/false or /sbin/nologin. Without a valid shell, a user cannot use the system. To lock a user account this way, you can edit the /etc/passwd file and make the appropriate change, use the –s option to the usermod command, or use the chsh command. The following example uses the chsh command to change the shell to /bin/false for the user bobg:


Deleting User Accounts
To delete a user account, you can use the userdel command and specify the user name as an argument. This removes entries from the /etc/passwd and /etc/shadow files corresponding to the user account. Furthermore, you can specify the –r option to the userdel command to remove the home directory for the user and all of its contents.

When a user account is deleted, any files that were previously owned by the user become owned by a number that represents the UID of the deleted user. Any future user account that is given the same UID then becomes the owner of those files.

Suppose, for example, that the user bobg leaves the company. To delete bobg’s user account and display the ownership of his old files, you can use the following commands:

 Enlarge Image
From the preceding output, you can see that the UID of the bobg user was 1002. Now suppose the company hires Sue—that is, the user sueb—to replace bobg. You can then assign the UID of 1002 to Sue’s user account. Although she will have her own home directory (/home/sueb), she will also own all of bobg’s old files within /home/bobg and otherwise. She can then copy the files that she needs to her own home directory and remove any files that she doesn’t need as part of her job function.

To create the user sueb with a UID of 1002 and list the ownership of the files in bobg’s home directory, you can use the following commands:

Managing Groups
By far, the easiest way to add groups to a system is to edit the /etc/group file using a text editor. Another method is to use the groupadd command. To add a group called group1 to the system and assign it a GID of 492, you can use the following command:

 
Then, you can use the -G option to the usermod command to add members to the group. To add the user maryj to this group and view the addition, you can use the following usermod command:

 
You can use the groupmod command to modify the group name and GID and the groupdel command to remove groups from the system.

To see a list of groups of which you are a member, run the groups command; to see the GIDs for each group, run the id command. Each command always lists the primary group first. The following output shows sample output of these commands when executed by the root user:

 
In the preceding output the primary group for the root user is the root group. This group is attached as the group owner for all files that are created by the root user, as shown in the following output:

 
To change the primary group temporarily to another group that is listed in the output of the groups and id commands, you can use the newgrp command. Any new files created afterward will then have the new group owner. The following output demonstrates how changing the primary group for the root user affects file ownership:

 Enlarge Image
Note  
If you use group passwords as described earlier in this section, you can use the newgrp command to change your primary group to a group of which you are not a member, provided you supply the appropriate group password when prompted.

The root user can use the newgrp command to change her primary group to any other group.

Although command-line utilities are commonly used to administer users and groups, you can instead use a graphical utility to create, modify, and delete user and group accounts on the system. These utilities run the appropriate command-line utility in the background. To create and manage users and groups in Fedora 28 from within a desktop environment, you can open a terminal application in the desktop environment, switch to the root user, and run the system-config-users command to open the User Manager utility shown in Figure 10-6.

Figure 10-6
Configuring users and groups within a desktop environment

Figure shows the User Manger window that helps configure users and groups within a desktop environment. The window has icons that allow to add a user and group. Tabs are present for viewing the list of user and groups. The user tab is currently selected and the details of user 1 is listed. User 1 has the user I d 1000. The primary group is listed as user 1 and the full name is Jason Eckert. The login shell is slash bin slash bash. The home directory is slash home slash user 1. The nobody user is also listed whose user i d is 65534. The primary group of this user is nobody. The full name is listed as Kernel Overflow User. The login shell is slash s bin slash no login. The home directory is root. The user manager window also has a search filter.

At times, you might want to reduce the size of a file or set of files due to limited disk space. You might also want to compress files that are sent across the Internet to decrease transfer time. In either case, you can choose from several utilities that reduce a file’s size by stripping out certain patterns of data via a process known as compression. The standard set of instructions used to compress a file is known as a compression algorithm. To decompress a file, you run the compression algorithm in reverse.

Because compression utilities use compression algorithms in different ways, they achieve different rates of compression, or compression ratios, for similar files types. To calculate the compression ratio for a utility, you subtract the compressed percentage from 100. For example, if a compression utility compresses a file to 52 percent of its original size, it has a compression ratio of 48 percent.

Many compression utilities are available to Linux users; this section examines the five most common:

compress

GNU zip

xz

zip

bzip2

Using compress
The compress command is one of the oldest compression utilities common to most UNIX and Linux systems. Its compression algorithm, which is called Adaptive Lempel-Ziv coding (LZW), has an average compression ratio of 40–50 percent.

To compress a file using compress, you specify the files to compress as arguments. Each file is renamed with a .Z filename extension to indicate that it is compressed. In addition, you can use the –v (verbose) option to the compress command to display the compression ratio during compression. The following output displays the filenames and size of the samplefile and samplefile2 files before and after compression:

 Enlarge Image
Note  
The compress command is not installed on most Linux distributions by default. To install it from a software repository on the Internet, you can run the dnf install ncompress command as the root user on Fedora 28, or the apt-get install ncompress command as the root user on Ubuntu Server.

The compress command preserves the original ownership, modification, and access time for each file that it compresses.

By default, compress does not compress symbolic links, hard links, or very small files unless you use the –f option.

You can compress all of the files in a certain directory by using the –r option and specifying the directory name as an argument to the compress command.

After compression, the zcat command can be used to display the contents of a compressed file, as shown in the following output:

 Enlarge Image
Note  
You can also use the zmore command and zless command to view the contents of a compressed file page by page, or the zgrep command to search the contents of a compressed file.

To decompress files that have been compressed with the compress command, use the uncompress command followed by the names of the files to be decompressed. This restores the original filename. The following output decompresses and displays the filenames for the samplefile.Z and samplefile2.Z files created earlier:

 Enlarge Image
Note  
The uncompress command prompts you for confirmation if any existing files will be overwritten during decompression. To prevent this confirmation, you can use the –f option to the uncompress command.

You can omit the .Z extension when using the uncompress command. The command uncompress –v samplefile samplefile2 would achieve the same results as the command shown in the preceding output.

Furthermore, the compress utility is a filter command that can take information from Standard Input and send it to Standard Output. For example, to send the output of the who command to the compress utility and save the compressed information to a file called file.Z, you can execute the following command:

 
Following this, you can display the contents of file.Z using the zcat command, or decompress it using the uncompress command, as shown in the following output:

 
Table 11-1 provides a summary of options commonly used with the compress utility.

Table 11-1
Common Options Used with the Compress Utility
Option	Description
-c	When used with the uncompress command, it displays the contents of the compressed file to Standard Output (same function as the zcat command).
-f	When used with the compress command, it can be used to compress linked files. When used with the uncompress command, it overwrites any existing files without prompting the user.
-r	Specifies to compress or decompress all files recursively within a specified directory.
-v	Displays verbose output (compression ratio and filenames) during compression and decompression.


Using GNU zip
GNU zip uses a Lempel-Ziv compression algorithm (LZ77) that varies slightly from the one used by the compress command. Typically, this algorithm yields better compression than the one used by compress. The average compression ratio for GNU zip is 60–70 percent. To compress files using GNU zip, you can use the gzip command.

Like compress, linked files are not compressed by the gzip command unless the –f option is given, and the –r option can be used to compress all files in a certain directory. In addition, the ownership, modification, and access times of compressed files are preserved by default, and the –v option to the gzip command can be used to display the compression ratio and filename. However, gzip uses the .gz filename extension by default.

To compress the samplefile and samplefile2 files shown earlier and view the compression ratio, you can use the following command:

 Enlarge Image
Because GNU zip uses the same fundamental compression algorithm as compress, you can use the zcat, zmore, zless, and zgrep commands to send the contents of a gzip-compressed file to Standard Output. Similarly, the gzip command can accept information via Standard Input. Thus, to compress the output of the date command to a file called file.gz and view its contents afterward, you can use the following commands:

 
To decompress the file.gz file in the preceding output, you can use the –d option to the gzip command, or the gunzip command, as shown in the following output:

 
Like the uncompress command, the gunzip command prompts you to overwrite existing files unless the –f option is specified. Furthermore, you can omit the .gz extension when decompressing files, as shown in the following example:

 Enlarge Image
One of the largest advantages that gzip has over compress is its ability to control the level of compression via a numeric option. The -1 option is also known as fast compression and results in a lower compression ratio. Alternatively, the -9 option is known as best compression and results in the highest compression ratio at the expense of time. If no level of compression is specified, the gzip command assumes the -6 option.

The following command compresses the samplefile file shown earlier using fast compression and displays the compression ratio:

 Enlarge Image
Notice from the preceding output that samplefile was compressed with a compression ratio of 78.8 percent, which is lower than the compression ratio of 82.2 percent obtained earlier when samplefile was compressed with the default level of 6.

Note  
You need not specify the level of compression when decompressing files, as it is built into the compressed file itself.

Many more options are available to gzip than to compress, and many of these options have a POSIX option equivalent. Table 11-2 shows a list of these options.

Table 11-2
Common GNU Zip Options
Option	Description
-#

When used with the gzip command, it specifies how thorough the compression will be, where # can be the number 1–9. The option -1 represents fast compression, which takes less time to compress but results in a lower compression ratio. The option -9 represents thorough compression, which takes more time but results in a higher compression ratio.

--best

When used with the gzip command, it results in a higher compression ratio; same as the -9 option.

-c

--stdout

--to-stdout

Displays the contents of the compressed file to Standard Output (same function as the zcat command) when used with the gunzip command.

-d

--decompress

--uncompress

Decompresses the files specified (same as the gunzip command) when used with the gzip command.

-f

--force

Compresses linked files and files with special permissions set when used with the gzip command. When used with the gunzip command, it overwrites any existing files without prompting the user.

--fast

When used with the gzip command, it results in a lower compression ratio; same as the -1 option.

-h

--help

Displays the syntax and available options for the gzip and gunzip commands.

-l

--list

Lists the compression ratio for files that have been compressed with gzip.

-n

--no-name

Does not allow gzip and gunzip to preserve the original modification and access time for files.

-q

--quiet

Suppresses all warning messages.

-r

--recursive

Specifies to compress or decompress all files recursively within a specified directory.

-S .suffix

--suffix .suffix

Specifies a file suffix other than .gz when compressing or decompressing files.

-t

--test

Performs a test decompression such that a user can view any error messages before decompression, when used with the gunzip command; it does not decompress files.

-v

--verbose

Displays verbose output (compression ratio and filenames) during compression and decompression.

Using xz
Like compress and gzip, the xz command can be used to compress files and Standard Input using a Lempel-Ziv compression algorithm (LZMA); however, it uses a different implementation that typically yields a higher compression ratio compared to compress and gzip for most files (60-80 percent, on average). Additionally, xz uses the .xz extension for compressed files by default, and implements the same gzip options shown in Table 11-2 with the exception of -n and -r. To decompress xz-compressed files, you can use the -d option to the xz command, or the unxz command.

The following example compresses the samplefile and samplefile2 files shown earlier with the highest possible compression ratio using xz, as well as decompresses the same files using unxz.

 Enlarge Image
Note  
Unlike uncompress and gunzip, you must include the filename extension when decompressing files using the unxz command.

You can use the xzcat, xzmore, xzless, or xzgrep command to send the contents of an xz-compressed file to Standard Output.

The zip command is a Linux implementation of the cross-platform PKZIP utility, which was originally designed to compress multiple files into a single compressed file. Because it compresses files and Standard Input using the same implementation of the Lempel-Ziv compression algorithm that gzip uses, it often yields a similar compression ratio on average. The zip command compresses linked files, as well as preserves file ownership, modification, and access time during compression. While there is no default file extension used for zip-compressed archives, .zip is often used by convention.

Note  
The zip command is not installed on Ubuntu Server by default. To install it from a software repository on the Internet, you can run the apt-get install zip command as the root user.

The following example compresses the samplefile and samplefile2 files shown earlier into a single samplefile.zip file and displays the compression ratio (-v) during the process:

 Enlarge Image
Note  
You can use the zcat, zmore, zless, or zgrep commands to send the contents of a zip-compressed file to Standard Output.

To view the contents of, or extract a zip-compressed file, you can use the unzip command. The following example views the contents of the samplefile.zip archive, and then extracts those contents to the current folder:

 Enlarge Image
Table 11-3 provides a summary of options commonly used with the zip utility.

Table 11-3
Common Options Used with the Zip Utility
Option	Description
-#

When used with the zip command, it specifies how thorough the compression will be, where # can be the number 1–9. The option -1 represents fast compression, which takes less time to compress but results in a lower compression ratio. The option -9 represents thorough compression, which takes more time but results in a higher compression ratio.

-c

Displays the contents of the compressed file to Standard Output (same function as the zcat command) when used with the unzip command.

-d directory

When used with the unzip command, it extracts the contents of the zip-compressed file to the specified directory.

-e

When used with the zip command, it encrypts the contents of the zip-compressed file using a password that the user is prompted to enter following the command. The user will be prompted for this password when using the unzip command.

-h

--help

Displays the syntax and available options for the zip and unzip commands.

-m

--move

When used with the zip command, it moves the specified files into the zip-compressed file.

-q

--quiet

Suppresses all warning messages.

-r

--recurse-path

When used with the zip command, it compresses all files recursively within a specified directory.

-v

--verbose

When used with the zip command, it displays the compression ratio and filenames. When used with the unzip command, it displays the contents and compression ratios of the files within the zip-compressed file.
Using bzip2
The bzip2 command differs from the compress, gzip, zip, and xz commands previously discussed in that it uses the Burrows-Wheeler Block Sorting Huffman Coding algorithm when compressing files, which is better suited to compressing already compressed files as well as files with binary data contents. It provides a compression ratio of 60–80 percent on average.

As with compress and gzip, symbolic links are only compressed if the –f option is used, and the –v option can be used to display compression ratios. Also, file ownership, modification, and access time are preserved during compression.

The filename extension given to files compressed with bzip2 is .bz2. To compress the samplefile and samplefile2 files and view their compression ratios and filenames, you can use the following commands:

 Enlarge Image
Because the compression algorithm is different from the one used by compress, gzip, zip, and xz, you must use the bzcat command to display the contents of bzip2-compressed files to Standard Output, as shown in the following example:

 Enlarge Image
Note  
You can also use the bzmore and bzless commands to view the contents of a bzip2-compressed file page by page, or the bzgrep command to search the contents of a bzip2-compressed file.

To decompress files, you can use the bunzip2 command followed by the filename(s) to decompress; the following command decompresses the samplefile.bz2 and samplefile2.bz2 files created earlier and displays the results:

 Enlarge Image
If any files are about to be overwritten, the bunzip2 command prompts the user for confirmation. To skip this confirmation, you can include the –f option. Table 11-4 lists other common options used with the bzip2 utility.

Table 11-4
Common Options Used with the bzip2 Utility
Option	Description
-#

When used with the bzip2 command, it specifies the block size used during compression; -1 indicates a block size of 100KB, whereas -9 indicates a block size of 900KB.

-c

--stdout

Displays the contents of the compressed file to Standard Output when used with the bunzip2 command.

-d

--decompress

Decompresses the files specified (same as the bunzip2 command) when used with the bzip2 command.

-f

--force

Compresses symbolic links when used with the bzip2 command. When used with the bunzip2 command, it overwrites any existing files without prompting the user.

-k

--keep

Keeps the original file during compression; a new file is created with the extension .bz2.

-q

--quiet

Suppresses all warning messages.

-s

--small

Minimizes memory usage during compression.

-t

--test

Performs a test decompression when used with the bunzip2 command, such that a user can view any error messages before decompression; it does not decompress files.

-v

--verbose

Displays verbose output (compression ratio) during compression and decompression.

System Backup
It’s a good idea to create backup copies of files and directories regularly and store them at an alternate location. You can then distribute these backup copies to other computers or use them to restore files lost as a result of a system failure or user error. This entire process is known as system backup, and the backup copies of files and directories are called archives.

You can create an archive on many types of media, such as tapes, CDs, and DVDs. Alternatively, you can create an archive within a single file stored in an existing filesystem on a USB flash drive, hard disk, or SSD. Traditionally, tapes were used to back up data, and while some organizations still use tapes to store archives, most archives are stored within files on a filesystem today. Larger backups are typically performed to files stored on hard disks, while smaller backups are typically performed to files stored on USB flash drives. Some organizations perform smaller backups to DVDs, which require special disc burning software described later in this chapter.

Table 11-5 shows a list of some common device files used for tape devices.

Table 11-5
Common Tape Device Files
Device file	Description
/dev/st0	First SCSI tape device (rewinding)
/dev/st1	Second SCSI tape device (rewinding)
/dev/st2	Third SCSI tape device (rewinding)
/dev/nst0	First SCSI tape device (nonrewinding)
/dev/ht0	First ATAPI IDE tape device (rewinding)
/dev/nht0	First ATAPI IDE tape device (nonrewinding)
After creating an archive within a file on a filesystem, it is good practice to copy that file to another server across the Internet; this is called an offsite backup and ensures that data can be recovered following a catastrophic event, such as a building fire. You can use the sftp (secure FTP) command, scp (secure copy) command or rsync (remote sync) command to copy an archive file to a remote server across the Internet; you examine these commands in later chapters.

A typical Linux system can include hundreds of thousands of files, but you don’t have to include all of them in an archive. For example, you don’t have to include temporary files in the /tmp and /var/tmp directories.

As a rule of thumb, you should back up files used by system services. For example, you should back up website content files if the Linux computer is used as a Web server, and database files if the Linux computer hosts a database server. In addition, you should back up user files from home directories and any important system configuration files such as /etc/passwd. Operating system components and programs such as grep and vi need not be backed up because they can be reinstalled in the event of a system failure.

After files have been selected for system backup, you can use a backup utility to copy the files to the appropriate media or archive file on a filesystem. Several backup utilities are available to Linux administrators. The most common are the following:

tape archive (tar)

copy in/out (cpio)

dump/restore

dd

disc burning software

Note  
In addition to the utilities mentioned here, many third-party commercial backup software suites can be used to back up data on multiple computers across the network; common examples include Arcserve Backup, Veritas NetBackup, NetApp, and Acronis Backup.

While software RAID, ZFS, and BTRFS can be configured to perform fault tolerance for data, you should still regularly back up the data stored on these volumes to ensure that data can be recovered if an entire software RAID, ZFS, or BTRFS volume fails.

Some storage technologies provide features that can provide duplicate copies of data for backup purposes. For example, ZFS supports snapshot clones, which can be used to create backup copies of ZFS filesystems, and the LVM supports automatic mirroring of logical volumes, which can be used to store backup copies of logical volumes on different physical volumes.

Using Tape Archive (Tar)
The tape archive (tar) utility is one of the oldest, most widely used backup utilities and is executed via the tar command. It can create an archive in a file on a filesystem or directly on a device.

Like the compression utilities discussed earlier, the tar command accepts options to determine the location of the archive and the action to perform on the archive. Any arguments specified to the tar command list the file(s) to place in the archive. Table 11-6 lists common options used with the tar command.

Table 11-6
Common Options Used with the tar Command
Option	Description
-A

--catenate

--concatenate

Appends whole archives to another archive

-c

--create

Creates a new archive

--exclude PATTERN

Excludes files that have a matching PATTERN in their filename when creating an archive

-f FILENAME

--file FILENAME

Specifies the location of the archive (FILENAME); it can be a file on a filesystem or a device file

-h

--dereference

Prevents tar from backing up symbolic links; instead, tar backs up the target files of symbolic links

-j

--bzip

Compresses/decompresses the archive using the bzip2 utility

-J

--xz

Compresses/decompresses the archive using the xz utility

-P

--absolute-paths

Stores filenames in an archive using absolute pathnames
-r

--append

Appends files to an existing archive

--remove-files

Removes files after adding them to an archive

-t

--list

Lists the filename contents (table of contents) of an existing archive

-u

--update

Appends files to an existing archive only if they are newer than the same filename inside the archive

-v

--verbose

Displays verbose output (file and directory information) when manipulating archives

-w

--interactive

--confirmation

Prompts the user for confirmation of each action

-W

--verify

Verifies the contents of each archive after creation

-x

--extract

--get

Extracts the contents of an archive

-z

--gzip

--ungzip

Compresses/decompresses the archive using the gzip utility

-Z

--compress

--uncompress

Compresses/decompresses the archive using the compress utility

Enlarge Table
To create an archive called /backup.tar that contains the contents of the current directory and view the results, you can use the following commands:

 Enlarge Image
Note from the preceding command that the –f option is followed by the pathname of the archive and that the * metacharacter indicates that all files in the current directory will be added to this archive. Also note that files are backed up recursively by default and stored using relative pathnames; to force the use of absolute pathnames when creating archives, use the –P option to the tar command.

Note  
The filename used for an archive need not have an extension. However, it is good practice to name archive files with an extension to identify their contents, as with /backup.tar in the preceding example.

The tar utility cannot back up device files or files with filenames longer than 255 characters.

After creating an archive, you can view its detailed contents by specifying the –t (table of contents) option to the tar command and the archive to view. For example, to view the detailed contents of the /backup.tar archive created earlier, you can issue the following command:

 Enlarge Image
You can use the –x option with the tar command to extract a specified archive. To extract the contents of the /backup.tar file to a new directory called /tartest and view the results, you can issue the following commands:

 
After an archive has been created in a file on a filesystem, that file can be sent to other computers across a network or the Internet. This is the most common form of backup today and a common method used to distribute software across the Internet. Unfortunately, the tar utility does not compress files inside the archive. Thus, the time needed to transfer the archive across a network is high. To reduce transfer times, you can compress the archive using a compression utility before transmission. Because this is a common task, the tar command accepts options that allow you to compress an archive immediately after creation using the compress, gzip, xz, or bzip2 command.

To create a gzip-compressed archive called /backup.tar.gz that contains the contents of the current directory and view the results, you can use the following commands:

 Enlarge Image
Note in the preceding output that the –z option indicated compression using the gzip utility, and that we chose to end the filename with the .tar.gz extension. In addition, the size of the /backup.tar.gz file is much less than the /backup.tar file created earlier.

Note  
Filenames that end with the .tar.gz or .tgz extension are commonly called tarballs because they represent compressed tar archives.

To view the contents of a gzip-compressed archive, you must use the –z option in addition to the –t option followed by the archive to view. The detailed contents of the /backup.tar.gz file can be viewed using the following command:

 Enlarge Image
Similarly, when extracting a gzip-compressed archive, you must supply the –z option to the tar command. To extract the contents of the /backup.tar.gz file to a new directory called /tartest2 and view the results, you can issue the following commands:

 
Backing up files to a compressed archive on a filesystem is useful when you plan to transfer the archived data across a network. However, you can use tar to back up data directly to a device such as a tape. To back up files to a device, you can use the –f option to the tar command to specify the pathname to the appropriate device file. Files are then transferred directly to the device, overwriting any other data or filesystems that might be present.

For example, to create an archive on the first rewinding SCSI tape device containing the contents of the current directory, you can use the following command:

 
You can then view the contents of the archive on the tape device used in the preceding example using the command tar –tvf /dev/st0 or extract the contents of the archive on the tape device using the command tar –xvf /dev/st0 in a similar fashion to the examples shown earlier.

Because tape devices can hold large amounts of information, you might want to add to a tar archive that already exists on the tape device. To do this, replace the –c option with the –r option when using the tar utility. 

Using Copy In/Out (Cpio)
Another common backup utility is copy in/out (cpio), which can be executed via the cpio command. Although cpio uses options similar to tar, it has some added features, including long filenames and the ability to back up device files.

Because its primary use is to back up files in case of system failure, cpio uses absolute pathnames by default when archiving. In addition, cpio normally takes a list of files to archive from Standard Input and sends the files “out” to the archive specified by the –O option. Conversely, when extracting an archive, you must include the –I option to indicate the archive from which to read “in” files.

Table 11-7 provides a list of commonly used options to the cpio command and their descriptions.

Table 11-7
Common Options Used with the Cpio Utility
Option	Description
-A

--append

Appends files to an existing archive

-B

Changes the default block size from 512 bytes to 5KB, thus speeding up the transfer of information

-c

Uses a storage format (SVR4) that is widely recognized by different versions of cpio for UNIX and Linux

-d

--make-directories

Creates directories as needed during extraction
-i

--extract

Reads files from an archive

-I FILENAME

Represents the input archive; it is the file or device file of the archive used when viewing or extracting files

-L

--dereference

Prevents cpio from backing up symbolic links; instead, cpio backs up the target files of symbolic links

--no-absolute-filenames

Stores filenames in an archive using relative pathnames

-o

--create

Creates a new archive

-O FILENAME

Represents the output archive; it is the file or device file of the target archive when backing up files

-t

--list

Lists the filename contents (table of contents) of an existing archive

-u

--unconditional

Overwrites existing files during extraction without prompting for user confirmation

-v

--verbose

Displays verbose output (file and directory information) when manipulating archives

Enlarge Table
To create an archive using cpio, you must first generate a list of filenames. You can do this using the find command. To list all filenames under the /root/sample directory, you can use the following command:

 
Next, you can send this list via Standard Input to the cpio command. For example, to verbosely back up all files in /root/sample to the first SCSI tape device using a block size of 5KB and a common format, you can use the following command:

 Enlarge Image
To view the verbose table of contents of this archive, you can use the following command:

 Enlarge Image
Following this, you can extract the archive on /dev/st0, creating directories and overwriting files as needed by using the following command:

 
Like tar, the cpio command can be used to create an archive on a file on the filesystem; to do this, specify the filename after the –O option. To create an archive called /root/sample.cpio that contains the files from the directory /root/sample, using a block size of 5KB as well as a common header, and to view the results, you can issue the following commands:

 Enlarge Image
As with the tar utility, cpio archive filenames need not have an extension to identify their contents. However, it is good practice to use extensions, as shown with /root/sample.cpio in the preceding example.


Using Dump/Restore
Like tar and cpio, the dump command can be used to back up files and directories to a device or to a file on the filesystem, and the restore command can be used to restore those files and directories. However, dump and restore can only work with files on ext2, ext3, and ext4 filesystems.

Note  
The dump and restore commands are not installed on Fedora 28 or Ubuntu Server by default. To install them from a software repository on the Internet, you can run the dnf install dump command as the root user on Fedora 28, or the apt-get install dump command as the root user on Ubuntu Server.

Although dump can be used to back up only certain files and directories, it was designed to back up entire filesystems to an archive and keep track of these filesystems in a file called /etc/dumpdates. Because archiving all data on a filesystem (known as a full backup) might take a long time, you can choose to perform a full backup only on weekends and incremental backups each evening during the week. An incremental backup backs up only the data that has been changed since the last backup. In the case of a system failure, you can restore the information from the full backup and then restore the information from all subsequent incremental backups in sequential order. You can perform up to nine different incremental backups using dump; number 0 represents a full backup, whereas numbers 1 through 9 represent incremental backups.

Suppose, for example, that you perform a full backup of the /dev/sda3 filesystem on Sunday, perform incremental backups from Monday to Wednesday, and on Thursday the /dev/sda3 filesystem becomes corrupted, as depicted in Figure 11-1.

Figure 11-1
A sample backup strategy

Illustration shows a sample back up strategy. On weekends, that is Sunday, a full back up is taken followed by incremental back ups taken on Monday, Tuesday, and Wednesday.
After the filesystem has been re-created, you should restore the full backup (0) followed by the first incremental backup (1), the second incremental backup (2), and the third incremental backup (3) to ensure that data has been properly recovered.

Note  
Differential backups are an alternative to incremental backups; they back up only the data that has been changed since the last full backup. This allows you to recover data by restoring a full backup followed by the most recent differential backup only. While dump/restore cannot perform differential backups, many third-party software suites can.

The dump and restore commands have many options available; Table 11-8 provides a list of these options.

Table 11-8
Common Options Used with dump/restore
Option	Description
-#	Specifies the type of backup when used with the dump command; if # is 0, a full backup is performed. If # is 1 through 9, the appropriate incremental backup is performed.
-b SIZE	When used with the dump command, it specifies a certain block size to use in kilobytes; the default block size is 10KB.
-f FILENAME	Specifies the pathname to the archive; the FILENAME can be a file on a filesystem or a device file.
-u	Specifies to update the /etc/dumpdates file after a successful backup.
-n	When used with the dump command, it notifies the user if any errors occur and when the backup has completed.
-r	Extracts an entire archive when used with the restore command.
-x FILENAME	Extracts a certain file or files represented by FILENAME when used with the restore command.
-i	Restores files interactively, prompting the user for confirmation for all actions, when used with the restore command.
-t	Lists the filename contents (table of contents) of an existing archive when used with the restore command.
-v	Displays verbose output (file and directory information) when manipulating archives.
Enlarge Table
Take, for example, the output from the following df command:

 Enlarge Image
To perform a full backup of the /data partition (/dev/sda3) to the first rewinding SCSI tape device and update the /etc/dumpdates file when completed, you can issue the following command:

 Enlarge Image
Note  
Alternatively, you can specify the filesystem mount point when using the dump command. The command dump -0uf /dev/st0 /data is equivalent to the one used in the preceding example.

To create a dump archive within a file, you can specify a filename instead of a device file. For example, the dump -0uf /data-archive.dump /data would create the same archive used in the preceding example within the /data-archive.dump file.

The contents of the /etc/dumpdates file now indicate that a full backup has taken place:

 
To perform the first incremental backup and view the contents of the /etc/dumpdates file, you can place a new tape into the SCSI tape drive and issue the following commands:

 Enlarge Image
To view the contents of an archive, you can specify the –t option to the restore command followed by the archive information. To view the contents of the full backup performed earlier, you can place the appropriate tape into the tape drive and execute the following command:

 Enlarge Image
To extract the full backup shown in the preceding output, you can specify the –r option to the restore command followed by the archive information. In addition, you can specify the –v option to list the filenames restored, as shown in the following example:

 
Using dd
Unlike the tar, cpio, and dump commands, which back up data in a file-based format, the dd command backs up data block by block to an archive device or file, without keeping track of the files that the blocks comprise. This type of backup is called an image backup and is often used to create archives of entire filesystems or disk structures (e.g., MBR).

At minimum, the dd command requires an input file (if) and an output file (of) option to be specified. For example, to create an image backup of the filesystem on /dev/sda2 within the archive file /sda2.img, you could use the following command:

 Enlarge Image
Note  
By default, the dd command copies information 512 bytes at a time, but you can modify this behavior using the bs (block size) option. For example, dd if=/dev/sda2 of=/sda2.img bs=1M would transfer information 1MB at a time, which would result in a faster transfer rate compared to the default.

To restore the filesystem on /dev/sda2 from the /sda2.img archive file, you could unmount the existing filesystem from /dev/sda2 and run the dd command with the input and output files reversed:

 Enlarge Image
Following this, you can remount the filesystem normally and access the data within.

Note  
The dd command can also be used to back up specific storage locations, such as the MBR, which uses the first 512 bytes of a disk device. For example, to back up the MBR on /dev/sda to the /MBRarchive.img file, you could use the dd if=/dev/sda of=/MBRarchive.img bs=512 count=1 command.


Using Disc Burning Software
The backup utilities you’ve examined thus far are primarily used to create archive files on filesystems or tape devices. To create an archive on CD or DVD media, you must use a program that allows you to select the data to copy, organize that data, build a CD or DVD filesystem, and write the entire filesystem (including the data) to the CD or DVD. Recall from Chapter 2 that the programs that can be used to do this are called disc burning software. Figure 11-2 shows the Brasero disc burning software. You can install Brasero on a Fedora 28 system using the dnf install brasero command as the root user, and launch it from the GNOME desktop by navigating to Activities, Show Applications, Brasero. You can then click the Data project button shown in Figure 11-2, select the data that you wish to back up, and click Burn to write it to your CD or DVD drive.

Software Installation
Primary responsibilities of most Linux administrators typically include installing and maintaining software packages. Software for Linux can consist of binary files that have been precompiled to run on certain hardware architectures such as 64-bit Intel (x86_64), or as source code, which must be compiled on the local architecture before use. The largest advantage to obtaining and compiling source code is that the source code is not created for a particular hardware architecture. After being compiled, the program executes natively on the architecture from which it was compiled.

Note  
The most common method for obtaining software for Linux is via the Internet. Appendix C lists some common websites that host Linux Open Source Software for download.

When downloading software files from the Internet, you may notice that the Internet site lists a checksum value for the file, which was calculated from the exact file contents. To ensure that the file was received in its entirety after you download it, you should verify that the checksum value is still the same. You can use one of several commands, depending on the algorithm used to create the checksum. For example, you can use the md5sum programfile command to check an MD5 checksum, the sha1sum programfile command to check an SHA-1 checksum, the sha256sum programfile command to check an SHA-256 checksum, or the sha512sum programfile command to check an SHA-512 checksum.

Precompiled binary programs can also be distributed in tarball format, but they are typically distributed in a format for use with a package manager. Recall from Chapter 1 that a package manager provides a standard format for distributing programs as well as a central database to store information about software packages installed on the system; this allows software packages to be queried and easily uninstalled. Many Linux distributions today, including Fedora, use the Red Hat Package Manager (RPM). However, Debian and Debian-based Linux distributions, such as Ubuntu Linux, use the Debian Package Manager (DPM).

Note  
RPM and DPM are used by nearly all mainstream Linux distributions. However, they are not the only package managers available on Linux systems. For example, Arch Linux uses the Pacman package manager, and Gentoo Linux uses the Portage package manager.


Compiling Source Code into Programs
Program source code is typically obtained by cloning an online git repository as described in Chapter 7, or by downloading a tarball that contains the source code and extracting its contents. If your system has a GUI installed, you can download a source code tarball from the Internet using a Web browser. Alternatively, if your system does not have a GUI installed, you can use the wget (Web get) command or curl (client for URLs) command to download a source code tarball. For example, to download a sample source code tarball called sample.tar.gz from https://sample.com to your current directory, you could use the wget https://sample.com/sample.tar.gz or curl https://sample.com/sample.tar.gz --output sample.tar.gz command.

After you clone a git repository of source code, or download and extract the contents of a source code tarball, there will be a subdirectory under the current directory containing the source code. This directory typically contains a text file that starts with README or INSTALL with information about the program and instructions for installation.

While inside the source code directory, the first step to installation is to run the configure program. This performs a preliminary check for system requirements and creates a list of what to compile inside a file called Makefile in the current directory.

Next, you can type the make command, which looks for the Makefile file and uses the information in it to compile the source code into binary programs using the appropriate compiler program for the local hardware architecture. For example, software written in the C programming language is compiled using the GNU C Compiler (gcc). After compilation, the binary files the program comprises remain in the source code directory. To install the compiled program on your system, you must type make install to copy the newly compiled executable files to a directory listed in your PATH variable, as well as copy supporting files, such as man pages, to the correct location on the filesystem.

Note  
Most Linux programs compiled from source code are installed to a subdirectory of the /usr/local directory after compilation.

After the program has been compiled and copied to the correct location on the filesystem, you can remove the source code directory and its contents from the system.

Suppose, for example, that you download the source code tarball for conky (a desktop system monitor app) version 1.9.0 from the Internet at sourceforge.net:

 
The first step to installing this program is to extract the contents of the tarball, which will create a directory containing the source code and supporting files. Next, you can move to this directory and view the file contents, as shown in the following output:

 Enlarge Image
In the preceding output, you can see that a README, INSTALL, and configure file exist and that the configure file is executable. To execute the configure shell script without using the PATH variable, you can enter the following command:

 Enlarge Image
If the configure shell script produces errors instead of running successfully, you will likely need to install the specified shared library necessary to support the program, often using a package manager as described later in this chapter. Shared libraries are files that contain executable code that can be used by multiple, different programs; rather than implementing that code within a program, most program developers reuse the code from a shared library to save time. If the required shared library is for an optional program features, you can instead choose to disable the optional program feature by specifying an option to the configure script. To see a list of available options for your configure script, you can run the ./configure --help command.

After the configure script has run successfully, a Makefile exists in the current directory, as shown in the following output:

 Enlarge Image
This Makefile contains most of the information and commands necessary to compile the program. Some program source code that you download might contain commented lines that you need to uncomment to enable certain features of the program or to allow the program to compile on your computer architecture. Instructions for these commented areas are documented in the Makefile itself; thus, it is good form to read the Makefile after you run the configure script. You can also edit the Makefile if you want to change the location to which the program is installed. For example, the line prefix=/usr/local within the Makefile installs the program to subdirectories under /usr/local.


Working with the Red Hat Package Manager (RPM)
RPM is the most widely used format for Linux software distributed via the Internet on distributions derived from Red Hat Linux. RPM packages have filenames that indicate the hardware architecture for which the software was compiled and end with the .rpm extension. The following output indicates that the bluefish RPM package (a Web page editor) version 2.2.10-8 was compiled for Fedora 28 (fc28) on the Intel x86_64 platform:

 
To install an RPM package, you can use the –i option to the rpm command. In addition, you can use the –v and –h options to print the verbose information and hash marks, respectively, during installation.

Some RPM packages require that other RPM packages or shared libraries be installed on your system first. This type of relationship is known as a package dependency. If you attempt to install an RPM package that has package dependencies, you receive an error message that indicates the packages and shared libraries that need to be installed first. After installing these prerequisites, you can successfully install your desired RPM package. Say, for example, that you download the RPM package for the bluefish Web page editor and run the following command to install it:

 Enlarge Image
The error shown in the preceding output indicates that there are two package dependencies for the bluefish RPM package called bluefish-shared-data and libgucharmap. Consequently, you must download the bluefish-shared-data package and libgucharmap shared library (part of the gucharmap-libs package) for your architecture. Following this, you can run the following command to install all three packages:

 Enlarge Image
After you install an RPM package, the RPM database (stored within files in the /var/lib/rpm directory) is updated to contain information about the package and the files contained within. To query the full package name after installation, you can use the –q (query) option to the rpm command followed by the common name of the package:

 
In addition, you can add the –i (info) option to the preceding command to display the detailed package information for the bluefish package:

 Enlarge Image
Because the Red Hat Package Manager keeps track of all installed files, you can find the executable file for the bluefish program by using the –q and –l (list) options followed by the RPM package name to list all files contained within the package. The following command lists the first 10 files in the bluefish package:

 
From the preceding output, you can see that the pathname to the executable file is /usr/bin/bluefish, which resides in a directory in the PATH variable. Upon execution in a desktop environment, you see the screen depicted in Figure 11-4.

Figure 11-4
The bluefish program

Screenshot shows the user interface of the Blue Fish software which is a code editor. A menu bar, code entry screen, icons, and a filesystem browser and other features are seen.Enlarge Image
Conversely, you can find out to which RPM package a certain file belongs by using the –q and –f (file) options with the rpm command, followed by the filename:

 
To remove an RPM package from the system, you can use the –e option to the rpm command; all files that belong to the package will be removed as well. To remove the bluefish and bluefish-shared-data RPM packages and verify the deletion, you can use the following commands:

 Enlarge Image
Table 11-9 displays a list of common options used with the rpm command.

Table 11-9
Common Options Used with the rpm Command
Option	Description
-a

--all

Displays all package names installed on the system (when used with the –q option)

-c

--configfiles

Displays the locations of the configuration files for a package installed on the system (when used with the –q option)

--dump

Displays detailed information regarding configuration files for a package installed on the system (when used following the –q and –c options)

-e

--erase

Removes a specified package from the system

-F

--freshen

Upgrades a specified package only if an older version exists on the system

-f

--file

Displays the package to which the specified file belongs (when used with the –q option)

-h

--hash

Prints hash marks on the screen to indicate installation progress (when used with the –i option)

-i

--install

Installs a specified package (provided the –q option is not used)

-i

--info

Displays full information about the specified package (when used with the –q option)

-K

When used before a filename argument, validates the checksum listed within the RPM file

-l

--list

Lists the filenames the specified package comprises (when used with the –q option)

--nodeps

Forces the RPM to avoid checking for dependencies before installing packages (when used following the –i option)

-q

--query

Queries information about packages on the system

--test

Performs a test installation only (when used with the –i option)

-U

--upgrade

Upgrades a specified package; the package is installed even if no older version exists on the system

-V

--verify

Verifies the location of all files that belong to the specified package

-v

Prints verbose information when installing or manipulating packages

Enlarge Table
Note  
RPM packages can also be converted to cpio archives using the rpm2cpio command. This allows you to easily inspect or extract file contents from an RPM package without installing the package on the system.

There are thousands of different RPM packages available for free download on Internet servers called software repositories. Moreover, each RPM on a software repository may have several package dependencies. Luckily, you can use the yum (Yellowdog Updater Modified) command to search Internet software repositories for RPM packages that map to your architecture, and automatically install or upgrade those packages on your system. Prior to installing the desired RPM package, the yum command also downloads and installs any package dependencies. The /etc/yum.conf and /etc/yum.repos.d/* files are used to specify the locations of Internet software repositories.

While many current Linux distributions still use the yum command, Fedora 28 and some other Linux distributions use the dnf (Dandified YUM) command alongside the repository information stored in the /etc/yum.conf and /etc/yum.repos.d/* files.

Note  
The dnf command is simply a faster version of the yum command that provides the same core options and functionality.

The SUSE and openSUSE Linux distributions use the zypper command to provide the same functionality as the yum command.

To install a particular RPM package and its dependencies following a Fedora 28 installation, you can use the dnf install packagename command. Multiple software packages can be installed using a single dnf command. For example, the dnf install package1name package2name package3name command will install three packages, including any package dependencies.

Note  
You have used the dnf command in previous chapters to install several programs on your Fedora 28 virtual machine from software repositories on the Internet.

Because newer versions of RPM packages are frequently added to software repositories, you can also upgrade an installed package to the latest version using the dnf update packagename command, or the dnf upgrade packagename command (which also removes any obsolete packages during the process). For example, to upgrade your BASH shell to the latest version, you could use the following command and press y when prompted to download and install the latest packages:

 Enlarge Image
Note  
Without a package name argument, the dnf update command will attempt to update all installed packages. You can also use the dnf check-update command to check for installed software updates.

Note from the previous output that the dnf command did not find any other package dependencies for BASH that needed updating. You may also find that the dnf command tries several different software repositories that contain the necessary software (called software mirrors) before finding one that accepts a connection. This is common, because software repositories limit their concurrent download connections to allow for fast download. When a software repository reaches its concurrent connection limit, it returns a negative response to the dnf command and the dnf command searches for the RPM package on the next software mirror listed in the repository configuration files.

In some cases, you may not know the exact RPM package name to use alongside the dnf command. Luckily, you can use the dnf search keyword command to search a software repository by keyword, or the dnf list available command to list available RPM packages. The dnf grouplist available command will display a list of package group names that are available for installation. A package group is a group of related RPM packages that are often installed together to provide a key function. For example, to install the Development Tools package group on Fedora 28 (which contains a core set of development libraries and commands), you can use the dnf groupinstall "Development Tools" command.

You can use the dnf repolist command to view the software repositories that are configured within the /etc/yum.conf and /etc/yum.repos.d/* files on your system. Many software authors host their RPM packages on private software repositories and include instructions on their website that guide you through the process of adding the private software repository to your repository configuration files (usually by installing a special RPM package). For example, if you want to install the VideoLAN VLC media player on Fedora 28, you can access the instructions for Fedora 28 from the VideoLAN website (www.videolan.org). Because the VideoLAN VLC media player is hosted on the private RPM Fusion software repository, the VideoLAN website instructs you to run the dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm command to install the appropriate files to the /etc/yum.repos.d directory that list the locations of the RPM fusion repositories. Next, you can run the dnf install vlc command to install the VLC media player.

Note  
The dnf command caches repository information and packages. You can run the dnf clean all command to clear these caches, which is often used to fix dnf-related problems.

The dnf command can also be used to view and manage installed RPM packages. The dnf list installed command will list installed RPM packages, the dnf grouplist installed command will list installed package groups, the dnf info packagename command will display detailed information for an installed RPM package, and the dnf groupinfo packagegroupname command will display the contents of a package group. You can also use the dnf remove packagename command to remove an installed RPM package, or the dnf groupremove packagegroupname command to remove all of the RPM packages within a particular package group.

Each Linux distribution includes a graphical software app that provides desktop users with an easy method for managing the software on the system as well as installing or updating packages from available software repositories on the Internet. Similarly, any packages downloaded within a desktop environment are opened by this graphical software app by default, which automatically downloads and installs any dependencies before installing the package.

On Fedora 28, the graphical Software utility shown in Figure 11-5 is used to view, manage, and install software within a desktop environment. To start the Software utility within the GNOME desktop environment, navigate to Activities, Software.

Figure 11-5
The Software utility

The user interface of the software utility is displayed. This utility is used to search for and install new software in Linux. It can also be used to check what software is already installed and also update it. Within the utility, software categories and some software are listed.

Working with the Debian Package Manager (DPM)
The Debian Package Manager (DPM) is used by default on Linux distributions based on the Debian Linux distribution, such as Ubuntu Linux. Like RPM packages, DPM packages contain software that has been precompiled for a specific platform, and they may have package dependencies. However, DPM packages use the .deb extension and are installed and maintained by the dpkg command. For example, to install the bluefish Web page editor on an Ubuntu Linux system from the downloaded DPM file bluefish_2.2.6-1_amd64.deb, you can use the dpkg -i bluefish_2.2.6-1_amd64.deb command. Table 11-10 displays a list of common options used with the dpkg command.

Table 11-10
Common Options Used with the dpkg Command
Option	Description
-i

--install

Installs a specified package

--ignore-

depends

Ignores any dependency checking for a specified package

-l

--list

Displays information for a specified package by name or wildcard pattern (or all packages if no argument is given); it may also be used with the dpkg-query command

-L

--listfiles

Displays the files that comprise a specified package; it may also be used with the dpkg-query command

-p

--print-avail

Displays detailed package information for a specified package; it may also be used with the dpkg-query command

-P

--purge

Removes a specified package from the system, including any configuration files used by the package

-r

--remove

Removes a specified package from the system, excluding any configuration files used by the package

-S

--search

Displays the package to which the specified file belongs; it may also be used with the dpkg-query command

-s

--status

Displays the status for a specified package; it may also be used with the dpkg-query command

--test

Performs a test installation only (when used with the –i option)

-V

--verify

Verifies the location of all files that belong to the specified package

Enlarge Table
Note  
The amd64 architecture is identical to the x86_64 architecture; you will often find amd64 used in place of x86_64 within DPM package names.

Some DPM packages prompt you to supply configuration options during installation. You can use dpkg-reconfigure command after package installation to modify these configuration options.

After you install a DPM package, the DPM database (stored within files in the /var/lib/dpkg directory) is updated to contain information about the package and the files contained within. As with the RPM, you can query package information for installed packages. For example, to list information about the BASH shell package, you can use the following command:

 Enlarge Image
Note  
You can instead use the dpkg-query command when searching for DPM package information. The dpkg-query command takes the same search arguments as the dpkg command.

Alternatively, you could use the following command to list detailed information about the BASH shell package:

 Enlarge Image
Because the Debian Package Manager keeps track of all installed files, you can also list the files that belong to a DPM package, verify DPM package contents, or search for the DPM package name for a specified file using the appropriate options from Table 11-10. For example, the following command lists the first 10 files in the BASH shell DPM package:

 
As with RPM, most DPM packages are hosted on Internet software repositories for free download. To download and install DPM packages, including any package dependencies, you can use the apt (Advanced Package Tool) command or apt-get command. You can search available repository information using the apt command or the apt-cache command.

Note  
The apt command is a newer alternative to the apt-get and apt-cache commands. It uses the same core options, but provides more detailed and useful output on the terminal screen. As a result, this section focuses primarily on the apt command.

In Chapter 6, you used the apt-get command to install the DPM packages required for ZFS, as well as used the apt command to install the DPM package for the lsscsi command on your Ubuntu Server 18 virtual machine.

The apt command is as easy to use as the dnf command with the RPM. For example, to install the nmap DPM package (a network port scanner) from an Internet software repository, you could run the following command:

 Enlarge Image
To remove the nmap DPM package, excluding any nmap configuration files, you could use the apt remove nmap command. Alternatively, you could use the apt purge nmap command to remove the nmap DPM package, including all nmap configuration files. If you remove a package, the package dependencies are not removed; however, you can use the apt autoremove command to remove package dependencies that are no longer required by programs on the system.

You can also use the apt update command to update the list of available DPM packages from Internet software repositories, as well as the apt upgrade command to upgrade the DPM packages on your system to the latest versions based on the updated package list.

Note  
All of the apt commands discussed thus far have an apt-get equivalent. For example, the apt install nmap command is equivalent to the apt-get install nmap command, and the apt purge nmap command is equivalent to the apt-get purge nmap command.

DPM repository information is stored in the /etc/apt/sources.list file and files within the /etc/apt/sources.list.d directory. You can add new repositories using the add-apt-repository command and search available repository and package information using either the apt or apt-cache command. For example, to search software repositories for DPM packages that contain the word nmap, you could use the apt search nmap or apt-cache search nmap command. To list all DPM packages available on software repositories, you could use the apt search . or apt-cache search . command. To view detailed information regarding the nmap package, use the apt show nmap or apt-cache show nmap command.

Note  
You used the add-apt-repository command in Chapter 6 to add the ZFS software repository to your Ubuntu Server 18 virtual machine.

You can also use the Aptitude utility (shown in Figure 11-6) to install, query, and remove DPM packages. You can start this utility by executing the aptitude command in a command-line or graphical terminal. In a command-line terminal, you can access the different Aptitude menus by pressing the Ctrl+t key combination. The aptitude command also accepts many of the same arguments as apt and apt-get. For example, aptitude install nmap will install the nmap DPM package from an Internet repository, aptitude remove nmap will remove the installed nmap DPM package (excluding nmap configuration files), and aptitude purge nmap will remove the installed nmap DPM package (including nmap configuration files).

Figure 11-6
The Aptitude utility

The user inteface of the aptitude utility is displayed. Aptitude is a command line based front end to the debian package manager.
Note  
The Aptitude utility is not installed in Ubuntu 18 by default. It can be installed using the apt install aptitude command.

Understanding Shared Libraries
Recall that when you install an RPM or DPM package, the package manager does a preliminary check to ensure that all shared libraries and prerequisite packages (package dependencies) required for the program to work properly have been installed. If you are using yum, dnf zypper, apt-get, or apt to install the package, then the package dependencies are automatically downloaded and installed; however, if you are using the rpm or dpkg command to install the package, then the installation will fail if the package dependencies are not installed. Similarly, when you compile a program from source code, the configure script checks for shared library dependencies; if those shared libraries are not present, the configure script will fail to create the Makefile necessary to compile the program.

If you uninstall an RPM or DPM package, including packages that contain shared libraries, the removal process normally stops if that package is a dependency for another package. However, this dependency check sometimes fails, the package dependency is removed from the system, and any programs that require the package dependency will fail to execute properly. Furthermore, if you remove an RPM or DPM package containing a shared library that is a dependency for other programs that were compiled from source code on the system, the removal process will be unaware that the package is a dependency. In this case, the package will be removed from the system and the programs that were compiled from source code that require the shared library will fail to execute.

Shared libraries are typically installed under the /lib, /lib64, /usr/lib, or /usr/lib64 directories. To identify which shared libraries are required by a certain program, you can use the ldd command. For example, the following output displays the shared libraries required by the /bin/bash program:

 Enlarge Image
If any shared libraries listed by the ldd command are missing, the ldd command will identify them as “not found” in the output. In this case, you can locate the appropriate RPM or DPM package that contains the library and install it on your system. After downloading and installing any shared libraries, it is good practice to run the ldconfig command to ensure that the list of shared library directories (/etc/ld.so.conf) and the list of shared libraries (/etc/ld.so.cache) are updated. Alternatively, you can create a variable in your BASH shell called LD_LIBRARY_PATH that lists the directories that contain the shared libraries.

Sometimes, the files that comprise a shared library can become corrupted. If this is the case, output of the ldd command will not report any problems, but the programs that depend on the shared library will produce errors that identify the shared library that is not functioning properly. In this case, you should reinstall the RPM or DPM package that contains the shared library to remedy these errors. For example, to reinstall the samplelibrary RPM package, you could use the dnf resinstall samplelibrary command, and to reinstall the samplelibrary DPM package, you could use the apt install --reinstall samplelibrary command.

Networks
Most functions that computers perform today involve the sharing of information between computers. Information is usually transmitted from computer to computer via media such as fiber optic, telephone, coaxial, or unshielded twisted pair (UTP) cable, but it can also be transmitted via wireless media such as radio, micro, or infrared waves. These media typically interact directly with a peripheral card on the computer, such as a network interface or modem device.

A network consists of two or more computers that are linked via media in order to exchange information. Networks that connect computers within close proximity are called local area networks (LANs), whereas networks that connect computers separated by large distances are called wide area networks (WANs).

Many companies use LANs to allow employees to connect to databases and other shared resources such as shared files and printers. Home users can also use LANs to connect several home computers together. As an alternative, home users can use a WAN to connect home computers to an Internet service provider (ISP) to gain access to resources such as websites on the worldwide public network called the Internet.

Note  
The Internet (the name is short for “internetwork”) is merely several interconnected public networks. Both home and company networks can be part of the Internet. Special computers called routers transfer information from one network to another.

Network media serve as the conduit for information as it travels across a network. But sending information through this conduit is not enough. In order for the devices on the network to make sense of this information, it must be organized according to a set of rules, or protocols. A network protocol breaks information down into packets that can be recognized by workstations, routers, and other devices on a network.

While you can configure many network protocols in Linux, nearly all Linux computers use the following three protocols by default:

Transmission Control Protocol/Internet Protocol (TCP/IP), which provides reliable communication of packets across the Internet.

User Datagram Protocol/Internet Protocol (UDP/IP), which provides fast, yet unreliable communication of packets across the Internet.

Internet Control Message Protocol (ICMP), which is used to send network-related information and error messages across the Internet.

Note  
While both TCP/IP and UDP/IP can send information packets across the Internet, TCP/IP is the most common network protocol used today, and the one that we’ll focus on within this chapter.

When transmitting information across a WAN, you might use a WAN protocol in addition to a specific LAN protocol to format packets for safer transmission. The most common WAN protocol is Point-to-Point Protocol (PPP).

Another important part of the puzzle, the media access method, is a set of rules that govern how the various devices on the network share the network media. The media access method is usually contained within the hardware on the network interface or modem. Although many media access methods are available, the one most commonly used to send packets onto network media is called Ethernet. It ensures that any packets are retransmitted onto the network if a network error occurs. Another media access method, Token Ring, controls which computer has the ability to transmit information by passing a special packet of information, called a token, around the network. Only the computer that currently has the token can transmit information.

Note  
Wireless-Fidelity (Wi-Fi) LANs also use Ethernet to place packets onto the network medium (in this case, the air).


The IP Protocol
TCP/IP is actually a set, or suite, of protocols with two core components: TCP and IP. Together, these two protocols ensure that information packets travel across a network as quickly as possible, without getting lost or mislabeled.

When you transfer information across a network such as the Internet, that information is often divided into many thousands of small IP packets. Each of these packets may take a different physical route when reaching its destination as routers can transfer information to multiple interconnected networks. TCP ensures that packets can be assembled in the correct order at their destination regardless of the order in which they arrive. Additionally, TCP ensures that any lost packets are retransmitted.

Note  
UDP is an alternative to TCP that does not provide packet ordering or retransmission.

IP is responsible for labeling each packet with the destination address. As a result, each computer that participates on an IP network must have a valid Internet Protocol (IP) address that identifies itself to the IP protocol. Nearly all computers on the Internet use a version of the IP protocol called IP version 4 (IPv4). However, a smaller number of computers use a next-generation IP protocol called IP version 6 (IPv6). We will examine the structure and configuration of IPv4 and IPv6 in this chapter.


The IPv4 Protocol
To participate on an IPv4 network, your computer must have a valid IP address as well as a subnet mask. Optionally, you can configure a default gateway to participate on larger networks such as the Internet.

IPv4 Addresses
An IP address is a unique number assigned to the computer that identifies itself on the network, similar to a unique postal address that identifies your location in the world. If any two computers on the same network have the same IP address, it is impossible for information to be correctly delivered to them. Directed communication from one computer to another single computer using IP is referred to as a unicast.

The most common format for IPv4 addresses is four numbers called octets that are separated by periods. Each octet represents an 8-bit binary number (0–255). An example of an IP address in this notation is 192.168.5.69.

You can convert between decimal and binary by recognizing that an 8-bit binary number represents the decimal binary powers of two in the following order:

128 64 32 16 8 4 2 1

Thus, the number 255 is 11111111 (128+64+32+16+8+4+2+1) in binary, and the number 69 is 01000101 (64+4+1) in binary. When the computer looks at an IP address, the numbers are converted to binary. To learn more about binary/decimal number conversion, visit www.wikihow.com/Convert-from-Decimal-to-Binary.

All IPv4 addresses are composed of two parts: the network ID and the host ID. The network ID represents the network on which the computer is located, whereas the host ID represents a single computer on that network. No two computers on the same network can have the same host ID; however, two computers on different networks can have the same host ID.

The network ID and the host ID are similar to postal mailing addresses, which are made up of a street name and a house number. The street name is similar to a network ID. No two streets in the same city can have the same name, just as no two networks can have the same network ID. The host ID is like the house number. Two houses can have the same house number as long as they are on different streets, just as two computers can have the same host ID as long as they are on different networks.

Only computers with the same network ID can communicate with each other without the use of a router. This allows administrators to logically separate computers on a network; computers in the Accounting Department could use one network ID, whereas computers in the Sales Department could use a different network number. If the two departments are connected by a router, computers in the Accounting Department can communicate with computers in the Sales Department and vice versa.

Note  
If your IP network is not connected to the Internet, the choice of IP address is entirely up to you. However, if your network is connected to the Internet, you might need to use preselected IP addresses for the computers on your network. IP addresses that can be used on the public Internet are assigned by your Internet service provider.

The IP address 127.0.0.1 is called the loopback IP address. It always refers to the local computer. In other words, on your computer, 127.0.0.1 refers to your computer. On your coworker’s computer, 127.0.0.1 refers to your coworker’s computer.

Subnet Masks
Each computer with an IPv4 address must also be configured with a subnet mask to define which part of its IP address is the network ID and which part is the host ID. Subnet masks are composed of four octets, just like an IP address. The simplest subnet masks use only the values 0 and 255. An octet in a subnet mask containing 255 is part of the network ID. An octet in a subnet mask containing 0 is part of the host ID. Your computer uses the binary process called ANDing to find the network ID. ANDing is a mathematical operation that compares two binary digits and gives a result of 1 or 0. If both binary digits being compared have a value of 1, the result is 1. If one digit is 0 and the other is 1, or if both digits are 0, the result is 0.

When an IP address is ANDed with a subnet mask, the result is the network ID. Figure 12-1 shows an example of how the network ID and host ID of an IP address can be calculated using the subnet mask.

Figure 12-1
A sample IP address and subnet mask

A sample I p address and subnet mask is shown. The I p address is 144 dot 58 dot 0 dot 1. This I p address can be represented in binary notation as 1 0 0 1 0 0 0 0 dot 1 0 1 1 1 0 1 0 dot 0 0 0 0 0 0 0 0 dot 0 0 0 0 0 0 0 1. The subnet mask for this i p address is 255 dot 255 dot 0 dot 0. The first two octects in the subnet mask are the network portion and the last two octects are the host portion. The subnet mask can be represented in binary notation as 1 1 1 1 1 1 1 1 dot 1 1 1 1 1 1 1 1 dot 0 0 0 0 0 0 0 0 dot 0 0 0 0 0 0 0 0.
Thus, the IP address shown in Figure 12-1 identifies the first computer (host portion 0.1) on the 144.58 network (network portion 144.58).

Note  
IP addresses and their subnet masks are often written using the classless interdomain routing (CIDR) notation. For example, the notation 144.58.0.1/16 refers to the IP address 144.58.0.1 with a 16-bit subnet mask (255.255.0.0).

The IP addresses 0.0.0.0 and 255.255.255.255 cannot be assigned to a host computer because they refer to all networks and all computers on all networks, respectively. Similarly, using the number 255 (all 1s in binary format) in an IP address can specify many hosts. For example, the IP address 192.168.131.255 refers to all hosts on the 192.168.131 network; this IP address is also called the broadcast address for the 192.168.131 network.

Note  
A computer uses its IP address and subnet mask to determine what network it is on. If two computers are on the same network, they can deliver packets directly to each other. If two computers are on different networks, they must use a router to communicate.

Default Gateway
Typically, all computers on a LAN are configured with the same network ID and different host IDs. A LAN can connect to another LAN by means of a router, which has IP addresses for both LANs and can forward packets to and from each network. Each computer on a LAN can contain the IP address of a router in its IP configuration; any packets that are not destined for the local LAN are then sent to the router, which can forward the packet to the appropriate network or to another router. The IP address of the network interface on the router to which you send packets is called the default gateway.

A router is often a dedicated hardware device from a vendor such as Cisco, D-Link, or HP. Other times, a router is actually a computer with multiple network cards. The one consistent feature of routers, regardless of the manufacturer, is that they can distinguish between different networks and move (or route) packets between them. A router has an IP address on every network to which it is attached. When a computer sends a packet to the default gateway for further delivery, the address of the router must be on the same network as the computer, as computers can send packets directly to devices only on their own network.

IPv4 Classes and Subnetting
IPv4 addresses are divided into classes to make them easier to manage. The class of an IP address defines the default subnet mask of the device using that address. All of the IP address classes can be identified by the first octet of the address, as shown in Table 12-1.

Table 12-1
IP Address Classes
Class	Subnet mask	First octet	Maximum number of networks	Maximum number of hosts	Example IP address
A	255.0.0.0	1–127	127	16,777,214	3.4.1.99
B	255.255.0.0	128–191	16,384	65,534	144.129.188.1
C	255.255.255.0	192–223	2,097,152	254	192.168.1.1
D	N/A	224–239	N/A	N/A	224.0.2.1
E	N/A	240–254	N/A	N/A	N/A
Class A addresses use 8 bits for the network ID and 24 bits for the host ID. You can see this is true by looking at the subnet mask, 255.0.0.0. The value of the first octet will always be somewhere in the range 1 to 127. This means there are only 127 potential Class A networks available for the entire Internet. Class A networks are only assigned to very large companies and Internet providers.

Class B addresses, which are identified by the subnet mask 255.255.0.0, use 16 bits for the network ID and 16 bits for the host ID. The value of the first octet ranges from 128 to 191. There are 16,384 Class B networks, with 65,534 hosts on each network. Class B networks are assigned to many larger organizations, such as governments, universities, and companies with several thousand users.

Class C addresses, which are identified by the subnet mask 255.255.255.0, use 24 bits for the network ID and 8 bits for the host ID. The value of the first octet ranges from 192 to 223. There are 2,097,152 Class C networks, with 254 hosts on each network. Although there are very many Class C networks, they have a relatively small number of hosts; thus, they are suited only to smaller organizations.

Class D addresses are not divided into networks, and they cannot be assigned to computers as IP addresses; instead, Class D addresses are used for multicasting. Multicast addresses are used by groups of computers. A packet addressed to a multicast address is delivered to each computer in the multicast group. This is better than a broadcast message because routers can be configured to allow multicast traffic to move from one network to another. In addition, all computers on the network process broadcasts, while only computers that are part of that multicast group process multicasts. Streaming media and network conferencing software often use multicasting to communicate to several computers at once.

Like Class D addresses, Class E addresses are not typically assigned to a computer. Class E addresses are considered experimental and are reserved for future use.

Notice from Table 12-1 that Class A and B networks can have many thousands or millions of hosts on a single network. Because this is not practically manageable, Class A and B networks are typically subnetted. Subnetting is the process in which a single large network is subdivided into several smaller networks to control traffic flow and improve manageability. After a network has been subnetted, a router is required to move packets from one subnet to another.

Note  
You can subnet any Class A, B, or C network.

To subnet a network, you take some bits from the host ID and give them to the network ID. Suppose, for example, that you want to divide the 3.0.0.0/8 network into 17 subnets. The binary representation of this network is:

 
You then borrow some bits from the host portion of the subnet mask. Because the number of combinations of binary numbers can be represented in binary powers of two, and because valid subnet masks do not contain all 0s or 1s, you can use the equation  to represent the minimum number of subnets required, where n is the number of binary bits that are borrowed from the host portion of the subnet mask. For our example, this is represented as:

 
Thus,  (because  is less than 15, but  is greater than 15). Following this, our subnet mask borrows four bits from the default Class A subnet mask:

 
Similarly, because there are 20 zeros in the preceding subnet mask, you can use the  equation to identify the number of hosts per subnet (the -2 accounts for the broadcast and network address for the subnet):

 
You can then work out the IP address ranges for each of the network ranges. Because four bits in the second octet were not borrowed during subnetting, the ranges of IP addresses that can be given to each subnet must be in ranges of . Thus, the first five ranges that can be given to different subnets on the 3.0.0.0/8 network that use the subnet mask 255.240.0.0 are as follows:

 
From the preceding ranges, a computer with the IP address 3.34.0.6/12 cannot communicate with the computer 3.31.0.99/12 because they are on different subnets. To communicate, there must be a router between them.

Note  
When subnetting a Class C network, ensure that you discard the first and last IP address in each range to account for the broadcast and network address for the subnet.

The IPv6 Protocol
As the Internet grew in the 1990s, ISPs realized that the number of IP addresses available using IPv4 was inadequate to accommodate future growth. As a result, the IPv6 protocol was designed in 1998 to accommodate far more IP addresses. IPv6 uses 128 bits to identify computers, whereas IPv4 only uses 32 bits (4 octets). This allows IPv6 to address up to 340,282,366,920,938,463,463,374,607,431,768,211,456 (or 340 trillion trillion trillion) unique computers.

IPv6 IP addresses are written using 8 colon-delimited 16-bit hexadecimal numbers—for example, 2001:0db8:3c4d:0015:0000:0000:adb6:ef12. If an IPv6 IP address contains 0000 segments, they are often omitted in most notation, thus 2001:0db8:3c4d:0015:::adb6:ef12 is equivalent to 2001:0db8:3c4d:0015:0000:0000:adb6:ef12. The IPv6 loopback address is 0000:0000:0000:0000:0000:0000:0000:0001, but it is often referred to as ::1 for simplicity.

Note  
Unlike our traditional decimal numbering scheme, hexadecimal uses an expanded numbering system that includes the letters A through F in addition to the numbers 0–9. Thus, the number 10 is called A in hexadecimal, the number 11 is called B in hexadecimal, the number 12 is called C in hexadecimal, the number 13 is called D in hexadecimal, the number 14 is called E in hexadecimal, and the number 15 is called F in hexadecimal.

Although IPv6 addresses can be expressed several ways, the first half (64 bits) of an IPv6 address identifies your network (the network ID); the first 48 bits are typically assigned by your ISP and identify your organization uniquely on the public Internet, and the following 16 bits can be used to identify unique subnets within your organization. The last 64 bits of an IPv6 address is used to uniquely identify a computer in your LAN (the host ID), and is often generated from the unique hardware address on each computer’s network interface.

Note  
The hardware address on a network interface is a 48-bit hexadecimal number called the Media Access Control (MAC) address that is unique for each network interface manufactured. Ethernet translates IPv4 and IPv6-addressed packets into MAC-addressed frames before sending it to the nearest host or router.

Although most operating systems today support IPv6, few networks and computers on the Internet have adopted it. In 2018, Google reported that less than 20 percent of all computers in any country have adopted IPv6. Most computers that have adopted IPv6 are small Internet-connected devices that are collectively referred to as the Internet of Things (IoT). Some example IoT devices include the NEST smart thermostat and the Google Home personal assistant; both of which run the Linux operating system. IoT devices often use an IPv6 address that can be accessed by an online app, and the IPv6 traffic they send is often encapsulated in IPv4 traffic using a protocol such as Teredo to allow it to work within IPv4-only networks.

This slow adoption of IPv6 is primarily the result of two technologies that allow IPv4 to address many more computers than was previously possible: proxy servers and Network Address Translation (NAT) routers.

Proxy servers and NAT routers are computers or hardware devices that have an IP address and access to a network such as the Internet. Other computers on the network can use a proxy server or NAT router to obtain network or Internet resources on their behalf. Moreover, there are three reserved ranges of IPv4 addresses that are not distributed to computers on the Internet and are intended only for use behind a proxy server or NAT router:

The entire 10.0.0.0 Class A network (10.0.0.0/8)

The 172.16 through 172.31 Class B networks (172.16–31.0.0/16)

The 192.168 Class C networks (192.168.0–255.0/24)

Thus, a computer behind a proxy server in Iceland and a computer behind a NAT router in Seattle could use the same IPv4 address—say, 10.0.5.4—without problems because each of these computers only requests Internet resources using its own proxy server or NAT router. A company may use a Cisco NAT router, for example, to allow other networks and computers in the company to gain access to the Internet. Similarly, a high-speed home Internet modem typically functions as a NAT router to allow multiple computers in your home to access the Internet.

Most computers in the world today obtain Internet access via a proxy server or NAT router. Because these computers share IPv4 addresses on a reserved network range, rather than using a unique IP address, the number of available IPv4 addresses has remained high and slowed the adoption of IPv6.


Configuring a Network Interface
Linux computers in a business environment typically connect to the company network via a wired or wireless network interface. At home, people typically connect to the Internet by means of a network interface, using technologies such as Fiber optic, cellular wireless, Wi-Fi, Digital Subscriber Line (DSL), and Broadband Cable Networks (BCNs).

Recall from Chapter 6 that network interface drivers are provided by modules that are inserted into the Linux kernel at boot time and given an alias that can be used to refer to the network interface afterwards. To view the driver modules and associated aliases for network interfaces in your system, you can use the lshw –class network command. On legacy Linux systems, such as Ubuntu 14, the first wired Ethernet network interface is called eth0, the second wired Ethernet network interface is called eth1, and so on. Similarly, the first wireless Ethernet network interface on a legacy Linux system is called wlan0, the second wireless Ethernet network interface in your system is called wlan1, and so on. Most modern Linux systems that use Systemd provide a more descriptive name for network interfaces that reflect the location of the hardware in the system. For example, enp0s3 refers to the wired Ethernet network interface on PCI bus 00 slot 03, and wlp8s0 refers to the wireless Ethernet network interface on PCI bus 08 slot 00.

Note  
The network interface aliases displayed by the lshw –class network command on a modern Linux system should match the PCI bus information associated with the network interface hardware displayed by the lspci command.

You can also use the ethtool command to display detailed information for network hardware. For example, ethtool -i enp0s3 will display driver information for the enp0s3 network interface.

After a driver module for a network interface has been loaded into the Linux kernel and given an alias, you can configure it to use IP. The ifconfig (interface configuration) command can be used to assign an IP configuration to a network interface as well as view the configuration of all network interfaces in the computer. To assign eth0 the IP address of 3.4.5.6 with a subnet mask of 255.0.0.0 and broadcast address of 3.255.255.255, you can use the following command at the command prompt:

 Enlarge Image
Alternatively, you can receive IP configuration from a Dynamic Host Configuration Protocol (DHCP) or Boot Protocol (BOOTP) server on the network. To obtain and configure IP information from a server on the network, you can use the dhclient command; for example, dhclient eth0 would attempt to obtain IP configuration from the network connected to eth0.

The process of obtaining an IP address for your network interface varies, depending on whether your computer is on an IPv4 or IPv6 network. If you attempt to obtain IPv4 configuration for your network interface from a DHCP or BOOTP server and no DHCP or BOOTP server exists on your network, your system will assign an IPv4 address of 169.254.x.x where .x.x is a randomly generated host ID. This automatic assignment feature is called Automatic Private IP Addressing (APIPA). If your network has IPv6-configured routers, an IPv6 address is automatically assigned to each network interface. This is because network interfaces use Internet Control Message Protocol version 6 (ICMPv6) router discovery messages to probe their network for IPv6 configuration information. Alternatively, you can obtain your IPv6 configuration from a DHCP server on the network. If there are no IPv6-configured routers or DHCP servers on your network from which you can obtain an IPv6 configuration for your network interface, your system will assign an IPv6 APIPA address that begins with FE80 and ends with the last half of your network interface’s MAC address.

Note  
A single network interface can have both an IPv4 and an IPv6 address. Each address can be used to access the Internet using the IPv4 and IPv6 protocols, respectively.

To view the configuration of all interfaces, you can use the ifconfig command without any arguments, as shown in the following output:

 Enlarge Image
The output of the ifconfig command shows that the eth0 network interface has an IPv4 address of 3.4.5.6 and an IPv6 address of fe80::280:c6ff:fef9:1b8c that you can tell was automatically configured by the system because the end of the IPv6 is identical to the last half of the MAC address (ether 00:80:C6:F9:1B:8C). It also shows receive (RX) and transmit (TX) statistics and the special loopback adapter (lo) with the IPv4 address 127.0.0.1 and IPv6 address ::1; these IP addresses represent the local computer and are required on all computers that use IP.

Note  
You can also use the –i option to the netstat command to show interface statistics.

Note  
The iwconfig command is similar to the ifconfig command, but only displays the configuration of wireless network interfaces in your system. You can also use the iwconfig command to set additional parameters for wireless network interfaces, such as frequency and channel.

If you restart the computer, the IP information configured for eth0 will be lost. To allow the system to activate and configure the IP information for an interface at each boot time, you can place entries in a configuration file that is read at boot time by your Linux distribution when activating the network. For Fedora 28 systems, you can add entries to the /etc/sysconfig/network-scripts/ifcfg-interface file, where interface is the name of the network interface. An example of the configuration file for enp0s3 is shown in the following output:

 Enlarge Image
The entries in the preceding output indicate that the IP configuration for the Ethernet adapter (enp0s3) will be activated at boot time (ONBOOT=yes) and IPv6 will be automatically configured using ICMPv6 if an IPv6-configured router is available (IPV6_AUTOCONF=yes). The network interface does not obtain information from a DHCP server (BOOTPROTO=none). IPv4 is instead configured using the IP address 3.4.5.6, an 8-bit subnet mask of 255.0.0.0 (PREFIX=8), and a default gateway of 3.0.0.254. To resolve Internet names, the first DNS server queried will be 8.8.8.8, followed by 8.8.4.4 if the first DNS server is unavailable (DNS1=8.8.8.8, DNS2=8.8.4.4). Also, you can change the BOOTPROTO=none line to BOOTPROTO=dhcp to obtain all IP configuration information from a DHCP server on the network.

After editing the /etc/sysconfig/network-scripts/ifcfg-eth0 file, you do not need to reboot your system to have the new IP configuration take effect. Instead, you can run the command ifdown eth0 to unconfigure the eth0 network interface, followed by ifup eth0 to configure the eth0 network interface using the settings in the /etc/sysconfig/network-scripts/ifcfg-eth0 file. Alternatively, you can use the ifconfig eth0 down and ifconfig eth0 up commands to deactivate and activate the eth0 network interface.

Note  
You can optionally use the /etc/sysconfig/network file in Fedora 28 to store network configuration settings that should be shared by all network interfaces on the system.

Ubuntu Linux systems don’t store network configuration within files under the /etc/sysconfig/network-scripts directory. On legacy systems, such as Ubuntu Server 14, all network interfaces are configured from information stored within the /etc/network/interfaces file. A sample /etc/network/interfaces file that has an IP address (3.4.5.6) manually assigned to eth0 is shown in the following output:

 Enlarge Image
To instead use DHCP configuration for eth0, you can change the line iface eth0 inet static to iface eth0 inet dhcp and remove the address, netmask, and gateway lines in the preceding example and then reload your configuration by using the ifdown eth0 command followed by the ifup eth0 command, as you would in Fedora Linux, to deactivate and activate the network interface.

On modern Ubuntu systems, such as Ubuntu Server 18, NetPlan is used to configure network interfaces; it allows for easy integration with the cloud-init system that Ubuntu uses to create new virtual machines within a cloud environment. NetPlan uses *.yaml files stored within the /etc/netplan directory to configure network interfaces. A sample /etc/netplan/50-cloud-init.yaml file that has an IP address (3.4.5.6) manually assigned to enp0s3 is shown in the following output:

 
To instead use DHCP configuration for enp0s3, you can change the line dhcp4: no to read dhcp4: yes or dhcp4: true and reload your configuration by using the netplan apply command.

Note  
YAML (YAML Ain’t Markup Language) is a file format that uses the attribute: value syntax defined by JSON (JavaScript Object Notation), but with additional support for comments. Due to its simplicity, YAML is increasingly being used to provide configuration information for modern Linux systems and cloud-related services.

To make the configuration of a network interface easier, you may access your configuration from within a GUI environment. Most Linux distributions contain a graphical network configuration tool that can be run if a GUI environment is installed. On Fedora 28, you can configure IP on your network interfaces using the Network utility. To do this, you can navigate to Activities, Show Applications, Settings, Network, and click the cog wheel icon next to your wired or wireless network interface shown in Figure 12-2.

Figure 12-2
The Network utility

A screenshot of the Network utility is displayed. Network connections can be configured in Linux using this utility. The utility indicates that a wired network connection is active. V P N is not set up and network proxy is off.
Portable computers with a GUI environment often have both a wired and wireless network interface, and connect to a wide variety of networks at different times. To simplify the switching and management of these networks, a daemon called NetworkManager is often used on Linux distributions that have a GUI environment, including the Fedora 28 Workstation distribution. If your system is running NetworkManager, you can additionally use the nmcli command to view or modify connection information. Without arguments, nmcli displays information about each network interface, and indicates the active connection as shown in the following output:

 Enlarge Image
In the previous output, the wireless Ethernet network interface wlp8s0 is connected to a wireless network called CLASSWIFI. On a Fedora system, NetworkManager will store the network configuration for this connection in a separate file called /etc/sysconfig/network-scripts/ifcfg-CLASSWIFI. It can then be reused if the system connects to the network in the future. This is especially useful if you manually configure IP addresses for a wireless network, such as a home or work network; each time you connect to the wireless network, the IP address information you configured previously will automatically be applied from the associated configuration file. To see a list of all wired and wireless networks that NetworkManager has connected to in the past, you can run the nmcli conn show command as shown in the following output:

 Enlarge Image
Systemd also contains a daemon called Systemd-networkd that provides the same functionality as NetworkManager. You can use the networkctl command to view and modify connection information for Systemd-network; without arguments, it displays information about each network interface, including the active connection. The following output indicates that enp0s3 is actively connected and managed by Systemd-networkd:

 Enlarge Image
Note  
Both NetworkManager and Systemd-networkd are optional network helper daemons, and only one can be active at any time.

After a network interface has been configured to use IP, you should test the configuration by using the ping (Packet Internet Groper) command. The ping command sends an ICMP packet to another IP address and awaits a response. By default, the ping command sends packets continuously every second until the Ctrl+c key combination is pressed; to send only five ping requests to the loopback interface, you can use the –c option to the ping command, as shown in the following example:

 
Note  
If the ping command fails to receive any responses from the loopback interface, there is a problem with IP itself.

Next, you need to test whether the Linux computer can ping other computers on the same network; the following command can be used to send five ping requests to the computer that has the IP address 3.0.0.2 configured:

 
Note  
If the ping command fails to receive any responses from other computers on the network, there is a problem with the network media.

You can use the ping6 command to send an ICMP6 message to an IPv6 address.

Configuring a PPP Interface
Instead of configuring IP to run on a network interface to gain network access, you can run IP over serial lines (such as telephone lines) using the PPP WAN protocol. Three common technologies use PPP to connect computers to the Internet or other networks:

Modems

ISDN

DSL

Modem (modulator-demodulator) devices use PPP to send IP information across normal telephone lines; they were the most common method for home users to gain Internet access in the 1990s. Modem connections are considered slow today compared to most other technologies; most modems can only transmit data at 56KB/s. Because modems transmit information on a serial port, the system typically makes a symbolic link called /dev/modem that points to the correct serial port device, such as /dev/ttyS0 for COM1.

Integrated Services Digital Network (ISDN) is a set of standards designed for transmitting voice, video, and data over normal copper telephone lines. It allows data to be transferred at 128KB/s. ISDN uses an ISDN modem device to connect to a different type of media than regular phone lines. Although ISDN is popular in Europe, it does not have a large presence in North America.

One of the most popular connection technologies in North America is DSL. DSL has many variants, such as Asynchronous DSL (ADSL), which is the most common DSL used in homes across North America, and High-bit-rate DSL (HDSL), which is common in business environments; for simplification, all variants of DSL are referred to as xDSL. You use an Ethernet network interface to connect to a DSL modem using IP and PPP; as a result, DSL connections are said to use PPP over Ethernet (PPPoE). The DSL modem then transmits information across normal telephone lines at speeds that can exceed 100MB/s.

Because modem, ISDN, and DSL connections require additional configuration information that is specific to the ISP, they are not normally configured during the Linux installation and must be configured manually.

Configuring a PPP connection requires support for PPP compiled into the kernel or available as a module, the PPP daemon (pppd), and a series of supporting utilities such as the chat program, which is used to communicate with a modem. PPP configuration in the past was tedious at best; you needed to create a chat script that contained the necessary information to establish a PPP connection (user name, password, and so on), a connection script that contained device parameters used by the PPP daemon, as well as use a program such as minicom to initiate network communication. Because the IP configuration is typically assigned by the ISP to which you connect, it rarely needs to be configured during the process.

Because modems and ISDN modems are relatively rare today, modern Linux distributions typically don’t ship with a graphical configuration tool by default. However, you can download and install the Modem Manager utility to configure modems and ISDN modems. On a Fedora 28 system, you can run the dnf install modem-manager-gui command as the root user to install this package, and then navigate to Activities, Show Applications, Modem Manager GUI to start the Modem Manager shown in Figure 12-3. The Modem Manager automatically detects any modem or ISDN hardware in your computer and allows you to configure the associated ISP configuration, including the ISP account username and password.

Figure 12-3
The Modem Manager utility

A screenshot of the Modem Manager utility is displayed. This G U I utility can be used to configure a modem for use with Linux.
DSL connections are very common today. As a result, nearly all modem Linux distributions contain a utility that can configure your network interface to work with a DSL modem. On Fedora 28, you can either execute the pppoe-setup command at a command-line terminal or the nm-connection-editor command within a terminal in a GUI environment. The nm-connection-editor command starts the graphical Network Connections tool shown in Figure 12-4, which is a component of NetworkManager. If you click the + button shown in Figure 12-4 and choose your device type (DSL/PPPoE), the Network Connections tool will attempt to detect your DSL modem and prompt you to supply the ISP account username and password.

Figure 12-4
The Network Connections utility

A screenshot of the Network Connections utility is displayed. This utility can be used to configure network connections for physical network devices in Linux.
Note  
In addition to DSL, the Network Connections tool can also be used to configure many other network technologies, including:

Overlay networks, including Virtual Private Network (VPN) connections that provide an encrypted virtual IP network on top of the existing unencrypted IP network.

Specialized technologies, such as Infiniband, that allow network interfaces to communicate directly to each other using Remote Direct Memory Access (RDMA).

Bonding (also called aggregation), in which two separate network interfaces connected to the same network can be combined to provide fault tolerance (in an active/passive configuration) or load balancing of network traffic to achieve higher transmission rates.

Bridging, in which two network interfaces connected to separate networks provide for seamless connectivity between the networks.

To configure a DSL connection on a system without a GUI environment that contains NetworkManager, you can execute the nmtui command to start a text version of the Network Connections tool, or run the nmcli command with the appropriate options. If NetworkManager is not installed (the default on Ubuntu Server), you can instead execute the pppoeconf command, which will open a basic graphical screen within your terminal and scan for DSL devices, as shown in Figure 12-5. If a DSL device is found, you will be prompted to supply the ISP account username and password to complete the configuration.

Figure 12-5
The pppoeconf utility

A screenshot shows the p p p o e conf utility which can be used to configure user account information for a D S L connection in Linux.
After a PPP modem, ISDN, or DSL connection has been configured, it will normally be activated automatically at boot time like other network interfaces on the system. On a Fedora 28 system, you will notice a new file for the connection within the /etc/sysconfig/network-scripts directory. On an Ubuntu Server 14 system, there will an additional paragraph added to the /etc/network/interfaces file, and on an Ubuntu Server 18 system, there will be an additional YAML configuration file within the /etc/netplan directory. Other configuration used by the PPP daemon is stored within the /etc/ppp and /etc/isdn directories. It is good form to double-check the passwords used to connect to the ISP because incorrect passwords represent the most common problem with PPP connections. These passwords are stored in two files: /etc/ppp/pap-secrets (Password Authentication Protocol secrets) and /etc/ppp/chap-secrets (Challenge Handshake Authentication Protocol secrets). If the ISP accepts passwords sent across the network in text form, the /etc/ppp/pap-secrets file is consulted for the correct password; however, if the ISP requires a more secure method for validating the identity of a user, the passwords in the /etc/ppp/chap-secrets file are used. When you configure a PPP connection, this information is automatically added to both files, as shown in the following output:

 Enlarge Image
After a PPP device has been configured, the ifconfig command indicates the PPP interface using the appropriate name; ppp0 is typically used for the first modem or xDSL device, and ippp0 is typically used for the first ISDN device. The following output shows the IP configuration (obtained from the ISP) when the first xDSL interface (ppp0) is activated:

 Name Resolution
Computers that communicate on an IP network identify themselves using unique IP addresses; however, this identification scheme is impractical for human use because it is difficult to remember IP addresses. As a result, every computer on a network is identified by a name that makes sense to humans, such as “Accounting1” or “ReceptionKiosk.” Because each computer on a network is called a host, the name assigned to an individual computer is its host name.

For computers that require a presence on the Internet, simple host names are rarely used. Instead, they are given a host name called a fully qualified domain name (FQDN) according to a hierarchical naming scheme called Domain Name Space (DNS). At the top of the Domain Name Space is the root domain, which is really just a theoretical starting point for the branching, tree-like structure. Below the root domain are the top-level domain names, which identify the type of organization in which a network is located. For example, the com domain is primarily used for business, or commercial, networks. Several second-level domains exist under each top-level domain name to identify the name of the organization, and simple host names are listed under the second-level domains. Figure 12-6 shows a portion of the Domain Name Space.

Figure 12-6
The domain name space

Illustration shows a hierarchy that represents a portion of domain name space. The hierarchy is in an inverted tree structure with 4 levels. At Level 1 is the root or unamed root. At Level 2 are the top level domains branching from the root at Level 1. The top level domains include o r g, net, com, c a, u s, and u k. At Level 3, branching from the top level domain o r g we have the second level domain. The second level domains included linux, k d e, g n u, red hat, and suse. At Level 4, branching from Linux in the second level domain, we have the host computer w w w.
Note  
For simplicity, FQDNs are often referred to as host names.

Thus, the host computer shown in Figure 12-6 has an FQDN of www.linux.org.

Note  
The host name www (World Wide Web) is often used by servers that host Web pages.

Second-level domains must be purchased and registered with an ISP in order to be recognized by other computers on the Internet. You can use the whois command to obtain registration information about any domain within the Domain Name Space. For example, to obtain information about the organization responsible for maintaining the linux.org domain, you can use the following command:

 Enlarge Image
You can view or set the host name for a Linux computer using the hostname command, as shown in the following output:

 
To configure the host name shown in the preceding output at every boot time, modify the HOSTNAME line in the /etc/hostname file, or run the hostnamectl command. For example, the commands in the following output set the host name to computer1.sampledomain.com and verify the configuration within /etc/hostname:

 Enlarge Image
Note  
Many network server daemons record the system host name in their configuration files during installation. As a result, some Linux server distributions prompt you for the host name during the Linux installation to ensure that you don’t need to modify network server daemon configuration files afterwards.

Although host names are easier to use when specifying computers on the network, IP cannot use them to identify computers. Thus, you must map host names to their associated IP addresses so that applications that contact other computers across the network can find the appropriate IP address for a host name.

The simplest method for mapping host names to IP addresses is by placing entries into the /etc/hosts file, as shown in the following example:

 Enlarge Image
Note  
You can also use the getent hosts command to view the contents of the /etc/hosts file.

The entries in the preceding output identify the local computer, 127.0.0.1, by the host names server1, server1.class.com, localhost, and localhost.localdomain. Similarly, you can use the host name ftp.sampledomain.com or fileserver to refer to the computer with the IP address of 3.0.0.2. Also, the computer with the IP address of 10.3.0.1 can be referred to using the name alpha.

Because it would be cumbersome to list names for all hosts on the Internet in the /etc/hosts file, ISPs can list FQDNs in DNS servers on the Internet. Applications can then ask DNS servers for the IP address associated with a certain FQDN. To configure your system to resolve names to IP addresses by contacting a DNS server, you can specify the IP address of the DNS server in the /etc/resolv.conf file. This file can contain up to three DNS servers; if the first DNS server is unavailable, the system attempts to contact the second DNS server, followed by the third DNS server listed in the file. An example /etc/resolv.conf file is shown in the following output:

 
Each network interface can have a different set of DNS servers that it uses for name resolution. To do this, you can specify the DNS servers within the IP configuration file for the network interface as shown earlier in this chapter. In this case, the DNS servers listed within the IP configuration file for the network interface are written to the /etc/resolv.conf file when the interface is activated.

Note  
On Ubuntu Server 14.04, the /etc/resolv.conf file is built dynamically at each boot. To manually add a DNS server, add the appropriate lines to /etc/resolvconf/resolv.conf.d/base and they will be incorporated into /etc/resolv.conf on each subsequent boot.

On Linux distributions that use Systemd, the Systemd-resolved service handles name resolution requests. This service queries the DNS servers listed in the /etc/resolv.conf file on most Linux distributions, but may instead be configured to use the /run/systemd/resolve/stub-resolv.conf file; in this case /etc/resolv.conf is merely a symlink to /run/systemd/resolve/stub-resolv.conf.

To test the DNS configuration by resolving a host name or FQDN to an IP address, you can supply the host name or FQDN as an argument to the nslookup, dig, or host command at a command prompt.

When you specify a host name while using a certain application, that application must then resolve that host name to the appropriate IP address by searching either the local /etc/hosts file or a DNS server. The method that applications use to resolve host names is determined by the “hosts:” line in the /etc/nsswitch.conf file; an example of this file is shown in the following output:

 
The preceding output indicates that applications first try to resolve host names using the /etc/hosts file (files). If unsuccessful, applications contact the DNS servers listed in the /etc/resolv.conf file (dns).

On older Linux computers, the /etc/host.conf file was used instead of /etc/nsswitch.conf; the /etc/host.conf file still exists today to support older programs and should contain the same name resolution order as /etc/nsswitch.conf if older programs are installed. An example /etc/host.conf file that tells applications to search the /etc/hosts file (hosts) followed by DNS servers (bind) is shown in the following output:

 Routing
Every computer on a network maintains a list of IP networks so that packets are sent to the appropriate location; this list is called a route table and is stored in system memory. To see the route table, you can use the route command, as shown in the following output:

 Enlarge Image
Note  
The netstat –r command is equivalent to the route command.

The /etc/networks file contains aliases for IP networks. The “default” line in the previous output refers to the 0.0.0.0 network because the /etc/networks file provides an alias for the 0.0.0.0 network called “default.” To view the route table without aliases, supply the –n option to the route or netstat -r command.

The route table shown in the preceding output indicates that all packets destined for the 10.0.0.0 network will be sent to the device wlan0. Similarly, all packets destined for the 192.168.0.0 network will be sent to the device eth0 and packets destined for the 127.0.0.0 network will be sent to the loopback adapter (lo). Packets that must be sent to any other network will be sent to the default gateway; the final line in the preceding output indicates that the default gateway is a computer with the IP address 192.168.0.1, via the eth0 device.

Note  
The default gateway is normally specified within the IP configuration file for the network interface as shown earlier in this chapter, and loaded when the network interface is activated.

If your computer has more than one network interface configured, the route table will have more entries that define the available IP networks; computers that have more than one network interface are called multihomed hosts. Multihomed hosts can be configured to forward packets from one interface to another to aid a packet in reaching its destination; this process is commonly called routing or IP forwarding. To enable routing on your Linux computer, place the number 1 in the file /proc/sys/net/ipv4/ip_forward for IPv4 or /proc/sys/net/ipv6/conf/all/forwarding for IPv6, as shown in the following output:

 Enlarge Image
Note  
The sysctl command can also be used to modify the contents of files under the /proc/sys directory. For example, sysctl net.ipv4.ip_forward = 1 would be equivalent to the echo 1 > /proc/sys/net/ipv4/ip_forward command.

To enable IPv4 routing at every boot, ensure that the line net.ipv4.ip_forward = 1 exists in the /etc/sysctl.conf file. To enable IPv6 routing at every boot, ensure that the line net.ipv6.conf.default.forwarding = 1 exists in the /etc/sysctl.conf file.

If your computer has more than one network interface and routing is enabled, your computer will route packets only to networks for which it has a network interface. On larger networks, however, you might have several routers, in which case packets might have to travel through several routers to reach their destination. Because routers only know the networks to which they are directly connected, you might need to add entries to the route table on a router so that it knows where to send packets that are destined for a remote network. Suppose, for example, your organization has three IP networks (1.0.0.0/8, 2.0.0.0/8, and 3.0.0.0/8) divided by two routers, as shown in Figure 12-7.

Figure 12-7
A sample routed network

Illustration shows a sample routed network. There are three t c p I p networks in this sample inter connected by two routers. The first network is the 1 dot 0 dot 0 dot 0 bar 8 network. This network is conneted to Router A with the i p address 1 dot 0 dot o dot 1. Router A is connected to the 2 dot 0 dot 0 dot 0 bar 8 network with the i p address 2 dot 0 dot 0 dot 1. The 2 dot 0 dot 0 dot 0 bar 8 network is connected to Router B with the i p address 2 dot 0 dot 0 dot 2. Router B is connected to the 3 dot 0 dot 0 dot 0 bar 8 network using the i p address 3 dot 0 dot 0 dot 1.Enlarge Image
RouterA has an entry in its route table that says it is connected to: 1) the 1.0.0.0/8 network via the network interface that has the IP address 1.0.0.1; and 2) the 2.0.0.0/8 network via the network interface that has the IP address 2.0.0.1. These two routes are automatically established when IP is configured. If RouterA receives a packet that is destined for the 3.0.0.0/8 network, it does not know where to forward it because it does not have a route for the 3.0.0.0/8 network in its routing table. To add the appropriate route to the 3.0.0.0/8 network on RouterA, you can run the following command on RouterA:

 Enlarge Image
Now, RouterA sends any packets destined for the 3.0.0.0/8 network to the computer 2.0.0.2 (RouterB). RouterB then forwards the packets to the 3.0.0.0/8 network because it has a route in its route table that says it is connected to the 3.0.0.0/8 network via the network interface that has the IP address 3.0.0.1.

Similarly, for RouterB to forward packets it receives destined for the 1.0.0.0/8 network, it must have a route that sends those packets to RouterA via the interface 2.0.0.1:

 Enlarge Image
Note  
You can use the route del <route> command to remove entries from the route table.

The ip command can also be used to view and manipulate the route table. For example, the command ip route add 1.0.0.0/8 via 2.0.0.1 can be used to add the route shown in the previous output to the route table, and the command ip route show can display the route table.

The contents of the route table are lost when the computer is powered off; to load the additional route shown in the previous output to the route table at every boot time, add the line 1.0.0.0/8 via 2.0.0.1 dev interface to the /etc/sysconfig/network-scripts/route-interface file on a Fedora 28 system, or add the line up route add -net 1.0.0.0 netmask 255.0.0.0 gw 2.0.0.1 to the network interface section of the /etc/network/interfaces file on an Ubuntu Server 14 system. For Ubuntu 18, you can add the following lines within the network interface section of the YAML file under the /etc/netplan directory:

 
You can also use a routing protocol on routers within your network to automate the addition of routes to the routing table. Two common routing protocols are Routing Information Protocol (RIP) and Open Shortest Path First (OSPF). If you install the quagga package, you can configure the RIP and OSPF routing protocols using the zebra command.

Because the list of all routes on large networks such as the Internet is too large to be stored in a route table on a router, most routers are configured with a default gateway. Any packets that are addressed to a destination that is not listed in the route table are sent to the default gateway, which is a router that can forward the packet to the appropriate network or to the router’s own default gateway and so on until the packets reach their destination.

If computers on your network are unable to connect to other computers on a remote network, the problem is likely routing-related. A common utility used to troubleshoot routing is the traceroute command; it displays all routers between the current computer and a remote computer. To trace the path from the local computer to the computer with the IP address 3.4.5.6, you can use the following command:

 
Note  
Two common alternatives to the traceroute command include the tracepath command and the mtr command.

To trace an IPv6 route, you can use the mtr, traceroute6, or tracepath6 command.

Network Services
Recall from Chapter 1 that Linux provides a wide variety of services that are available to users across a network. Before you are able to configure the appropriate network services to meet your organization’s needs, you must first identify the types and features of network services.

Network services are processes that run on your computer and provide some type of valuable service for client computers on the network. They are often represented by a series of daemon processes that listen for certain requests on the network. Daemons identify the packets to which they should respond using a port number that uniquely identifies each network service. Different daemons listen for different port numbers. A port number is like an apartment number for the delivery of mail. The network ID of the IP address ensures that the packet is delivered to the correct street (network); the host ID ensures that the packet is delivered to the correct building (host); and the transport layer protocol and port number ensure that the packet is delivered to the proper apartment in the building (service).

Ports and their associated protocols are defined in the /etc/services file. To see to which port the telnet daemon listens, you can use the following command:

 Enlarge Image
The preceding output indicates that the telnet daemon listens on port 23 using both TCP/IP and UDP/IP.

Ports range in number from 0 to 65534. The ports 0–1023 are called well-known ports because they represent commonly used services. Table 12-2 provides a list of common well-known ports.

Table 12-2
Common Well-Known Ports
Service	Port
FTP	TCP 20, 21
Secure Shell (SSH)	TCP 22
Telnet	TCP 23
SMTP	TCP 25
HTTP / HTTPS	TCP 80 / TCP 443
rlogin	TCP 513
DNS	TCP 53, UDP 53
Trivial FTP (TFTP)	UDP 69
NNTP / NNTPS	TCP 119 / TCP 563
POP3 / POP3S	TCP 110 / TCP 995
IMAP4 / IMAP4S	TCP 143 / TCP 993
NTP	TCP 123, UDP 123
SNMP	TCP 161, TCP 162, UDP 161, UDP 162
NetBIOS	TCP 139, UDP 139
SMB/CIFS	TCP 445, UDP 445
Syslog	TCP 514, UDP 514
LDAP / LDAPS	TCP 389, UDP 389 / TCP 636, UDP 636
Enlarge Table
Note  
Many protocols have a secure version that uses encrypted communication. For example, secure HTTP is called HTTPS and uses a different port number as a result.

You can use the netstat or ss (socket statistics) command to display active TCP/IP and UDP/IP connections on your system; the netstat -t or ss -t command will display active TCP/IP connections, whereas the netstat -u or ss -u command will display active UDP/IP connections.

Network utilities can connect to daemons that provide network services directly; these daemons are called stand-alone daemons. Alternatively, network utilities can connect to network services via the Extended Internet Super Daemon (xinetd), which starts the appropriate daemon to provide the network service as needed. This structure is shown in Figure 12-8.

Figure 12-8
Interacting with network services

Illustration shows how interaction occurs with network services. The client computer interacts with the server computer using network media. The server computer has several network service daemons running. One of these could be the xinted daemon. The network utility on the client computer utilizes the network media to interact with the daemons on the server.
Note  
The xinetd daemon is an extended version of the original Internet Super Daemon (inetd) that was available on legacy UNIX and Linux systems.

Because xinetd only starts daemons on demand to conserve memory and other system resources, it is often used on smaller Linux systems, such as IoT devices. On larger Linux systems, xinetd is optionally used to start and manage connections for smaller network daemons such as telnet and rlogin. Ubuntu Server 14, Ubuntu Server 18, and Fedora 28 systems do not contain the xinetd daemon by default, but you can install it from a software repository.

The xinetd daemon is configured via entries within the /etc/xinetd.conf file. Normally, this file incorporates all of the files in the /etc/xinetd.d directory as well. Most daemons that are managed by xinetd are configured by files in the /etc/xinetd.d directory named after the daemons. For example, if you install the telnet daemon, you can configure it to be started by xinetd via the /etc/xinetd.d/telnet file, as shown in the following output:

 
Large network daemons are rarely started by xinetd. Instead, they are stand-alone daemons that are started at boot time from rc scripts as part of the UNIX SysV or Systemd system initialization process.

Many stand-alone and xinetd-managed daemons also have one or more configuration files that control how they operate. Most of these configuration files contain many commented lines that indicate the usage of certain configuration parameters. As a result, these configuration files can be very large; the main configuration file used by the Apache Web Server is often several hundred lines long. In addition to this, most stand-alone network daemons do not use the system log daemon (rsyslogd) or Systemd journal daemon (journald) to log information related to their operation. Instead, they log this information themselves to subdirectories of the same name under the /var/log directory. For example, log files for the Samba daemon are located in the /var/log/samba directory.

Table 12-3 lists the names and features of network services that are commonly found on Linux computers that participate in a network environment. You’ll learn how to configure many of these network services in this chapter as well as within Chapter 13.

Table 12-3
Common Network Services
Network service	Type	Port	Description
Apache Web Server (httpd)	Stand-alone	TCP 80 TCP 443	Serves Web pages using HTTP/HTTPS to other computers on the network that have a Web browser Configuration file: /etc/httpd/conf/httpd.conf or /etc/apache2/apache2.conf
BIND / DNS Server (named)	Stand-alone	TCP 53 UDP 53	Resolves fully qualified domain names to IP addresses for a certain namespace on the Internet Configuration file: /etc/named.conf
DHCP Server (dhcpd)	Stand-alone	UDP 67 UDP 68	Provides IP configuration for computers on a network Configuration file: /etc/dhcp/dhcpd.conf
Washington University FTP Server (in.ftpd)	xinetd	TCP 20 TCP 21 UDP 69	Transfers files to and accepts files from other computers on the network with an FTP utility Configuration file: /etc/ftpaccess Hosts denied FTP access: /etc/ftphosts Users denied FTP access: /etc/ftpusers FTP data compression: /etc/ftpconversions
Very Secure FTP Server (vsftpd)	Stand-alone	TCP 20 TCP 21 UDP 69	Transfers files to and accepts files from other computers on the network with an FTP utility Configuration file: /etc/vsftpd/vsftpd.conf or /etc/vsftpd.conf Users denied FTP access: /etc/vsftpd/ftpusers or /etc/ftpusers
Internetwork News Server (innd)	Stand-alone	TCP 119 TCP 563	Accepts and manages newsgroup postings and transfers them to other news servers Configuration file: /etc/news/inn.conf
NFS Server (rpc.nfsd)	Stand-alone	TCP 2049	Shares files to other computers on the network that have an NFS client utility Configuration file: /etc/exports
POP3 Server (ipop3d)	Stand-alone	TCP 110 TCP 995	Allows users with an email reader to obtain email from the server using the Post Office Protocol version 3
IMAP4 Server (imapd)	Stand-alone	TCP 143 TCP 993	Allows users with an email reader to obtain email from the server using the Intenet Message Access Protocol
Sendmail Email Server (sendmail)	Stand-alone	TCP 25	Accepts and sends email to users or other email servers on the Internet using the Simple Mail Transfer Protocol (SMTP) Configuration file: /etc/sendmail.cf
Postfix Email Server (postfix)	Stand-alone	TCP 25	Accepts and sends email to users or other email servers on the Internet using the Simple Mail Transfer Protocol (SMTP) Configuration file: /etc/postfix/main.cf
rlogin Daemon (in.rlogind)	xinetd	TCP 513	Allows users who use the rlogin and rcp utilities the ability to copy files and obtain shells on other computers on the network
rsh Daemon (in.rshd)	xinetd	TCP 514	Allows users who use the rsh utility the ability to run commands on other computers on the network
Samba Server (smbd & nmbd)	Stand-alone	TCP 137 TCP 138 TCP 139 TCP 445	Allows Windows users to view shared files and printers on a Linux server Configuration file: /etc/samba/smb.conf
Secure Shell Daemon (sshd)	Stand-alone	TCP 22	Provides a secure alternative to the telnet, rlogin, and rsh utilities by using encrypted communication Configuration file: /etc/ssh/sshd_config
Squid Proxy Server (squid)	Stand-alone	TCP 3128	Allows computers on a network to share one connection to the Internet. It is also known as a proxy server Configuration file: /etc/squid/squid.conf
telnet Daemon (in.telnetd)	xinetd	TCP 23	Allows users who have a telnet utility the ability to log in to the system from across the network and obtain a shell
X.org (X11)	stand-alone	TCP 6000	Allows users who use X.org to obtain graphics from another X.org computer across the network using the XDMCP protocol. You can specify the hosts that can connect to X.org usingthe xhost command. Alternatively, you can use the xauth command to generate credentials (stored in ~/.Xauthority) that are used to connect to a remote X.org server.
Enlarge Table
To ensure that a service is responding to client requests, you can use the ncat (net cat) command to interact with it. For example, to interact with the sshd service running on port 22 on the local computer (127.0.0.1), you could run the ncat 127.0.0.1 22 command and view the output. If the service doesn’t display any output, you can often restart the daemon (sshd) to fix the problem.

Note  
Many Linux distributions create a symlink to the ncat command called nc.

Remote Administration
As we discussed in Chapter 6, Linux servers are typically installed on a rackmount server system that is located on a rack in a server room and administered remotely. There are several ways to perform command-line and graphical administration of remote Linux servers, including telnet, remote commands, Secure Shell (SSH), and Virtual Network Computing (VNC).


Telnet
The easiest way to perform administration on a remote Linux computer is via a command-line interface. The telnet command has traditionally been used to obtain a command-line shell on remote UNIX and Linux servers across the network that run a telnet server daemon. Nearly all Macintosh, Linux, and UNIX systems come with a telnet command. For Windows systems, you can download the free Putty program at www.chiark.greenend.org.uk/~sgtatham/putty/ to start a telnet session to another computer.

The telnet server daemon is not installed by default on most modern Linux distributions, but it can easily be installed from a software repository. On systems that use the Systemd system initialization process, such as Fedora 28, the telnet server daemon is managed directly by Systemd as a socket unit (telnet.socket). On systems that use the Sys Vinit system initialization process, such as Ubuntu Server 14, the telnet server daemon is managed by xinetd.

Note  
By default, you are prevented from logging in and obtaining a shell as the root user to certain network services such as telnet due to entries in the /etc/securetty file. Removing or renaming this file allows the root user to log in and receive a shell across the network using the telnet command.

After the telnet daemon has been configured, you can connect to it from a remote computer. To do this, specify the host name or IP address of the target computer to the telnet command and log in with the appropriate user name and password. A shell obtained during a telnet session runs on a pseudo terminal (a terminal that does not obtain input directly from the computer keyboard) rather than a local terminal, and it works much the same way a normal shell does; you can execute commands and use the exit command to kill the BASH shell and end the session. A sample telnet session is shown in the following output using a computer with a host name of server1:

 Secure Shell (SSH)
Although the telnet command can be quickly used to perform remote administration, it doesn’t encrypt the information that passes between computers. Secure Shell (SSH) was designed as a secure replacement for telnet (and other legacy commands, such as rsh, rlogin, and rcp) that encrypts information that passes across the network. As a result, the SSH daemon (sshd) is installed by default on most Linux distributions.

Note  
On Fedora 28, sshd is installed by default but not set to start automatically at boot time.

To connect to a remote Linux computer running sshd, you can use the ssh command followed by the host name or IP address of the target computer. For example, you can connect to a computer with the host name of appserver using the ssh appserver command. Your local user name will be passed to the server automatically during the SSH request, and you will be prompted to supply the password for the same user on the target computer. If you need to log in using a different user name on the remote appserver computer, you can instead use the ssh –l username appserver command or the ssh username@appserver command. A sample ssh session is shown in the following output:

 
Note  
You can also use the Putty program on a Windows computer to connect to sshd running on a Linux, UNIX, or macOS computer.

SSH can also be used to transfer files between computers. For example, to transfer the /root/sample file on a remote computer called appserver to the /var directory on the local computer, you could run the following command:

 Enlarge Image
Similarly, to transfer the /root/sample file on the local computer to the /var directory on a remote computer called appserver, you could run the following command:

 Enlarge Image
Alternatively, you can use the scp command to copy files using SSH. For example, to transfer the /root/sample file on a remote computer called appserver to the /var directory on the local computer, you could run the following scp command:

 
Similarly, to copy the /root/sample file to the /var directory on appserver, you could use the following scp command:

 
Many Linux utilities have built-in support for SSH, including rsync, which can also be used to copy files to other computers. For example, to copy the /root/sample file using rsync with SSH encryption to the /var directory on appserver, you could run the following rsync command:

 Enlarge Image
Although SSH is used to perform command-line administration of remote systems, the –X option to the ssh command can be used to tunnel X Windows information through the SSH connection if you are using the ssh command within a GUI environment. For example, if you start a command-line terminal within a GNOME desktop and run the command ssh –X root@appserver, you will receive a command prompt where you can type commands as you would in any command-line SSH session. However, if you type a graphical command (e.g., gnome-control-center), you will execute the graphical utility on appserver across the SSH tunnel. All graphics, keystrokes, and mouse movement will be passed between X Windows on appserver and X Windows on the local system.

Understanding SSH Host and User Keys
SSH uses symmetric encryption to encrypt the data that is sent over the network, but requires asymmetric encryption to securely communicate the symmetric encryption key between the two computers at the beginning of the SSH connection. This asymmetric encryption requires that each computer running sshd have a public key and private key for the various asymmetric encryption algorithms supported by SSH (DSA, RSA, ECDSA, and ED25519); these keys are called the SSH host keys and are stored in the /etc/ssh directory as shown in the following output:

 
In the previous output, the ssh_host_ecdsa_key file contains the ECDSA private key for the host, whereas the ssh_host_ecdsa_key.pub file contains the ECDSA public key for the host. At the beginning of the SSH connection, the two computers negotiate the strongest asymmetric encryption algorithm that is supported by both computers.

When you connect to a new computer for the first time using SSH, you will be prompted to accept the encryption fingerprint for the target computer, which is stored in ~/.ssh/known_hosts for subsequent connections. If the target computer’s encryption keys are regenerated, you will need to remove the old key from the ~/.ssh/known_hosts file before you connect again.

Note  
You can regenerate the host keys used by sshd using the ssh-keygen command.

To make authenticating to remote SSH hosts easier, you can generate a public key and private key for your user account for use with SSH using the ssh-keygen command; these keys are called SSH user keys and are stored in the ~/.ssh directory. For example, the ~/.ssh/id_rsa file contains the RSA private key for your user, and the ~/.ssh/id_rsa.pub file contains the RSA public key for your user. SSH user keys can be used in place of a password when connecting to trusted computers. To do this, you use the ssh-copy-id command to copy your public key to the user account on each computer that you would like to connect to without supplying a password. The target computer will store your public key in the ~/.ssh/authorized_keys file for each user account you specified with the ssh-copy-id command. The following example generates SSH user keys for the root user on server1, copies them to the root user account on server2, and then connects to server2 as the root user without specifying a password:

 Enlarge Image
Note from the ssh-keygen command in the previous output that no passphrase was supplied to protect the private key. If you supply a passphrase, then you will need to supply that passphrase each time you use the private key, including each time you connect to another computer using your SSH user keys. To prevent this, you can use the ssh-agent command to start the SSH agent process on your computer and run the ssh-add command to add your private key to this process (supplying your passphrase when prompted). The SSH agent process will then automatically supply your passphrase when your SSH user keys are used to connect to other computers.

Configuring SSH
You can configure the functionality of sshd by editing the /etc/ssh/sshd_config file. Most of this file is commented and should only be edited to change the default settings that sshd uses when servicing SSH clients. The most commonly changed options in this file are those that deal with authentication and encryption. For example, to allow the root user to log into SSH, you can modify the value of the PermitRootLogin line to yes within /etc/ssh/sshd_config and restart sshd.

Note  
On Ubuntu Server, root access via SSH is denied by default. This is considered good security practice because malicious software on the Internet often attempts to log in as the root user. In this case, you can access SSH as a regular user and then use the su command to switch to the root user to perform administrative tasks.

Recall that the data communicated across the network with SSH is encrypted using a symmetric encryption algorithm that is negotiated at the beginning of the SSH connection, after the SSH host keys have been exchanged. Each symmetric encryption algorithm differs in its method of encryption and the cryptography key lengths used to encrypt data; the longer the key length, the more difficult it is for malicious users to decode the data. The main types of symmetric encryption supported by sshd are as follows:

Triple Data Encryption Standard (3DES), which encrypts blocks of data in three stages using a 168-bit key length

Advanced Encryption Standard (AES), an improvement on 3DES encryption and is available in 128-, 192-, and 256-bit key lengths

Blowfish, an encryption algorithm that is much faster than 3DES and can use keys up to 448 bits in length

Carlisle Adams Stafford Tavares (CAST), a general-purpose encryption similar to 3DES that is commonly available using a 128-bit key length

ARCfour, a fast encryption algorithm that operates on streams of data instead of blocks of data and uses variable-length keys up to 2048 bits in length

In addition, all of the aforementioned types of encryption except ARCfour typically use Cipher Block Chaining (CBC), which can be used to encrypt larger amounts of data.

Client computers can use a /etc/ssh/ssh_config or ~/ssh/ssh_config file to set SSH options for use with the ssh command, including the symmetric encryption types that can be used, because the SSH client is responsible for randomly generating the symmetric encryption key at the beginning of the SSH connection. However, the encryption types on the client computer must match those supported by the SSH server for a connection to be successful.
Virtual Network Computing (VNC)
Like the –X option of ssh, Virtual Network Computing (VNC) is another graphical option for administrating a Linux system remotely. After installing a VNC server daemon on a computer, other computers that run a VNC client can connect to the VNC server daemon across a network to obtain a full desktop environment. VNC uses a special platform-independent protocol called Remote FrameBuffer (RFB) to transfer graphics, mouse movements, and keystrokes across the network.

Note  
VNC server software and client software exist for Linux, UNIX, Mac, and Windows systems. This allows you to use a single technology to obtain the desktop of all of the Linux, UNIX, macOS, and Windows systems on your network.

Because X Windows is not installed on Ubuntu Server by default, we’ll focus on the configuration of VNC on Fedora 28 in this section. However, the steps are similar on other Linux distributions.

On Fedora 28, you can install a VNC server by running the dnf install tigervnc-server command. Next, you can set a VNC connection password for a user using the vncpasswd command. The VNC password is stored in the ~/.vnc/passwd file; for user1, the VNC password will be stored in the /home/user1/.vnc/passwd file. Finally, you can run the vncserver command as the user you specified the VNC password for; this will start a VNC server process with the next available display number, starting from 1.

Note  
By default, the VNC server process listens on port 5900 + display number. For display number 1, the VNC server will listen on port 5901, for display number 2, the VNC server will listen on port 5902, and so on.

Other computers can then connect to the VNC server using a VNC viewer program, such as RealVNC. When using a VNC viewer program to connect to a remote VNC server, you can specify the server name or IP address, port, and display number using the syntax server:port:display. For example, to connect to the VNC server that uses display number 1 on the computer server1.class.com, you could use the syntax server1.class.com:5901:1. You will then obtain a desktop session on the remote computer, as shown in Figure 12-9.

Figure 12-9
A remote VNC session

Illustration shows a remote Linux session that is occuring through the utility V N C viewer. V N C viewer is a remote desktop tool for the Linux operating system.Enlarge Image
Note  
Many VNC viewers allow you to specify the server name or IP address and either the port or display number. For example, you could specify server1.class.com:5901 or server1.class.com:1 within a VNC viewer to connect to the server shown in Figure 12-9.

You can use the remote or local port forwarding feature of SSH to encrypt the traffic sent by other services, such as VNC. For example, if you run the command ssh -R 5901:server1.class.com:5901 bob@client1.class.com, bob on the client1.class.com computer can start a VNC viewer and connect to client1.class.com:5901 in order to access the VNC server running on port 5901 on server1.class.com via SSH. Alternatively, you can run the ssh -L 5901:localhost:5901 bob@server1.class.com command on your computer to start an SSH session that forwards any local traffic on port 5901 to server1.class.com on port 5901 as bob. Following this, you can start a VNC viewer and connect to localhost:5901 in order to access the VNC server running on port 5901 on server1.class.com via SSH.

To configure a VNC server process to run persistently, you must create a VNC Systemd service unit that specifies the user and display number. To create a VNC Systemd service unit that listens on the third X Windows display, you can use the following command to copy it from the /lib/systemd/system/vncserver@.service template:

 Enlarge Image
Note  
On systems that do not use the Systemd system initialization system, the /etc/sysconfig/vncservers configuration file is used in place of /lib/systemd/system/vncserver@:3.service.

Next, you must edit the VNC server configuration file and, at minimum, modify the line(s) that indicate(s) the user that can connect (replace <user> with a valid username).

Following this, you must ensure that the user specified within the VNC Systemd service unit file has a VNC connection password set. Finally, you can start the VNC service unit for the display number, and configure it to start automatically at system initialization. For display number 3, you can use the following commands:

 
Note  
There are many alternatives to VNC that provide for remote access to Linux desktops, including Xrdp, which uses the Microsoft Remote Desktop Protocol, and NoMachine, which uses the proprietary NX protocol.

A network is a collection of connected computers that share information.

A protocol is a set of rules that define the format of information that is transmitted across a network. TCP/IP is the standard protocol used by the Internet and most networks.

Each computer on an IP network must have a valid IPv4 or IPv6 address.

The IPv4 configuration of a network interface can be specified manually, obtained automatically from a DHCP or BOOTP server, or autoconfigured by the system.

The IPv6 configuration of a network interface can be obtained from a router using ICMPv6, from a DHCP server, or autoconfigured by the system.

On a Fedora 28 system, the /etc/sysconfig/network-scripts directory contains the configuration for network interfaces. On an Ubuntu 18 system, the /etc/netplan directory contains the configuration for network interfaces, and on an Ubuntu Server 14 system, network interface configuration is stored within the /etc/network/interfaces file.

Host names are computer names that, unlike IP addresses, are easy for humans to remember. Host names that are generated by the hierarchical Domain Name Space are called FQDNs.

Host names must be resolved to an IP address before network communication can take place.

Routers are devices that forward IP packets from one network to another. Each computer and router has a route table that it uses to determine how IP packets are forwarded.

Network services are started by a stand-alone daemon, or by xinetd on demand. In either case, they listen for requests on a certain port.

There are many ways to remotely administer a Linux system. You can perform command-line administration remotely via the telnet and ssh commands. For graphical remote administration, you can use the ssh –X command or VNC.

Many compression utilities are available for Linux systems; each of them uses a different compression algorithm and produces a different compression ratio.

Files can be backed up to an archive using a backup utility. To back up files to CD or DVD, you must use burning software instead of a backup utility.

The tar command is the most common backup utility used today; it is typically used to create compressed archives called tarballs.

The source code for Linux software can be obtained and compiled afterward using the GNU C Compiler; most source code is available in tarball format via the Internet.

Package managers install and manage compiled software of the same format. The Red Hat Package Manager (RPM) and Debian Package Manager (DPM) are the most common package managers available for Linux systems today.

In addition to managing and removing installed RPM packages, you can install and upgrade RPM packages from software repositories on the Internet using the dnf, yum, or zypper commands, depending on your Linux distribution. You can also use the rpm command to query installed RPM packages as well as perform RPM package management.

In addition to managing and removing installed DPM packages, you can install or upgrade DPM packages from software repositories on the Internet using the apt and apt-get commands. You can also use the dpkg command to view installed DPM packages or perform DPM package management.

Most programs depend on shared libraries for functionality. If a shared library is removed from the system, dependent programs will encounter errors.
Print jobs are spooled to a print queue before being printed to a printer.

You can configure spooling or printing for a printer by using the cupsaccept, cupsreject, cupsenable, and cupsdisable commands.

Print jobs are created using the lp command, can be viewed in the print queue using the lpstat command, and are removed from the print queue using the cancel command.

You can configure printers using the lpadmin command, the CUPS Web administration tool, or by modifying the /etc/cups/printers.conf file.

Most log files on a Linux system are stored in the /var/log directory.

System events are typically logged to files by the System Log Daemon (rsyslogd) or to a database by the Systemd Journal Daemon (journald).

You can use the jounalctl command to view the contents of the journald database.

Log files should be cleared or rotated over time to save disk space; the logrotate utility can be used to rotate log files.

User and group account information is typically stored in the /etc/passwd, /etc/shadow, and /etc/group files.

You can use the useradd command to create users and the groupadd command to create groups.

All users must have a valid password before logging in to a Linux system.

Users can be modified with the usermod, chage, chfn, chsh, and passwd commands, and groups can be modified using the groupmod command.

The userdel and groupdel commands can be used to remove users and groups from the system, respectively.
